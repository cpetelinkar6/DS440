{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_kWsqDlhU8X",
        "outputId": "67ea7ad9-9ce8-4c06-eee2-e4003e0845be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sqlite3\n",
        "import gc\n",
        "\n",
        "# import preprocessing\n",
        "from sklearn.preprocessing import OrdinalEncoder, LabelEncoder\n",
        "\n",
        "# import models\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.svm import SVR\n",
        "\n",
        "# import train test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "#from sklearn.inspection import permutation_importance\n",
        "\n",
        "# read data\n",
        "## mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "## read in files\n",
        "dir = '/content/drive/MyDrive/DS440/Wildfire_Data/Data/'\n",
        "cnx = sqlite3.connect(dir + 'Wildfire_data.sqlite') \n",
        "cursor = cnx.cursor()\n",
        "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='Fires';\")\n",
        "tables = cursor.fetchall()\n",
        "Fires = pd.read_sql_query(\"SELECT * FROM 'Fires'\", cnx).set_index('FOD_ID')\n",
        "Fires.drop(columns=['FPA_ID','SOURCE_SYSTEM_TYPE','SOURCE_SYSTEM','NWCG_REPORTING_AGENCY','NWCG_REPORTING_UNIT_ID',\\\n",
        "                    'NWCG_REPORTING_UNIT_NAME','SOURCE_REPORTING_UNIT','SOURCE_REPORTING_UNIT_NAME','LOCAL_FIRE_REPORT_ID',\\\n",
        "                    'LOCAL_INCIDENT_ID','FIRE_CODE','FIRE_NAME','ICS_209_PLUS_INCIDENT_JOIN_ID','ICS_209_PLUS_COMPLEX_JOIN_ID',\\\n",
        "                    'MTBS_ID','MTBS_FIRE_NAME','COMPLEX_NAME','DISCOVERY_TIME','OWNER_DESCR','NWCG_CAUSE_CLASSIFICATION',\\\n",
        "                    'NWCG_CAUSE_AGE_CATEGORY', 'FIRE_SIZE_CLASS', 'CONT_TIME','CONT_DATE'],inplace=True) #'FIRE_SIZE_CLASS',\n",
        "gc.collect()\n",
        "# combine data into one dataframe\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCm60GfhoYhU"
      },
      "source": [
        "### Data Preprocessing (Gabe)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ec4IekyUoX5c",
        "outputId": "81726c9a-e01a-430a-c9d4-f165d581e33b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# extract month from discovery date\n",
        "def get_first_element(date_list):\n",
        "  return date_list[0]\n",
        "\n",
        "Fires['DISCOVERY_DATE'] = Fires['DISCOVERY_DATE'].astype(\"string\")\n",
        "Fires['DISCOVERY_LIST'] = Fires['DISCOVERY_DATE'].str.split(pat='/')\n",
        "Fires['DISCOVERY_MONTH'] = Fires['DISCOVERY_LIST'].map(get_first_element)\n",
        "Fires['DISCOVERY_MONTH'] = Fires['DISCOVERY_MONTH'].astype(\"int64\")\n",
        "\n",
        "\n",
        "# encode causes into integers\n",
        "Fires['NWCG_GENERAL_CAUSE'] = Fires['NWCG_GENERAL_CAUSE'].astype(\"string\")\n",
        "cause_enc = LabelEncoder()\n",
        "Fires['NWCG_CAUSE_CLASSIFICATION_ORD'] = cause_enc.fit_transform(Fires['NWCG_GENERAL_CAUSE'])\n",
        "\n",
        "\n",
        "# figure out number of days to contain the fire\n",
        "Fires['DAYS_TO_CONT'] = Fires['CONT_DOY'] - Fires['DISCOVERY_DOY']\n",
        "##### if it was contained the next year, need to do something about that\n",
        "def convert_negative_days(day):\n",
        "  if day < 0:\n",
        "    return 365 + day\n",
        "  else:\n",
        "    return day\n",
        "\n",
        "Fires['DAYS_TO_CONT'] = Fires['DAYS_TO_CONT'].map(convert_negative_days)\n",
        "\n",
        "\n",
        "# encode state categories into integers\n",
        "Fires['STATE'] = Fires['STATE'].astype(\"string\")\n",
        "state_enc = LabelEncoder()\n",
        "Fires['STATE_ORD'] = state_enc.fit_transform(Fires['STATE'])\n",
        "\n",
        "# what to do about counties? Use fips code (must be an int)\n",
        "Fires['FIPS_CODE'] = Fires['FIPS_CODE'].astype(\"string\")\n",
        "Fires['FIPS_CODE'].fillna('1000000',inplace=True)\n",
        "Fires['FIPS_CODE'] = Fires['FIPS_CODE'].astype(\"int64\")\n",
        "\n",
        "# fill NA with mean of day to containment (mean = 0.92)\n",
        "Fires['DAYS_TO_CONT'].fillna(value=Fires['DAYS_TO_CONT'].mean(),inplace=True)\n",
        "\n",
        "\n",
        "# drop date and list\n",
        "Fires.drop(columns=['DISCOVERY_DATE','DISCOVERY_LIST','NWCG_GENERAL_CAUSE',\\\n",
        "                    'CONT_DOY','DISCOVERY_DOY','STATE','FIPS_NAME',\\\n",
        "                    'COUNTY'],inplace=True)\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "state_enc.classes_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1_WxdQNO3ce",
        "outputId": "6db38ff9-37f3-43c7-9056-7c1723be4e7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['AK', 'AL', 'AR', 'AZ', 'CA', 'CO', 'CT', 'DC', 'DE', 'FL', 'GA',\n",
              "       'HI', 'IA', 'ID', 'IL', 'IN', 'KS', 'KY', 'LA', 'MA', 'MD', 'ME',\n",
              "       'MI', 'MN', 'MO', 'MS', 'MT', 'NC', 'ND', 'NE', 'NH', 'NJ', 'NM',\n",
              "       'NV', 'NY', 'OH', 'OK', 'OR', 'PA', 'PR', 'RI', 'SC', 'SD', 'TN',\n",
              "       'TX', 'UT', 'VA', 'VT', 'WA', 'WI', 'WV', 'WY'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Fires"
      ],
      "metadata": {
        "id": "mHvVCVcvN_gk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "outputId": "9ce5b5d8-c148-47ae-dbf2-424f04d6b416"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           FIRE_YEAR  FIRE_SIZE   LATITUDE   LONGITUDE  FIPS_CODE  \\\n",
              "FOD_ID                                                              \n",
              "1               2005       0.10  40.036944 -121.005833       6063   \n",
              "2               2004       0.25  38.933056 -120.404444       6061   \n",
              "3               2004       0.10  38.984167 -120.735556       6017   \n",
              "4               2004       0.10  38.559167 -119.913333       6003   \n",
              "5               2004       0.10  38.559167 -119.933056       6003   \n",
              "...              ...        ...        ...         ...        ...   \n",
              "400482078       2016     125.00  36.431667  -84.421667      47151   \n",
              "400482080       2016     120.00  31.143611  -88.111944       1129   \n",
              "400482083       2017     120.00  33.920000  -85.310000       1029   \n",
              "400482085       2017     110.00  34.640000  -93.400000       5051   \n",
              "400482086       2015     109.00  34.794167  -94.958000      40079   \n",
              "\n",
              "           DISCOVERY_MONTH  NWCG_CAUSE_CLASSIFICATION_ORD  DAYS_TO_CONT  \\\n",
              "FOD_ID                                                                    \n",
              "1                        2                              9      0.000000   \n",
              "2                        5                              7      0.000000   \n",
              "3                        5                              1      0.000000   \n",
              "4                        6                              7      5.000000   \n",
              "5                        6                              7      5.000000   \n",
              "...                    ...                            ...           ...   \n",
              "400482078               10                              5      0.923788   \n",
              "400482080               10                              5      0.923788   \n",
              "400482083                4                              5      0.923788   \n",
              "400482085               11                              5      0.923788   \n",
              "400482086                7                              5      0.923788   \n",
              "\n",
              "           STATE_ORD  \n",
              "FOD_ID                \n",
              "1                  4  \n",
              "2                  4  \n",
              "3                  4  \n",
              "4                  4  \n",
              "5                  4  \n",
              "...              ...  \n",
              "400482078         43  \n",
              "400482080          1  \n",
              "400482083          1  \n",
              "400482085          2  \n",
              "400482086         36  \n",
              "\n",
              "[2166753 rows x 9 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-94d251ab-69dd-4618-b952-f3d498d582e6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>FIRE_YEAR</th>\n",
              "      <th>FIRE_SIZE</th>\n",
              "      <th>LATITUDE</th>\n",
              "      <th>LONGITUDE</th>\n",
              "      <th>FIPS_CODE</th>\n",
              "      <th>DISCOVERY_MONTH</th>\n",
              "      <th>NWCG_CAUSE_CLASSIFICATION_ORD</th>\n",
              "      <th>DAYS_TO_CONT</th>\n",
              "      <th>STATE_ORD</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>FOD_ID</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2005</td>\n",
              "      <td>0.10</td>\n",
              "      <td>40.036944</td>\n",
              "      <td>-121.005833</td>\n",
              "      <td>6063</td>\n",
              "      <td>2</td>\n",
              "      <td>9</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2004</td>\n",
              "      <td>0.25</td>\n",
              "      <td>38.933056</td>\n",
              "      <td>-120.404444</td>\n",
              "      <td>6061</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2004</td>\n",
              "      <td>0.10</td>\n",
              "      <td>38.984167</td>\n",
              "      <td>-120.735556</td>\n",
              "      <td>6017</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2004</td>\n",
              "      <td>0.10</td>\n",
              "      <td>38.559167</td>\n",
              "      <td>-119.913333</td>\n",
              "      <td>6003</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2004</td>\n",
              "      <td>0.10</td>\n",
              "      <td>38.559167</td>\n",
              "      <td>-119.933056</td>\n",
              "      <td>6003</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>400482078</th>\n",
              "      <td>2016</td>\n",
              "      <td>125.00</td>\n",
              "      <td>36.431667</td>\n",
              "      <td>-84.421667</td>\n",
              "      <td>47151</td>\n",
              "      <td>10</td>\n",
              "      <td>5</td>\n",
              "      <td>0.923788</td>\n",
              "      <td>43</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>400482080</th>\n",
              "      <td>2016</td>\n",
              "      <td>120.00</td>\n",
              "      <td>31.143611</td>\n",
              "      <td>-88.111944</td>\n",
              "      <td>1129</td>\n",
              "      <td>10</td>\n",
              "      <td>5</td>\n",
              "      <td>0.923788</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>400482083</th>\n",
              "      <td>2017</td>\n",
              "      <td>120.00</td>\n",
              "      <td>33.920000</td>\n",
              "      <td>-85.310000</td>\n",
              "      <td>1029</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>0.923788</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>400482085</th>\n",
              "      <td>2017</td>\n",
              "      <td>110.00</td>\n",
              "      <td>34.640000</td>\n",
              "      <td>-93.400000</td>\n",
              "      <td>5051</td>\n",
              "      <td>11</td>\n",
              "      <td>5</td>\n",
              "      <td>0.923788</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>400482086</th>\n",
              "      <td>2015</td>\n",
              "      <td>109.00</td>\n",
              "      <td>34.794167</td>\n",
              "      <td>-94.958000</td>\n",
              "      <td>40079</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>0.923788</td>\n",
              "      <td>36</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2166753 rows × 9 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-94d251ab-69dd-4618-b952-f3d498d582e6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-94d251ab-69dd-4618-b952-f3d498d582e6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-94d251ab-69dd-4618-b952-f3d498d582e6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yi5tTObsX02l"
      },
      "source": [
        "### EDA (Gabe and Colby)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "HtNXyYrFjrA-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "outputId": "5c674268-2d1f-418f-cd10-9abfcd9b2e03"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([2.16669e+06, 5.20000e+01, 1.10000e+01]),\n",
              " array([1.000e-05, 2.209e+05, 4.418e+05, 6.627e+05]),\n",
              " <a list of 3 Patch objects>)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.hist(Fires['FIRE_SIZE'], density=False, bins=3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(Fires['LATITUDE'], density=False, bins=10)"
      ],
      "metadata": {
        "id": "agzyeJRsObBV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "outputId": "91e66b70-c473-4826-90e0-6091fbc4fdb5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([3.21080e+04, 4.68400e+04, 6.11675e+05, 7.58469e+05, 4.29648e+05,\n",
              "        2.73888e+05, 6.00000e+00, 1.28300e+03, 1.05150e+04, 2.32100e+03]),\n",
              " array([17.93972222, 23.17881   , 28.41789778, 33.65698555, 38.89607333,\n",
              "        44.13516111, 49.37424889, 54.61333667, 59.85242444, 65.09151222,\n",
              "        70.3306    ]),\n",
              " <a list of 10 Patch objects>)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(Fires['LONGITUDE'], density=False, bins=10)"
      ],
      "metadata": {
        "id": "wVP6FWyFOrTM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "outputId": "300ea357-cccb-40dc-f278-e4bc2d494de7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([6.00000e+00, 1.04670e+04, 1.17640e+04, 1.62700e+03, 8.31450e+04,\n",
              "        4.40302e+05, 2.42508e+05, 5.09149e+05, 6.97258e+05, 1.70527e+05]),\n",
              " array([-178.8026    , -167.44803444, -156.09346889, -144.73890333,\n",
              "        -133.38433778, -122.02977222, -110.67520666,  -99.32064111,\n",
              "         -87.96607555,  -76.61151   ,  -65.25694444]),\n",
              " <a list of 10 Patch objects>)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "4U0e15sKglUx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7ca9707-2677-45d9-b8c6-f0698879f6f6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([2165295,     743,     290,     138,      76,      41,      27,\n",
              "             30,      23,      14]),\n",
              " array([1.000000e-05, 1.992880e+04, 3.985760e+04, 5.978640e+04,\n",
              "        7.971520e+04, 9.964400e+04, 1.195728e+05, 1.395016e+05,\n",
              "        1.594304e+05, 1.793592e+05, 1.992880e+05]))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "np.histogram(Fires[Fires['FIRE_SIZE'] <= 2e+05]['FIRE_SIZE'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "orxyEBNXglek",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "outputId": "cbb71e7e-cc8b-4c90-942b-548c083e0868"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([1027824.,  310697.,   59251.,  138733.,   24451.,   80514.,\n",
              "          12070.,   45486.,    7720.,   68306.]),\n",
              " array([1.000000e-05, 5.000090e-01, 1.000008e+00, 1.500007e+00,\n",
              "        2.000006e+00, 2.500005e+00, 3.000004e+00, 3.500003e+00,\n",
              "        4.000002e+00, 4.500001e+00, 5.000000e+00]),\n",
              " <a list of 10 Patch objects>)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.hist(Fires[Fires['FIRE_SIZE'] <= 5]['FIRE_SIZE'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "r8sQokjowy9l",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "outputId": "6a59aab5-06f7-4f1d-83a8-397b08c855d6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Looking at Fire size compared to year\n",
        "plt.scatter(Fires['FIRE_SIZE'], Fires['FIRE_YEAR'], marker='o');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "cku0sJYuth0H",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "outputId": "0e365366-fdce-497b-f2b8-4be92f5c990c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([817539., 175824.,   2205.,  17205., 547438., 377437.,   9763.,\n",
              "         28491.,  36437., 154414.]),\n",
              " array([ 0. ,  1.2,  2.4,  3.6,  4.8,  6. ,  7.2,  8.4,  9.6, 10.8, 12. ]),\n",
              " <a list of 10 Patch objects>)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.hist(Fires['NWCG_CAUSE_CLASSIFICATION_ORD'], density=False, bins=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "WQax3L1xpo4V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "outputId": "c25303c3-d0d5-4354-a107-e2d2f4656886"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.scatter(Fires['NWCG_CAUSE_CLASSIFICATION_ORD'], Fires['FIRE_YEAR'], marker='o');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "nS8t0bdPp3OV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc9f94f3-5592-41cc-8618-cc72d7676fd7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Arson/incendiarism', 'Debris and open burning',\n",
              "       'Equipment and vehicle use', 'Firearms and explosives use',\n",
              "       'Fireworks', 'Missing data/not specified/undetermined',\n",
              "       'Misuse of fire by a minor', 'Natural', 'Other causes',\n",
              "       'Power generation/transmission/distribution',\n",
              "       'Railroad operations and maintenance', 'Recreation and ceremony',\n",
              "       'Smoking'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "cause_enc.classes_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-Oc9ZBVYR_j"
      },
      "source": [
        "### Created baseline MAE and MSE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nuJ8bu9tavqn"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h6US3XAvYOkO"
      },
      "outputs": [],
      "source": [
        "FIRE_SIZE_PERMUTE = Fires['FIRE_SIZE'].sample(frac=1,random_state=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEZd5OQ2YOYw",
        "outputId": "9ab277f6-9c12-463f-ab43-b25ca46b9f9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MAE:  149.40381504892986\n",
            "MSE:  12867179.721882567\n"
          ]
        }
      ],
      "source": [
        "print(\"MAE: \",mean_absolute_error(FIRE_SIZE_PERMUTE,Fires['FIRE_SIZE']))\n",
        "print(\"MSE: \",mean_squared_error(FIRE_SIZE_PERMUTE,Fires['FIRE_SIZE']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wg3mAaZkVGvQ"
      },
      "outputs": [],
      "source": [
        "Fires12 = Fires[Fires['FIRE_YEAR'] < 2013].copy()\n",
        "Fires13 = Fires[Fires['FIRE_YEAR'] >= 2013].copy()\n",
        "X_train = Fires12.loc[:, Fires12.columns != 'FIRE_SIZE']\n",
        "y_train = Fires12['FIRE_SIZE'].copy()\n",
        "X_test = Fires13.loc[:, Fires13.columns != 'FIRE_SIZE']\n",
        "y_test = Fires13['FIRE_SIZE'].copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "W2CMi8LWV5XT",
        "outputId": "0a980b26-1cb1-458a-8b11-2570a18c2147"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-8db4fbdf-0df8-4e88-a835-2cc4722c0a01\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>FIRE_YEAR</th>\n",
              "      <th>LATITUDE</th>\n",
              "      <th>LONGITUDE</th>\n",
              "      <th>FIPS_CODE</th>\n",
              "      <th>DISCOVERY_MONTH</th>\n",
              "      <th>NWCG_CAUSE_CLASSIFICATION_ORD</th>\n",
              "      <th>DAYS_TO_CONT</th>\n",
              "      <th>STATE_ORD</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>FOD_ID</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2005</td>\n",
              "      <td>40.036944</td>\n",
              "      <td>-121.005833</td>\n",
              "      <td>6063</td>\n",
              "      <td>2</td>\n",
              "      <td>9</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2004</td>\n",
              "      <td>38.933056</td>\n",
              "      <td>-120.404444</td>\n",
              "      <td>6061</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2004</td>\n",
              "      <td>38.984167</td>\n",
              "      <td>-120.735556</td>\n",
              "      <td>6017</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2004</td>\n",
              "      <td>38.559167</td>\n",
              "      <td>-119.913333</td>\n",
              "      <td>6003</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2004</td>\n",
              "      <td>38.559167</td>\n",
              "      <td>-119.933056</td>\n",
              "      <td>6003</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>400443634</th>\n",
              "      <td>2012</td>\n",
              "      <td>33.485700</td>\n",
              "      <td>-112.148200</td>\n",
              "      <td>4013</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>400443635</th>\n",
              "      <td>2012</td>\n",
              "      <td>33.484100</td>\n",
              "      <td>-112.158900</td>\n",
              "      <td>4013</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>400443636</th>\n",
              "      <td>2012</td>\n",
              "      <td>33.453900</td>\n",
              "      <td>-112.087900</td>\n",
              "      <td>4013</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>400443637</th>\n",
              "      <td>2012</td>\n",
              "      <td>33.469300</td>\n",
              "      <td>-112.133700</td>\n",
              "      <td>4013</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>400443638</th>\n",
              "      <td>2012</td>\n",
              "      <td>33.506700</td>\n",
              "      <td>-112.112300</td>\n",
              "      <td>4013</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1707428 rows × 8 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8db4fbdf-0df8-4e88-a835-2cc4722c0a01')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8db4fbdf-0df8-4e88-a835-2cc4722c0a01 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8db4fbdf-0df8-4e88-a835-2cc4722c0a01');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "           FIRE_YEAR   LATITUDE   LONGITUDE  FIPS_CODE  DISCOVERY_MONTH  \\\n",
              "FOD_ID                                                                    \n",
              "1               2005  40.036944 -121.005833       6063                2   \n",
              "2               2004  38.933056 -120.404444       6061                5   \n",
              "3               2004  38.984167 -120.735556       6017                5   \n",
              "4               2004  38.559167 -119.913333       6003                6   \n",
              "5               2004  38.559167 -119.933056       6003                6   \n",
              "...              ...        ...         ...        ...              ...   \n",
              "400443634       2012  33.485700 -112.148200       4013                3   \n",
              "400443635       2012  33.484100 -112.158900       4013                3   \n",
              "400443636       2012  33.453900 -112.087900       4013                3   \n",
              "400443637       2012  33.469300 -112.133700       4013                3   \n",
              "400443638       2012  33.506700 -112.112300       4013                5   \n",
              "\n",
              "           NWCG_CAUSE_CLASSIFICATION_ORD  DAYS_TO_CONT  STATE_ORD  \n",
              "FOD_ID                                                             \n",
              "1                                      9           0.0          4  \n",
              "2                                      7           0.0          4  \n",
              "3                                      1           0.0          4  \n",
              "4                                      7           5.0          4  \n",
              "5                                      7           5.0          4  \n",
              "...                                  ...           ...        ...  \n",
              "400443634                              5           0.0          3  \n",
              "400443635                              5           0.0          3  \n",
              "400443636                              5           0.0          3  \n",
              "400443637                              5           0.0          3  \n",
              "400443638                              5           0.0          3  \n",
              "\n",
              "[1707428 rows x 8 columns]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Plw1Yh4KXjzi"
      },
      "source": [
        "### Initial Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "reGstsqrZ8WV"
      },
      "outputs": [],
      "source": [
        "Fires_sample = Fires.sample(frac=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Fires_sample"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "id": "ISWPwmDdO6g_",
        "outputId": "e5ddd1bd-24fc-477e-9aca-84107c7d7723"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           FIRE_YEAR  FIRE_SIZE   LATITUDE   LONGITUDE  FIPS_CODE  \\\n",
              "FOD_ID                                                              \n",
              "19504938        2010       0.20  34.962783  -77.502450      37133   \n",
              "1457762         2010       0.05  33.500451  -82.794310      13301   \n",
              "201399347       2010      10.00  28.707753  -98.197174      48297   \n",
              "1121811         2002      15.00  28.825800  -82.558600      12017   \n",
              "1317621         1995       0.10  37.498889 -119.985000    1000000   \n",
              "...              ...        ...        ...         ...        ...   \n",
              "400355309       2018      25.00  37.458649  -83.449547      21025   \n",
              "400133913       2017       0.10  35.250588 -120.622893       6079   \n",
              "1805854         1993       0.10  37.937698  -82.420230      54099   \n",
              "400158956       2017       0.10  37.647425 -120.972071       6099   \n",
              "511619          2005       2.00  30.498751  -90.336797      22105   \n",
              "\n",
              "           DISCOVERY_MONTH  NWCG_CAUSE_CLASSIFICATION_ORD  DAYS_TO_CONT  \\\n",
              "FOD_ID                                                                    \n",
              "19504938                 3                              5      0.000000   \n",
              "1457762                  3                              1      0.000000   \n",
              "201399347                6                              5      0.923788   \n",
              "1121811                  3                             10      0.923788   \n",
              "1317621                  7                              2      0.923788   \n",
              "...                    ...                            ...           ...   \n",
              "400355309                4                              0      1.000000   \n",
              "400133913                9                              1      0.923788   \n",
              "1805854                  1                              0      0.000000   \n",
              "400158956                6                              5      0.923788   \n",
              "511619                  10                             11      0.000000   \n",
              "\n",
              "           STATE_ORD  \n",
              "FOD_ID                \n",
              "19504938          27  \n",
              "1457762           10  \n",
              "201399347         44  \n",
              "1121811            9  \n",
              "1317621            4  \n",
              "...              ...  \n",
              "400355309         17  \n",
              "400133913          4  \n",
              "1805854           50  \n",
              "400158956          4  \n",
              "511619            18  \n",
              "\n",
              "[21668 rows x 9 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-17b14049-5b60-438b-a55b-67713e720ed7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>FIRE_YEAR</th>\n",
              "      <th>FIRE_SIZE</th>\n",
              "      <th>LATITUDE</th>\n",
              "      <th>LONGITUDE</th>\n",
              "      <th>FIPS_CODE</th>\n",
              "      <th>DISCOVERY_MONTH</th>\n",
              "      <th>NWCG_CAUSE_CLASSIFICATION_ORD</th>\n",
              "      <th>DAYS_TO_CONT</th>\n",
              "      <th>STATE_ORD</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>FOD_ID</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>19504938</th>\n",
              "      <td>2010</td>\n",
              "      <td>0.20</td>\n",
              "      <td>34.962783</td>\n",
              "      <td>-77.502450</td>\n",
              "      <td>37133</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1457762</th>\n",
              "      <td>2010</td>\n",
              "      <td>0.05</td>\n",
              "      <td>33.500451</td>\n",
              "      <td>-82.794310</td>\n",
              "      <td>13301</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>201399347</th>\n",
              "      <td>2010</td>\n",
              "      <td>10.00</td>\n",
              "      <td>28.707753</td>\n",
              "      <td>-98.197174</td>\n",
              "      <td>48297</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>0.923788</td>\n",
              "      <td>44</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1121811</th>\n",
              "      <td>2002</td>\n",
              "      <td>15.00</td>\n",
              "      <td>28.825800</td>\n",
              "      <td>-82.558600</td>\n",
              "      <td>12017</td>\n",
              "      <td>3</td>\n",
              "      <td>10</td>\n",
              "      <td>0.923788</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1317621</th>\n",
              "      <td>1995</td>\n",
              "      <td>0.10</td>\n",
              "      <td>37.498889</td>\n",
              "      <td>-119.985000</td>\n",
              "      <td>1000000</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>0.923788</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>400355309</th>\n",
              "      <td>2018</td>\n",
              "      <td>25.00</td>\n",
              "      <td>37.458649</td>\n",
              "      <td>-83.449547</td>\n",
              "      <td>21025</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>400133913</th>\n",
              "      <td>2017</td>\n",
              "      <td>0.10</td>\n",
              "      <td>35.250588</td>\n",
              "      <td>-120.622893</td>\n",
              "      <td>6079</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>0.923788</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1805854</th>\n",
              "      <td>1993</td>\n",
              "      <td>0.10</td>\n",
              "      <td>37.937698</td>\n",
              "      <td>-82.420230</td>\n",
              "      <td>54099</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>400158956</th>\n",
              "      <td>2017</td>\n",
              "      <td>0.10</td>\n",
              "      <td>37.647425</td>\n",
              "      <td>-120.972071</td>\n",
              "      <td>6099</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>0.923788</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>511619</th>\n",
              "      <td>2005</td>\n",
              "      <td>2.00</td>\n",
              "      <td>30.498751</td>\n",
              "      <td>-90.336797</td>\n",
              "      <td>22105</td>\n",
              "      <td>10</td>\n",
              "      <td>11</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>21668 rows × 9 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-17b14049-5b60-438b-a55b-67713e720ed7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-17b14049-5b60-438b-a55b-67713e720ed7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-17b14049-5b60-438b-a55b-67713e720ed7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tP35Ly00MKMa"
      },
      "outputs": [],
      "source": [
        "Train_Fires = Fires_sample[Fires_sample['FIRE_YEAR'] < 2013].copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aH8pOuQ_M2Zp"
      },
      "outputs": [],
      "source": [
        "Train_Fires.size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TClAF_XLRP6e"
      },
      "outputs": [],
      "source": [
        "Train_Fires"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1DGy4mk6N6W0"
      },
      "outputs": [],
      "source": [
        "Test_Fires = Fires_sample[Fires_sample['FIRE_YEAR'] > 2013].copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJxQd1MiN-br"
      },
      "outputs": [],
      "source": [
        "Test_Fires.size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EPTuEg4a-i6o"
      },
      "outputs": [],
      "source": [
        "### split in to test and train by year\n",
        "Fires12 = Fires_sample[Fires_sample['FIRE_YEAR'] < 2013].copy()\n",
        "Fires13 = Fires_sample[Fires_sample['FIRE_YEAR'] >= 2013].copy()\n",
        "X_train = Fires12.loc[:, Fires12.columns != 'FIRE_SIZE']\n",
        "y_train = Fires12['FIRE_SIZE'].copy()\n",
        "X_test = Fires13.loc[:, Fires13.columns != 'FIRE_SIZE']\n",
        "y_test = Fires13['FIRE_SIZE'].copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CuyMpr2kZ-WY"
      },
      "outputs": [],
      "source": [
        "#### split data into training, validation, and testing\n",
        "X = Fires_sample.loc[:, Fires_sample.columns != 'FIRE_SIZE']\n",
        "X = X[['FIRE_YEAR','LATITUDE','LONGITUDE','DISCOVERY_MONTH','DAYS_TO_CONT']] #,'STATE_ORD','NWCG_CAUSE_CLASSIFICATION_ORD','FIPS_CODE',\n",
        "\n",
        "y = Fires_sample['FIRE_SIZE']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state= 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ORhZQanPavow"
      },
      "outputs": [],
      "source": [
        "X_train = X_train[X_train['FIRE_YEAR'] < 2013]\n",
        "X_train['FIRE_YEAR'].max()\n",
        "\n",
        "X_test = X_test[X_test['FIRE_YEAR'] >= 2013]\n",
        "X_test['FIRE_YEAR'].min()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tqi0nfBPOYut"
      },
      "outputs": [],
      "source": [
        "X_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0PxDtRbavmu",
        "outputId": "1f42601e-dad3-4160-dabb-b34d8a352258"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'sklearn.linear_model._base.LinearRegression'>\n",
            "4525542.625181656 \n",
            " 234.10585365459815\n",
            "<class 'sklearn.ensemble._forest.RandomForestRegressor'>\n",
            "4855995.152449286 \n",
            " 305.46645756989534\n",
            "<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>\n",
            "4792943.50989135 \n",
            " 168.32515230889217\n",
            "<class 'sklearn.ensemble._weight_boosting.AdaBoostRegressor'>\n",
            "5805595.686211213 \n",
            " 408.2975803799013\n",
            "<class 'sklearn.neighbors._regression.KNeighborsRegressor'>\n",
            "5610328.144643633 \n",
            " 263.67627170505233\n",
            "<class 'sklearn.svm._classes.SVR'>\n",
            "4537057.0517183365 \n",
            " 86.3380801125481\n"
          ]
        }
      ],
      "source": [
        "#### split data into training, validation, and testing\n",
        "#X = Fires.loc[:, Fires.columns != 'FIRE_SIZE']\n",
        "#y = Fires['FIRE_SIZE']\n",
        "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "#### list of models to train\n",
        "# xgboost, linear regression (with and without regularization), random forest, naive bayes, nearest neighbors\n",
        "# k-means, svm\n",
        "model_list = [LinearRegression, RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor,\n",
        "               KNeighborsRegressor, SVR] #GaussianNB, \n",
        "model_scores = dict()\n",
        "for model in model_list:\n",
        "  # fit model to data\n",
        "  print(model)\n",
        "  regressor = model().fit(X_train,y_train)\n",
        "  y_pred = regressor.predict(X_test)\n",
        "  model_mse = mean_squared_error(y_pred,y_test)\n",
        "  model_mae = mean_absolute_error(y_pred,y_test)\n",
        "  model_scores[model] = (regressor,y_pred,model_mse,model_mae)\n",
        "  print(model_mse,'\\n',model_mae)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Stress Test\n",
        "model_list = [RandomForestRegressor, GradientBoostingRegressor, SVR] #GaussianNB, \n",
        "model_scores = dict()\n",
        "for model in model_list:\n",
        "  # fit model to data\n",
        "  print(model)\n",
        "  regressor = model().fit(X_train,y_train)\n",
        "  y_pred = regressor.predict(X_test)\n",
        "  model_mse = mean_squared_error(y_pred,y_test)\n",
        "  model_mae = mean_absolute_error(y_pred,y_test)\n",
        "  model_scores[model] = (regressor,y_pred,model_mse,model_mae)\n",
        "  print(model_mse,'\\n',model_mae)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BsdR_l7AR5vz",
        "outputId": "ffd5b0ce-ca72-4199-83e5-f4560d7d22b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'sklearn.ensemble._forest.RandomForestRegressor'>\n",
            "10847611.718927996 \n",
            " 154.00123735956214\n",
            "<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>\n",
            "10259555.29927107 \n",
            " 142.83898137171846\n",
            "<class 'sklearn.svm._classes.SVR'>\n",
            "10483515.600567665 \n",
            " 93.4093840501023\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<class 'sklearn.ensemble._forest.RandomForestRegressor'>\n",
        "10847611.718927996 \n",
        " 154.00123735956214\n",
        "<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>\n",
        "10259555.29927107 \n",
        " 142.83898137171846\n",
        "<class 'sklearn.svm._classes.SVR'>\n",
        "10483515.600567665 \n",
        " 93.4093840501023"
      ],
      "metadata": {
        "id": "dCSBLQyYWVkC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Stress Test 30%\n",
        "model_list = [RandomForestRegressor, GradientBoostingRegressor, SVR] #GaussianNB, \n",
        "model_scores = dict()\n",
        "for model in model_list:\n",
        "  # fit model to data\n",
        "  print(model)\n",
        "  regressor = model().fit(X_train,y_train)\n",
        "  y_pred = regressor.predict(X_test)\n",
        "  model_mse = mean_squared_error(y_pred,y_test)\n",
        "  model_mae = mean_absolute_error(y_pred,y_test)\n",
        "  model_scores[model] = (regressor,y_pred,model_mse,model_mae)\n",
        "  print(model_mse,'\\n',model_mae)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0GGSIYaWFg8",
        "outputId": "ef7e278b-fe58-4635-f08f-08dbaaf84c93"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'sklearn.ensemble._forest.RandomForestRegressor'>\n",
            "7252792.573173017 \n",
            " 266.42396432470713\n",
            "<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>\n",
            "6568377.590433821 \n",
            " 167.7819483408839\n",
            "<class 'sklearn.svm._classes.SVR'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ensemble Models for Non-Weather data"
      ],
      "metadata": {
        "id": "unzaUyvMUHNi"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMEuy3CY5BdE"
      },
      "source": [
        "https://www.geeksforgeeks.org/ensemble-methods-in-python/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btBlYzEO6moz"
      },
      "source": [
        "#### Averaging Method (Used for stress treat, beat Boosting model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jue9KoiC3t5h"
      },
      "outputs": [],
      "source": [
        "# importing utility modules\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import xgboost as xgb\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        " \n",
        "# Splitting between train data into training and validation dataset\n",
        "Fires12 = Fires_sample[Fires_sample['FIRE_YEAR'] < 2013].copy()\n",
        "Fires13 = Fires_sample[Fires_sample['FIRE_YEAR'] >= 2013].copy()\n",
        "X_train = Fires12.loc[:, Fires12.columns != 'FIRE_SIZE']\n",
        "y_train = Fires12['FIRE_SIZE'].copy()\n",
        "X_test = Fires13.loc[:, Fires13.columns != 'FIRE_SIZE']\n",
        "y_test = Fires13['FIRE_SIZE'].copy()\n",
        " \n",
        "# initializing all the model objects with default parameters\n",
        "model_1 = GradientBoostingRegressor()\n",
        "model_2 = SVR()\n",
        "model_3 = RandomForestRegressor()\n",
        " \n",
        "# training all the model on the training dataset\n",
        "model_1.fit(X_train, y_train)\n",
        "model_2.fit(X_train, y_train)\n",
        "model_3.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the validation dataset\n",
        "pred_1 = model_1.predict(X_test)\n",
        "pred_2 = model_2.predict(X_test)\n",
        "pred_3 = model_3.predict(X_test)\n",
        " \n",
        "# final prediction after averaging on the prediction of all 3 models\n",
        "pred_final = (pred_1+pred_2+pred_3)/3.0\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "mod1 = (mean_absolute_error(pred_1, y_test))\n",
        "mod2 = (mean_absolute_error(pred_2, y_test))\n",
        "mod3 = (mean_absolute_error(pred_3, y_test))\n",
        "#print(mean_absolute_error(pred_final, y_test))  ##switch Y_pred and test\n",
        "#Averaging_model = (mod1+mod2+mod3)/3.0\n",
        "Avg_mod = (mean_absolute_error(pred_final, y_test))\n",
        "print(\"Averaging Model:\", Avg_mod)\n",
        "\n",
        "#Averaging Model: 192.75162591271354"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ud7PTVtq-3tR"
      },
      "source": [
        "#### Boosting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TTAiVnfgZ_-T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23424305-536d-408c-948c-25676eadf2d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Boosting Model: 203.92772701977273\n"
          ]
        }
      ],
      "source": [
        "# importing utility modules\n",
        "import pandas as pd\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        " \n",
        "# Splitting between train data into training and validation dataset\n",
        "\n",
        "Fires12 = Fires_sample[Fires_sample['FIRE_YEAR'] < 2013].copy()\n",
        "Fires13 = Fires_sample[Fires_sample['FIRE_YEAR'] >= 2013].copy()\n",
        "X_train = Fires12.loc[:, Fires12.columns != 'FIRE_SIZE']\n",
        "y_train = Fires12['FIRE_SIZE'].copy()\n",
        "X_test = Fires13.loc[:, Fires13.columns != 'FIRE_SIZE']\n",
        "y_test = Fires13['FIRE_SIZE'].copy()\n",
        "\n",
        " \n",
        "# initializing the boosting module with default parameters\n",
        "model = GradientBoostingRegressor() #163\n",
        " \n",
        "# training the model on the train dataset\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Boost_mod = (mean_absolute_error(pred_final, y_test))\n",
        "print(\"Boosting Model:\", Boost_mod)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Bagging (Used for Stress Test, beat Boosting model testing against Avg)"
      ],
      "metadata": {
        "id": "tJls0t-fJgut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "import xgboost as xgb\n",
        " \n",
        "# importing bagging module\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "\n",
        "# Splitting between train data into training and validation dataset\n",
        "Fires12 = Fires_sample[Fires_sample['FIRE_YEAR'] < 2013].copy()\n",
        "Fires13 = Fires_sample[Fires_sample['FIRE_YEAR'] >= 2013].copy()\n",
        "X_train = Fires12.loc[:, Fires12.columns != 'FIRE_SIZE']\n",
        "y_train = Fires12['FIRE_SIZE'].copy()\n",
        "X_test = Fires13.loc[:, Fires13.columns != 'FIRE_SIZE']\n",
        "y_test = Fires13['FIRE_SIZE'].copy()\n",
        " \n",
        "# initializing the bagging model using XGboost as base model with default parameters\n",
        "model = BaggingRegressor(base_estimator=xgb.XGBRegressor())\n",
        " \n",
        "# training model\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Bag_mod = (mean_absolute_error(pred_final, y_test))\n",
        "print(\"Bagging Model:\", Bag_mod)\n",
        "#Bagging Model: 212.678238645268"
      ],
      "metadata": {
        "id": "FpxLuUoVJgPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Season models"
      ],
      "metadata": {
        "id": "Rtu_xLjK5TmV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Seasons 1-3"
      ],
      "metadata": {
        "id": "6iX9nXOB591Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Fire Size between 0-.25"
      ],
      "metadata": {
        "id": "GGTfMfWPABRe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Fires_sample['FIRE_SIZE'].value_counts()[:100]"
      ],
      "metadata": {
        "id": "7FlzPro0j17H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########## Spliting by size\n",
        "\n",
        "Size_sample1 = Fires_sample[Fires_sample['FIRE_SIZE'] <= .25]\n",
        "\n",
        "########### Spliting by month only needed when switching seasons\n",
        "Season_sample1 = Size_sample1[Size_sample1['DISCOVERY_MONTH'] <= 3]\n",
        "\n",
        "################ Train test split\n",
        "\n",
        "Fires12 = Season_sample1[Season_sample1['FIRE_YEAR'] <= 2017].copy()\n",
        "Fires13 = Season_sample1[Season_sample1['FIRE_YEAR'] > 2017].copy()\n",
        "X_train = Fires12.loc[:, Fires12.columns != 'FIRE_SIZE']\n",
        "y_train = Fires12['FIRE_SIZE'].copy()\n",
        "X_test = Fires13.loc[:, Fires13.columns != 'FIRE_SIZE']\n",
        "y_test = Fires13['FIRE_SIZE'].copy()\n",
        "\n",
        "############################################ Testing best methods\n",
        "\n",
        "model_list = [RandomForestRegressor, GradientBoostingRegressor, SVR] #GaussianNB, \n",
        "model_scores = dict()\n",
        "for model in model_list:\n",
        "  # fit model to data\n",
        "  print(model)\n",
        "  regressor = model().fit(X_train,y_train)\n",
        "  y_pred = regressor.predict(X_test)\n",
        "  model_mse = mean_squared_error(y_pred,y_test)\n",
        "  model_mae = mean_absolute_error(y_pred,y_test)\n",
        "  model_scores[model] = (regressor,y_pred,model_mse,model_mae)\n",
        "  print(model_mse,'\\n',model_mae)\n",
        "\n",
        "############################################# Averaging Method\n",
        "#############################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import xgboost as xgb\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        " \n",
        "# initializing all the model objects with default parameters\n",
        "model_1 = GradientBoostingRegressor()\n",
        "model_2 = SVR()\n",
        "model_3 = RandomForestRegressor()\n",
        " \n",
        "# training all the model on the training dataset\n",
        "model_1.fit(X_train, y_train)\n",
        "model_2.fit(X_train, y_train)\n",
        "model_3.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the validation dataset\n",
        "pred_1 = model_1.predict(X_test)\n",
        "pred_2 = model_2.predict(X_test)\n",
        "pred_3 = model_3.predict(X_test)\n",
        " \n",
        "# final prediction after averaging on the prediction of all 3 models\n",
        "pred_final = (pred_1+pred_2+pred_3)/3.0\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "mod1 = (mean_absolute_error(pred_1, y_test))\n",
        "mod2 = (mean_absolute_error(pred_2, y_test))\n",
        "mod3 = (mean_absolute_error(pred_3, y_test))\n",
        "#print(mean_absolute_error(pred_final, y_test))  ##switch Y_pred and test\n",
        "#Averaging_model = (mod1+mod2+mod3)/3.0\n",
        "Avg_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Avg_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('AVERAGING')\n",
        "print(\"Averaging Model MAE:\", Avg_mod)\n",
        "print(\"Averaging Model MSE:\", Avg_mod1)\n",
        "\n",
        "############################################# Boosting Method\n",
        "#############################################\n",
        "\n",
        "# importing utility modules\n",
        "import pandas as pd\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        " \n",
        "# initializing the boosting module with default parameters\n",
        "model = GradientBoostingRegressor() #163\n",
        " \n",
        "# training the model on the train dataset\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Boost_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Boost_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('BOOSTING')\n",
        "print(\"Averaging Model MAE:\", Boost_mod)\n",
        "print(\"Averaging Model MSE:\", Boost_mod1)\n",
        "print('BAGGING')\n",
        "\n",
        "################################################# Bagging Method\n",
        "#################################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "import xgboost as xgb\n",
        " \n",
        "# importing bagging module\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "\n",
        "# initializing the bagging model using XGboost as base model with default parameters\n",
        "model = BaggingRegressor(base_estimator=xgb.XGBRegressor())\n",
        " \n",
        "# training model\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Bag_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Bag_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print(\"Bagging Model MAE:\", Bag_mod)\n",
        "print(\"Bagging Model MSE:\", Bag_mod1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TQWeYTCu_nE",
        "outputId": "57b7c3ef-9f82-456e-ffbd-b739d1a9cdf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'sklearn.ensemble._forest.RandomForestRegressor'>\n",
            "0.003151568880079504 \n",
            " 0.03605617124580397\n",
            "<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>\n",
            "0.0029062805349802795 \n",
            " 0.03797238725505548\n",
            "<class 'sklearn.svm._classes.SVR'>\n",
            "0.004744317793710989 \n",
            " 0.05203630924786866\n",
            "AVERAGING\n",
            "Averaging Model MAE: 0.03937519364225787\n",
            "Averaging Model MSE: 0.003048090648337457\n",
            "BOOSTING\n",
            "Averaging Model MAE: 0.0379723872550555\n",
            "Averaging Model MSE: 0.0029062805349802786\n",
            "BAGGING\n",
            "[00:19:26] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[00:19:32] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[00:19:39] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[00:19:45] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[00:19:52] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[00:19:58] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[00:20:04] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[00:20:11] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[00:20:17] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[00:20:24] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "Bagging Model MAE: 0.03984317811035111\n",
            "Bagging Model MSE: 0.0029434153064197706\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Fire Size .25-1"
      ],
      "metadata": {
        "id": "4QK7AqX4vUjp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "########## Spliting by size\n",
        "\n",
        "Size_sample10 = Fires_sample[(Fires_sample[\"FIRE_SIZE\"] > .25) & (Fires_sample[\"FIRE_SIZE\"] <= 1)]\n",
        "\n",
        "########### Spliting by month only needed when switching seasons\n",
        "Season_sample10 = Size_sample10[Size_sample10['DISCOVERY_MONTH'] <= 3]\n",
        "# Season_sample50 = Size_sample50[(Size_sample50[\"DISCOVERY_MONTH\"] > 1) & (Size_sample10[\"DISCOVERY_MONTH\"] <= 3)]\n",
        "\n",
        "################ Train test split\n",
        "\n",
        "Fires12 = Season_sample10[Season_sample10['FIRE_YEAR'] <= 2017].copy()\n",
        "Fires13 = Season_sample10[Season_sample10['FIRE_YEAR'] > 2017].copy()\n",
        "X_train = Fires12.loc[:, Fires12.columns != 'FIRE_SIZE']\n",
        "y_train = Fires12['FIRE_SIZE'].copy()\n",
        "X_test = Fires13.loc[:, Fires13.columns != 'FIRE_SIZE']\n",
        "y_test = Fires13['FIRE_SIZE'].copy()\n",
        "\n",
        "############################################ Testing best methods\n",
        "\n",
        "model_list = [RandomForestRegressor, GradientBoostingRegressor, SVR] #GaussianNB, \n",
        "model_scores = dict()\n",
        "for model in model_list:\n",
        "  # fit model to data\n",
        "  print(model)\n",
        "  regressor = model().fit(X_train,y_train)\n",
        "  y_pred = regressor.predict(X_test)\n",
        "  model_mse = mean_squared_error(y_pred,y_test)\n",
        "  model_mae = mean_absolute_error(y_pred,y_test)\n",
        "  model_scores[model] = (regressor,y_pred,model_mse,model_mae)\n",
        "  print(model_mse,'\\n',model_mae)\n",
        "\n",
        "############################################# Averaging Method\n",
        "#############################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import xgboost as xgb\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        " \n",
        "# initializing all the model objects with default parameters\n",
        "model_1 = GradientBoostingRegressor()\n",
        "model_2 = SVR()\n",
        "model_3 = RandomForestRegressor()\n",
        " \n",
        "# training all the model on the training dataset\n",
        "model_1.fit(X_train, y_train)\n",
        "model_2.fit(X_train, y_train)\n",
        "model_3.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the validation dataset\n",
        "pred_1 = model_1.predict(X_test)\n",
        "pred_2 = model_2.predict(X_test)\n",
        "pred_3 = model_3.predict(X_test)\n",
        " \n",
        "# final prediction after averaging on the prediction of all 3 models\n",
        "pred_final = (pred_1+pred_2+pred_3)/3.0\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "mod1 = (mean_absolute_error(pred_1, y_test))\n",
        "mod2 = (mean_absolute_error(pred_2, y_test))\n",
        "mod3 = (mean_absolute_error(pred_3, y_test))\n",
        "#print(mean_absolute_error(pred_final, y_test))  ##switch Y_pred and test\n",
        "#Averaging_model = (mod1+mod2+mod3)/3.0\n",
        "Avg_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Avg_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('AVERAGING')\n",
        "print(\"Averaging Model MAE:\", Avg_mod)\n",
        "print(\"Averaging Model MSE:\", Avg_mod1)\n",
        "\n",
        "############################################# Boosting Method\n",
        "#############################################\n",
        "\n",
        "# importing utility modules\n",
        "import pandas as pd\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        " \n",
        "# initializing the boosting module with default parameters\n",
        "model = GradientBoostingRegressor() #163\n",
        " \n",
        "# training the model on the train dataset\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Boost_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Boost_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('BOOSTING')\n",
        "print(\"Averaging Model MAE:\", Boost_mod)\n",
        "print(\"Averaging Model MSE:\", Boost_mod1)\n",
        "print('BAGGING')\n",
        "\n",
        "################################################# Bagging Method\n",
        "#################################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "import xgboost as xgb\n",
        " \n",
        "# importing bagging module\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "\n",
        "# initializing the bagging model using XGboost as base model with default parameters\n",
        "model = BaggingRegressor(base_estimator=xgb.XGBRegressor())\n",
        " \n",
        "# training model\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Bag_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Bag_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print(\"Bagging Model MAE:\", Bag_mod)\n",
        "print(\"Bagging Model MSE:\", Bag_mod1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uyKrGQ7KvXQA",
        "outputId": "b24566e0-c791-4724-9f08-d64f37c92bd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'sklearn.ensemble._forest.RandomForestRegressor'>\n",
            "0.08189410395070161 \n",
            " 0.2390386199512356\n",
            "<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>\n",
            "0.07073947025075637 \n",
            " 0.2332372468868417\n",
            "<class 'sklearn.svm._classes.SVR'>\n",
            "0.08067258327719384 \n",
            " 0.24035986893075797\n",
            "AVERAGING\n",
            "Averaging Model MAE: 0.2336954760934393\n",
            "Averaging Model MSE: 0.06930084518220624\n",
            "BOOSTING\n",
            "Averaging Model MAE: 0.23323724688684144\n",
            "Averaging Model MSE: 0.07073947025075628\n",
            "BAGGING\n",
            "[02:02:50] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[02:02:58] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[02:03:05] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[02:03:13] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[02:03:20] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[02:03:28] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[02:03:35] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[02:03:43] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[02:03:50] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[02:03:58] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "Bagging Model MAE: 0.2336692807590608\n",
            "Bagging Model MSE: 0.07056252535171778\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Fire size 1-5"
      ],
      "metadata": {
        "id": "pQth3nwNrKxo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "########## Spliting by size\n",
        "\n",
        "Size_sample20 = Fires_sample[(Fires_sample[\"FIRE_SIZE\"] > 1) & (Fires_sample[\"FIRE_SIZE\"] <= 5)]\n",
        "\n",
        "########### Spliting by month only needed when switching seasons\n",
        "Season_sample20 = Size_sample20[Size_sample20['DISCOVERY_MONTH'] <= 3]\n",
        "# Season_sample50 = Size_sample50[(Size_sample50[\"DISCOVERY_MONTH\"] > 1) & (Size_sample10[\"DISCOVERY_MONTH\"] <= 3)]\n",
        "\n",
        "################ Train test split\n",
        "\n",
        "Fires12 = Season_sample20[Season_sample20['FIRE_YEAR'] <= 2017].copy()\n",
        "Fires13 = Season_sample20[Season_sample20['FIRE_YEAR'] > 2017].copy()\n",
        "X_train = Fires12.loc[:, Fires12.columns != 'FIRE_SIZE']\n",
        "y_train = Fires12['FIRE_SIZE'].copy()\n",
        "X_test = Fires13.loc[:, Fires13.columns != 'FIRE_SIZE']\n",
        "y_test = Fires13['FIRE_SIZE'].copy()\n",
        "\n",
        "############################################ Testing best methods\n",
        "\n",
        "model_list = [RandomForestRegressor, GradientBoostingRegressor, SVR] #GaussianNB, \n",
        "model_scores = dict()\n",
        "for model in model_list:\n",
        "  # fit model to data\n",
        "  print(model)\n",
        "  regressor = model().fit(X_train,y_train)\n",
        "  y_pred = regressor.predict(X_test)\n",
        "  model_mse = mean_squared_error(y_pred,y_test)\n",
        "  model_mae = mean_absolute_error(y_pred,y_test)\n",
        "  model_scores[model] = (regressor,y_pred,model_mse,model_mae)\n",
        "  print(model_mse,'\\n',model_mae)\n",
        "\n",
        "############################################# Averaging Method\n",
        "#############################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import xgboost as xgb\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        " \n",
        "# initializing all the model objects with default parameters\n",
        "model_1 = GradientBoostingRegressor()\n",
        "model_2 = SVR()\n",
        "model_3 = RandomForestRegressor()\n",
        " \n",
        "# training all the model on the training dataset\n",
        "model_1.fit(X_train, y_train)\n",
        "model_2.fit(X_train, y_train)\n",
        "model_3.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the validation dataset\n",
        "pred_1 = model_1.predict(X_test)\n",
        "pred_2 = model_2.predict(X_test)\n",
        "pred_3 = model_3.predict(X_test)\n",
        " \n",
        "# final prediction after averaging on the prediction of all 3 models\n",
        "pred_final = (pred_1+pred_2+pred_3)/3.0\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "mod1 = (mean_absolute_error(pred_1, y_test))\n",
        "mod2 = (mean_absolute_error(pred_2, y_test))\n",
        "mod3 = (mean_absolute_error(pred_3, y_test))\n",
        "#print(mean_absolute_error(pred_final, y_test))  ##switch Y_pred and test\n",
        "#Averaging_model = (mod1+mod2+mod3)/3.0\n",
        "Avg_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Avg_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('AVERAGING')\n",
        "print(\"Averaging Model MAE:\", Avg_mod)\n",
        "print(\"Averaging Model MSE:\", Avg_mod1)\n",
        "\n",
        "############################################# Boosting Method\n",
        "#############################################\n",
        "\n",
        "# importing utility modules\n",
        "import pandas as pd\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        " \n",
        "# initializing the boosting module with default parameters\n",
        "model = GradientBoostingRegressor() #163\n",
        " \n",
        "# training the model on the train dataset\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Boost_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Boost_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('BOOSTING')\n",
        "print(\"Averaging Model MAE:\", Boost_mod)\n",
        "print(\"Averaging Model MSE:\", Boost_mod1)\n",
        "print('BAGGING')\n",
        "\n",
        "################################################# Bagging Method\n",
        "#################################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "import xgboost as xgb\n",
        " \n",
        "# importing bagging module\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "\n",
        "# initializing the bagging model using XGboost as base model with default parameters\n",
        "model = BaggingRegressor(base_estimator=xgb.XGBRegressor())\n",
        " \n",
        "# training model\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Bag_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Bag_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print(\"Bagging Model MAE:\", Bag_mod)\n",
        "print(\"Bagging Model MSE:\", Bag_mod1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_KPb6GxkrRAi",
        "outputId": "b4294f7f-baea-4c8a-cc65-4147b7129660"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'sklearn.ensemble._forest.RandomForestRegressor'>\n",
            "1.672299604081633 \n",
            " 1.135734693877551\n",
            "<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>\n",
            "1.5190495674070537 \n",
            " 1.055176075980878\n",
            "<class 'sklearn.svm._classes.SVR'>\n",
            "1.2732615018986644 \n",
            " 0.9719005170825763\n",
            "AVERAGING\n",
            "Averaging Model MAE: 1.0473532611823853\n",
            "Averaging Model MSE: 1.4564219317416656\n",
            "BOOSTING\n",
            "Averaging Model MAE: 1.055176075980878\n",
            "Averaging Model MSE: 1.5190495674070539\n",
            "BAGGING\n",
            "[04:26:02] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[04:26:02] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[04:26:02] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[04:26:03] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[04:26:03] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[04:26:03] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[04:26:03] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[04:26:03] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[04:26:03] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[04:26:03] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "Bagging Model MAE: 1.0313036374851148\n",
            "Bagging Model MSE: 1.4280168331384564\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Fire size 5-20"
      ],
      "metadata": {
        "id": "rDfX2m04xHt5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "########## Spliting by size\n",
        "\n",
        "Size_sample30 = Fires_sample[(Fires_sample[\"FIRE_SIZE\"] > 5) & (Fires_sample[\"FIRE_SIZE\"] <= 20)]\n",
        "\n",
        "########### Spliting by month only needed when switching seasons\n",
        "Season_sample30 = Size_sample30[Size_sample30['DISCOVERY_MONTH'] <= 3]\n",
        "# Season_sample50 = Size_sample50[(Size_sample50[\"DISCOVERY_MONTH\"] > 1) & (Size_sample10[\"DISCOVERY_MONTH\"] <= 3)]\n",
        "\n",
        "################ Train test split\n",
        "\n",
        "Fires12 = Season_sample30[Season_sample30['FIRE_YEAR'] <= 2017].copy()\n",
        "Fires13 = Season_sample30[Season_sample30['FIRE_YEAR'] > 2017].copy()\n",
        "X_train = Fires12.loc[:, Fires12.columns != 'FIRE_SIZE']\n",
        "y_train = Fires12['FIRE_SIZE'].copy()\n",
        "X_test = Fires13.loc[:, Fires13.columns != 'FIRE_SIZE']\n",
        "y_test = Fires13['FIRE_SIZE'].copy()\n",
        "\n",
        "############################################ Testing best methods\n",
        "\n",
        "model_list = [RandomForestRegressor, GradientBoostingRegressor, SVR] #GaussianNB, \n",
        "model_scores = dict()\n",
        "for model in model_list:\n",
        "  # fit model to data\n",
        "  print(model)\n",
        "  regressor = model().fit(X_train,y_train)\n",
        "  y_pred = regressor.predict(X_test)\n",
        "  model_mse = mean_squared_error(y_pred,y_test)\n",
        "  model_mae = mean_absolute_error(y_pred,y_test)\n",
        "  model_scores[model] = (regressor,y_pred,model_mse,model_mae)\n",
        "  print(model_mse,'\\n',model_mae)\n",
        "\n",
        "############################################# Averaging Method\n",
        "#############################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import xgboost as xgb\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        " \n",
        "# initializing all the model objects with default parameters\n",
        "model_1 = GradientBoostingRegressor()\n",
        "model_2 = SVR()\n",
        "model_3 = RandomForestRegressor()\n",
        " \n",
        "# training all the model on the training dataset\n",
        "model_1.fit(X_train, y_train)\n",
        "model_2.fit(X_train, y_train)\n",
        "model_3.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the validation dataset\n",
        "pred_1 = model_1.predict(X_test)\n",
        "pred_2 = model_2.predict(X_test)\n",
        "pred_3 = model_3.predict(X_test)\n",
        " \n",
        "# final prediction after averaging on the prediction of all 3 models\n",
        "pred_final = (pred_1+pred_2+pred_3)/3.0\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "mod1 = (mean_absolute_error(pred_1, y_test))\n",
        "mod2 = (mean_absolute_error(pred_2, y_test))\n",
        "mod3 = (mean_absolute_error(pred_3, y_test))\n",
        "#print(mean_absolute_error(pred_final, y_test))  ##switch Y_pred and test\n",
        "#Averaging_model = (mod1+mod2+mod3)/3.0\n",
        "Avg_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Avg_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('AVERAGING')\n",
        "print(\"Averaging Model MAE:\", Avg_mod)\n",
        "print(\"Averaging Model MSE:\", Avg_mod1)\n",
        "\n",
        "############################################# Boosting Method\n",
        "#############################################\n",
        "\n",
        "# importing utility modules\n",
        "import pandas as pd\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        " \n",
        "# initializing the boosting module with default parameters\n",
        "model = GradientBoostingRegressor() #163\n",
        " \n",
        "# training the model on the train dataset\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Boost_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Boost_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('BOOSTING')\n",
        "print(\"Averaging Model MAE:\", Boost_mod)\n",
        "print(\"Averaging Model MSE:\", Boost_mod1)\n",
        "print('BAGGING')\n",
        "\n",
        "################################################# Bagging Method\n",
        "#################################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "import xgboost as xgb\n",
        " \n",
        "# importing bagging module\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "\n",
        "# initializing the bagging model using XGboost as base model with default parameters\n",
        "model = BaggingRegressor(base_estimator=xgb.XGBRegressor())\n",
        " \n",
        "# training model\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Bag_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Bag_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print(\"Bagging Model MAE:\", Bag_mod)\n",
        "print(\"Bagging Model MSE:\", Bag_mod1)"
      ],
      "metadata": {
        "id": "x_atifgiw-2-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6870a357-c55f-413d-f142-fb29dd2cc96e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'sklearn.ensemble._forest.RandomForestRegressor'>\n",
            "20.752207649821845 \n",
            " 3.854811042813456\n",
            "<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>\n",
            "20.91077641688581 \n",
            " 3.908607998223302\n",
            "<class 'sklearn.svm._classes.SVR'>\n",
            "23.034609006181565 \n",
            " 3.820908999201525\n",
            "AVERAGING\n",
            "Averaging Model MAE: 3.8474502197432763\n",
            "Averaging Model MSE: 20.430687086919335\n",
            "BOOSTING\n",
            "Averaging Model MAE: 3.9086079982233026\n",
            "Averaging Model MSE: 20.91077641688581\n",
            "BAGGING\n",
            "[04:27:28] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[04:27:29] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[04:27:29] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[04:27:30] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[04:27:31] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[04:27:31] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[04:27:32] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[04:27:33] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[04:27:33] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[04:27:34] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "Bagging Model MAE: 3.9363564705105007\n",
            "Bagging Model MSE: 20.852345726672773\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Fire size 20-50"
      ],
      "metadata": {
        "id": "dAWSsoX_xFb2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "########## Spliting by size\n",
        "\n",
        "Size_sample40 = Fires_sample[(Fires_sample[\"FIRE_SIZE\"] > 20) & (Fires_sample[\"FIRE_SIZE\"] <= 50)]\n",
        "\n",
        "########### Spliting by month only needed when switching seasons\n",
        "Season_sample40 = Size_sample40[Size_sample40['DISCOVERY_MONTH'] <= 3]\n",
        "# Season_sample50 = Size_sample50[(Size_sample50[\"DISCOVERY_MONTH\"] > 1) & (Size_sample10[\"DISCOVERY_MONTH\"] <= 3)]\n",
        "\n",
        "################ Train test split\n",
        "\n",
        "Fires12 = Season_sample40[Season_sample40['FIRE_YEAR'] <= 2017].copy()\n",
        "Fires13 = Season_sample40[Season_sample40['FIRE_YEAR'] > 2017].copy()\n",
        "X_train = Fires12.loc[:, Fires12.columns != 'FIRE_SIZE']\n",
        "y_train = Fires12['FIRE_SIZE'].copy()\n",
        "X_test = Fires13.loc[:, Fires13.columns != 'FIRE_SIZE']\n",
        "y_test = Fires13['FIRE_SIZE'].copy()\n",
        "\n",
        "############################################ Testing best methods\n",
        "\n",
        "model_list = [RandomForestRegressor, GradientBoostingRegressor, SVR] #GaussianNB, \n",
        "model_scores = dict()\n",
        "for model in model_list:\n",
        "  # fit model to data\n",
        "  print(model)\n",
        "  regressor = model().fit(X_train,y_train)\n",
        "  y_pred = regressor.predict(X_test)\n",
        "  model_mse = mean_squared_error(y_pred,y_test)\n",
        "  model_mae = mean_absolute_error(y_pred,y_test)\n",
        "  model_scores[model] = (regressor,y_pred,model_mse,model_mae)\n",
        "  print(model_mse,'\\n',model_mae)\n",
        "\n",
        "############################################# Averaging Method\n",
        "#############################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import xgboost as xgb\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        " \n",
        "# initializing all the model objects with default parameters\n",
        "model_1 = GradientBoostingRegressor()\n",
        "model_2 = SVR()\n",
        "model_3 = RandomForestRegressor()\n",
        " \n",
        "# training all the model on the training dataset\n",
        "model_1.fit(X_train, y_train)\n",
        "model_2.fit(X_train, y_train)\n",
        "model_3.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the validation dataset\n",
        "pred_1 = model_1.predict(X_test)\n",
        "pred_2 = model_2.predict(X_test)\n",
        "pred_3 = model_3.predict(X_test)\n",
        " \n",
        "# final prediction after averaging on the prediction of all 3 models\n",
        "pred_final = (pred_1+pred_2+pred_3)/3.0\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "mod1 = (mean_absolute_error(pred_1, y_test))\n",
        "mod2 = (mean_absolute_error(pred_2, y_test))\n",
        "mod3 = (mean_absolute_error(pred_3, y_test))\n",
        "#print(mean_absolute_error(pred_final, y_test))  ##switch Y_pred and test\n",
        "#Averaging_model = (mod1+mod2+mod3)/3.0\n",
        "Avg_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Avg_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('AVERAGING')\n",
        "print(\"Averaging Model MAE:\", Avg_mod)\n",
        "print(\"Averaging Model MSE:\", Avg_mod1)\n",
        "\n",
        "############################################# Boosting Method\n",
        "#############################################\n",
        "\n",
        "# importing utility modules\n",
        "import pandas as pd\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        " \n",
        "# initializing the boosting module with default parameters\n",
        "model = GradientBoostingRegressor() #163\n",
        " \n",
        "# training the model on the train dataset\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Boost_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Boost_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('BOOSTING')\n",
        "print(\"Averaging Model MAE:\", Boost_mod)\n",
        "print(\"Averaging Model MSE:\", Boost_mod1)\n",
        "print('BAGGING')\n",
        "\n",
        "################################################# Bagging Method\n",
        "#################################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "import xgboost as xgb\n",
        " \n",
        "# importing bagging module\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "\n",
        "# initializing the bagging model using XGboost as base model with default parameters\n",
        "model = BaggingRegressor(base_estimator=xgb.XGBRegressor())\n",
        " \n",
        "# training model\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Bag_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Bag_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print(\"Bagging Model MAE:\", Bag_mod)\n",
        "print(\"Bagging Model MSE:\", Bag_mod1)"
      ],
      "metadata": {
        "id": "o_eAPzcUw-f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a17b1992-7ca9-4842-d28d-d267e9479219"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'sklearn.ensemble._forest.RandomForestRegressor'>\n",
            "92.52435220223626 \n",
            " 8.15852220531009\n",
            "<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>\n",
            "82.88923984933524 \n",
            " 7.914675664147711\n",
            "<class 'sklearn.svm._classes.SVR'>\n",
            "99.92957871754439 \n",
            " 7.895132416223108\n",
            "AVERAGING\n",
            "Averaging Model MAE: 7.86993289347098\n",
            "Averaging Model MSE: 86.04506310181765\n",
            "BOOSTING\n",
            "Averaging Model MAE: 7.91467566414771\n",
            "Averaging Model MSE: 82.88923984933525\n",
            "BAGGING\n",
            "[04:32:22] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[04:32:24] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[04:32:25] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[04:32:27] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[04:32:28] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[04:32:30] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[04:32:31] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[04:32:33] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[04:32:34] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[04:32:36] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "Bagging Model MAE: 7.926490347990888\n",
            "Bagging Model MSE: 82.85634609586917\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Firse size 50-200"
      ],
      "metadata": {
        "id": "WQT8UZ1LxDPV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "########## Spliting by size\n",
        "\n",
        "Size_sample50 = Fires_sample[(Fires_sample[\"FIRE_SIZE\"] > 50) & (Fires_sample[\"FIRE_SIZE\"] <= 200)]\n",
        "\n",
        "########### Spliting by month only needed when switching seasons\n",
        "Season_sample50 = Size_sample50[Size_sample50['DISCOVERY_MONTH'] <= 3]\n",
        "# Season_sample50 = Size_sample50[(Size_sample50[\"DISCOVERY_MONTH\"] > 1) & (Size_sample10[\"DISCOVERY_MONTH\"] <= 3)]\n",
        "\n",
        "################ Train test split\n",
        "\n",
        "Fires12 = Season_sample50[Season_sample50['FIRE_YEAR'] <= 2017].copy()\n",
        "Fires13 = Season_sample50[Season_sample50['FIRE_YEAR'] > 2017].copy()\n",
        "X_train = Fires12.loc[:, Fires12.columns != 'FIRE_SIZE']\n",
        "y_train = Fires12['FIRE_SIZE'].copy()\n",
        "X_test = Fires13.loc[:, Fires13.columns != 'FIRE_SIZE']\n",
        "y_test = Fires13['FIRE_SIZE'].copy()\n",
        "\n",
        "############################################ Testing best methods\n",
        "\n",
        "model_list = [RandomForestRegressor, GradientBoostingRegressor, SVR] #GaussianNB, \n",
        "model_scores = dict()\n",
        "for model in model_list:\n",
        "  # fit model to data\n",
        "  print(model)\n",
        "  regressor = model().fit(X_train,y_train)\n",
        "  y_pred = regressor.predict(X_test)\n",
        "  model_mse = mean_squared_error(y_pred,y_test)\n",
        "  model_mae = mean_absolute_error(y_pred,y_test)\n",
        "  model_scores[model] = (regressor,y_pred,model_mse,model_mae)\n",
        "  print(model_mse,'\\n',model_mae)\n",
        "\n",
        "############################################# Averaging Method\n",
        "#############################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import xgboost as xgb\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        " \n",
        "# initializing all the model objects with default parameters\n",
        "model_1 = GradientBoostingRegressor()\n",
        "model_2 = SVR()\n",
        "model_3 = RandomForestRegressor()\n",
        " \n",
        "# training all the model on the training dataset\n",
        "model_1.fit(X_train, y_train)\n",
        "model_2.fit(X_train, y_train)\n",
        "model_3.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the validation dataset\n",
        "pred_1 = model_1.predict(X_test)\n",
        "pred_2 = model_2.predict(X_test)\n",
        "pred_3 = model_3.predict(X_test)\n",
        " \n",
        "# final prediction after averaging on the prediction of all 3 models\n",
        "pred_final = (pred_1+pred_2+pred_3)/3.0\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "mod1 = (mean_absolute_error(pred_1, y_test))\n",
        "mod2 = (mean_absolute_error(pred_2, y_test))\n",
        "mod3 = (mean_absolute_error(pred_3, y_test))\n",
        "#print(mean_absolute_error(pred_final, y_test))  ##switch Y_pred and test\n",
        "#Averaging_model = (mod1+mod2+mod3)/3.0\n",
        "Avg_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Avg_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('AVERAGING')\n",
        "print(\"Averaging Model MAE:\", Avg_mod)\n",
        "print(\"Averaging Model MSE:\", Avg_mod1)\n",
        "\n",
        "############################################# Boosting Method\n",
        "#############################################\n",
        "\n",
        "# importing utility modules\n",
        "import pandas as pd\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        " \n",
        "# initializing the boosting module with default parameters\n",
        "model = GradientBoostingRegressor() #163\n",
        " \n",
        "# training the model on the train dataset\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Boost_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Boost_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('BOOSTING')\n",
        "print(\"Averaging Model MAE:\", Boost_mod)\n",
        "print(\"Averaging Model MSE:\", Boost_mod1)\n",
        "print('BAGGING')\n",
        "\n",
        "################################################# Bagging Method\n",
        "#################################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "import xgboost as xgb\n",
        " \n",
        "# importing bagging module\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "\n",
        "# initializing the bagging model using XGboost as base model with default parameters\n",
        "model = BaggingRegressor(base_estimator=xgb.XGBRegressor())\n",
        " \n",
        "# training model\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Bag_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Bag_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print(\"Bagging Model MAE:\", Bag_mod)\n",
        "print(\"Bagging Model MSE:\", Bag_mod1)"
      ],
      "metadata": {
        "id": "ElY5Et7_xCEN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e74478b8-2764-4bdf-beb5-9d3db94e0254"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'sklearn.ensemble._forest.RandomForestRegressor'>\n",
            "1732.8769750890217 \n",
            " 33.400697834506666\n",
            "<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>\n",
            "1559.8206580109907 \n",
            " 31.259118804088654\n",
            "<class 'sklearn.svm._classes.SVR'>\n",
            "1864.5916332103398 \n",
            " 30.909458763081428\n",
            "AVERAGING\n",
            "Averaging Model MAE: 30.639319453496974\n",
            "Averaging Model MSE: 1593.9924406229557\n",
            "BOOSTING\n",
            "Averaging Model MAE: 31.259118804088647\n",
            "Averaging Model MSE: 1559.8206580109907\n",
            "BAGGING\n",
            "[04:33:54] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[04:33:55] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[04:33:56] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[04:33:57] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[04:33:58] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[04:33:59] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[04:34:00] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[04:34:00] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[04:34:01] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[04:34:02] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "Bagging Model MAE: 31.35396170917969\n",
            "Bagging Model MSE: 1560.5250017976161\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Fire size 200-1000"
      ],
      "metadata": {
        "id": "dCA4_C4Jxg5m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "########## Spliting by size\n",
        "\n",
        "Size_sample100 = Fires_sample[(Fires_sample[\"FIRE_SIZE\"] > 200) & (Fires_sample[\"FIRE_SIZE\"] <= 1000)]\n",
        "\n",
        "########### Spliting by month only needed when switching seasons\n",
        "Season_sample100 = Size_sample100[Size_sample100['DISCOVERY_MONTH'] <= 3]\n",
        "\n",
        "################ Train test split\n",
        "\n",
        "Fires12 = Season_sample100[Season_sample100['FIRE_YEAR'] <= 2017].copy()\n",
        "Fires13 = Season_sample100[Season_sample100['FIRE_YEAR'] > 2017].copy()\n",
        "X_train = Fires12.loc[:, Fires12.columns != 'FIRE_SIZE']\n",
        "y_train = Fires12['FIRE_SIZE'].copy()\n",
        "X_test = Fires13.loc[:, Fires13.columns != 'FIRE_SIZE']\n",
        "y_test = Fires13['FIRE_SIZE'].copy()\n",
        "\n",
        "############################################ Testing best methods\n",
        "\n",
        "model_list = [RandomForestRegressor, GradientBoostingRegressor, SVR] #GaussianNB, \n",
        "model_scores = dict()\n",
        "for model in model_list:\n",
        "  # fit model to data\n",
        "  print(model)\n",
        "  regressor = model().fit(X_train,y_train)\n",
        "  y_pred = regressor.predict(X_test)\n",
        "  model_mse = mean_squared_error(y_pred,y_test)\n",
        "  model_mae = mean_absolute_error(y_pred,y_test)\n",
        "  model_scores[model] = (regressor,y_pred,model_mse,model_mae)\n",
        "  print(model_mse,'\\n',model_mae)\n",
        "\n",
        "############################################# Averaging Method\n",
        "#############################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import xgboost as xgb\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        " \n",
        "# initializing all the model objects with default parameters\n",
        "model_1 = GradientBoostingRegressor()\n",
        "model_2 = SVR()\n",
        "model_3 = RandomForestRegressor()\n",
        " \n",
        "# training all the model on the training dataset\n",
        "model_1.fit(X_train, y_train)\n",
        "model_2.fit(X_train, y_train)\n",
        "model_3.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the validation dataset\n",
        "pred_1 = model_1.predict(X_test)\n",
        "pred_2 = model_2.predict(X_test)\n",
        "pred_3 = model_3.predict(X_test)\n",
        " \n",
        "# final prediction after averaging on the prediction of all 3 models\n",
        "pred_final = (pred_1+pred_2+pred_3)/3.0\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "mod1 = (mean_absolute_error(pred_1, y_test))\n",
        "mod2 = (mean_absolute_error(pred_2, y_test))\n",
        "mod3 = (mean_absolute_error(pred_3, y_test))\n",
        "#print(mean_absolute_error(pred_final, y_test))  ##switch Y_pred and test\n",
        "#Averaging_model = (mod1+mod2+mod3)/3.0\n",
        "Avg_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Avg_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('AVERAGING')\n",
        "print(\"Averaging Model MAE:\", Avg_mod)\n",
        "print(\"Averaging Model MSE:\", Avg_mod1)\n",
        "\n",
        "############################################# Boosting Method\n",
        "#############################################\n",
        "\n",
        "# importing utility modules\n",
        "import pandas as pd\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        " \n",
        "# initializing the boosting module with default parameters\n",
        "model = GradientBoostingRegressor() #163\n",
        " \n",
        "# training the model on the train dataset\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Boost_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Boost_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('BOOSTING')\n",
        "print(\"Averaging Model MAE:\", Boost_mod)\n",
        "print(\"Averaging Model MSE:\", Boost_mod1)\n",
        "print('BAGGING')\n",
        "\n",
        "################################################# Bagging Method\n",
        "#################################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "import xgboost as xgb\n",
        " \n",
        "# importing bagging module\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "\n",
        "# initializing the bagging model using XGboost as base model with default parameters\n",
        "model = BaggingRegressor(base_estimator=xgb.XGBRegressor())\n",
        " \n",
        "# training model\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Bag_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Bag_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print(\"Bagging Model MAE:\", Bag_mod)\n",
        "print(\"Bagging Model MSE:\", Bag_mod1)"
      ],
      "metadata": {
        "id": "jit52CJIxjXe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5a6f6ce-4bdc-4099-c3ce-1d55a1e44b78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'sklearn.ensemble._forest.RandomForestRegressor'>\n",
            "45038.223955671056 \n",
            " 177.25540335966386\n",
            "<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>\n",
            "39694.3857789167 \n",
            " 163.93019999274324\n",
            "<class 'sklearn.svm._classes.SVR'>\n",
            "49747.55845558034 \n",
            " 160.773420721565\n",
            "AVERAGING\n",
            "Averaging Model MAE: 160.29459006606731\n",
            "Averaging Model MSE: 40357.93087755803\n",
            "BOOSTING\n",
            "Averaging Model MAE: 163.74916300006865\n",
            "Averaging Model MSE: 39631.53261483312\n",
            "BAGGING\n",
            "[04:34:14] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[04:34:14] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[04:34:15] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[04:34:15] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[04:34:15] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[04:34:15] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[04:34:16] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[04:34:16] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[04:34:16] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[04:34:16] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "Bagging Model MAE: 164.44361963608682\n",
            "Bagging Model MSE: 40050.59658168496\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Fire size +1000"
      ],
      "metadata": {
        "id": "0e01znotA8RG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "######### Spliting by size\n",
        "\n",
        "Size_sampleinf = Fires_sample[Fires_sample['FIRE_SIZE'] > 1000]\n",
        "\n",
        "########### Spliting by month only needed when switching seasons\n",
        "Season_sampleinf = Size_sampleinf[Size_sampleinf['DISCOVERY_MONTH'] <= 3]\n",
        "# Season_sample50 = Size_sample50[(Size_sample50[\"DISCOVERY_MONTH\"] > 1) & (Size_sample10[\"DISCOVERY_MONTH\"] <= 3)]\n",
        "\n",
        "################ Train test split\n",
        "\n",
        "Fires12 = Season_sampleinf[Season_sampleinf['FIRE_YEAR'] <= 2017].copy()\n",
        "Fires13 = Season_sampleinf[Season_sampleinf['FIRE_YEAR'] > 2017].copy()\n",
        "X_train = Fires12.loc[:, Fires12.columns != 'FIRE_SIZE']\n",
        "y_train = Fires12['FIRE_SIZE'].copy()\n",
        "X_test = Fires13.loc[:, Fires13.columns != 'FIRE_SIZE']\n",
        "y_test = Fires13['FIRE_SIZE'].copy()\n",
        "\n",
        "############################################ Testing best methods\n",
        "\n",
        "model_list = [RandomForestRegressor, GradientBoostingRegressor, SVR] #GaussianNB, \n",
        "model_scores = dict()\n",
        "for model in model_list:\n",
        "  # fit model to data\n",
        "  print(model)\n",
        "  regressor = model().fit(X_train,y_train)\n",
        "  y_pred = regressor.predict(X_test)\n",
        "  model_mse = mean_squared_error(y_pred,y_test)\n",
        "  model_mae = mean_absolute_error(y_pred,y_test)\n",
        "  model_scores[model] = (regressor,y_pred,model_mse,model_mae)\n",
        "  print(model_mse,'\\n',model_mae)\n",
        "\n",
        "############################################# Averaging Method\n",
        "#############################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import xgboost as xgb\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        " \n",
        "# initializing all the model objects with default parameters\n",
        "model_1 = GradientBoostingRegressor()\n",
        "model_2 = SVR()\n",
        "model_3 = RandomForestRegressor()\n",
        " \n",
        "# training all the model on the training dataset\n",
        "model_1.fit(X_train, y_train)\n",
        "model_2.fit(X_train, y_train)\n",
        "model_3.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the validation dataset\n",
        "pred_1 = model_1.predict(X_test)\n",
        "pred_2 = model_2.predict(X_test)\n",
        "pred_3 = model_3.predict(X_test)\n",
        " \n",
        "# final prediction after averaging on the prediction of all 3 models\n",
        "pred_final = (pred_1+pred_2+pred_3)/3.0\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "mod1 = (mean_absolute_error(pred_1, y_test))\n",
        "mod2 = (mean_absolute_error(pred_2, y_test))\n",
        "mod3 = (mean_absolute_error(pred_3, y_test))\n",
        "#print(mean_absolute_error(pred_final, y_test))  ##switch Y_pred and test\n",
        "#Averaging_model = (mod1+mod2+mod3)/3.0\n",
        "Avg_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Avg_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('AVERAGING')\n",
        "print(\"Averaging Model MAE:\", Avg_mod)\n",
        "print(\"Averaging Model MSE:\", Avg_mod1)\n",
        "\n",
        "############################################# Boosting Method\n",
        "#############################################\n",
        "\n",
        "# importing utility modules\n",
        "import pandas as pd\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        " \n",
        "# initializing the boosting module with default parameters\n",
        "model = GradientBoostingRegressor() #163\n",
        " \n",
        "# training the model on the train dataset\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Boost_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Boost_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('BOOSTING')\n",
        "print(\"Averaging Model MAE:\", Boost_mod)\n",
        "print(\"Averaging Model MSE:\", Boost_mod1)\n",
        "print('BAGGING')\n",
        "\n",
        "################################################# Bagging Method\n",
        "#################################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "import xgboost as xgb\n",
        " \n",
        "# importing bagging module\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "\n",
        "# initializing the bagging model using XGboost as base model with default parameters\n",
        "model = BaggingRegressor(base_estimator=xgb.XGBRegressor())\n",
        " \n",
        "# training model\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Bag_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Bag_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print(\"Bagging Model MAE:\", Bag_mod)\n",
        "print(\"Bagging Model MSE:\", Bag_mod1)"
      ],
      "metadata": {
        "id": "efbViLmR28Tq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab1b39e6-9558-4221-a1c3-7e997de4b2c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'sklearn.ensemble._forest.RandomForestRegressor'>\n",
            "1760160552.3984818 \n",
            " 14033.806581195653\n",
            "<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>\n",
            "1672276260.165357 \n",
            " 14152.392874717281\n",
            "<class 'sklearn.svm._classes.SVR'>\n",
            "17239868.462554682 \n",
            " 2210.2780384775874\n",
            "AVERAGING\n",
            "Averaging Model MAE: 8755.572474437367\n",
            "Averaging Model MSE: 621380563.2899944\n",
            "BOOSTING\n",
            "Averaging Model MAE: 14536.996111947863\n",
            "Averaging Model MSE: 1760756314.320604\n",
            "BAGGING\n",
            "[04:34:19] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[04:34:19] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[04:34:19] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[04:34:19] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[04:34:19] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[04:34:20] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[04:34:20] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[04:34:20] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[04:34:20] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[04:34:20] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "Bagging Model MAE: 12414.850170261549\n",
            "Bagging Model MSE: 976839873.2315283\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Seasons 4-6"
      ],
      "metadata": {
        "id": "pPCtVcDtBBSk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Fire size 0-.25"
      ],
      "metadata": {
        "id": "yxgjNqZ-E8vm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "######## Spliting by size\n",
        "\n",
        "Size_sampleinf = Fires_sample[Fires_sample['FIRE_SIZE'] <= .25]\n",
        "\n",
        "########### Spliting by month only needed when switching seasons\n",
        "Season_sampleinf = Size_sampleinf[(Size_sampleinf[\"DISCOVERY_MONTH\"] > 3) & (Size_sampleinf[\"DISCOVERY_MONTH\"] <= 6)]\n",
        "\n",
        "################ Train test split\n",
        "\n",
        "Fires12 = Season_sampleinf[Season_sampleinf['FIRE_YEAR'] <= 2017].copy()\n",
        "Fires13 = Season_sampleinf[Season_sampleinf['FIRE_YEAR'] > 2017].copy()\n",
        "X_train = Fires12.loc[:, Fires12.columns != 'FIRE_SIZE']\n",
        "y_train = Fires12['FIRE_SIZE'].copy()\n",
        "X_test = Fires13.loc[:, Fires13.columns != 'FIRE_SIZE']\n",
        "y_test = Fires13['FIRE_SIZE'].copy()\n",
        "\n",
        "############################################ Testing best methods\n",
        "\n",
        "model_list = [RandomForestRegressor, GradientBoostingRegressor, SVR] #GaussianNB, \n",
        "model_scores = dict()\n",
        "for model in model_list:\n",
        "  # fit model to data\n",
        "  print(model)\n",
        "  regressor = model().fit(X_train,y_train)\n",
        "  y_pred = regressor.predict(X_test)\n",
        "  model_mse = mean_squared_error(y_pred,y_test)\n",
        "  model_mae = mean_absolute_error(y_pred,y_test)\n",
        "  model_scores[model] = (regressor,y_pred,model_mse,model_mae)\n",
        "  print(model_mse,'\\n',model_mae)\n",
        "\n",
        "############################################# Averaging Method\n",
        "#############################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import xgboost as xgb\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        " \n",
        "# initializing all the model objects with default parameters\n",
        "model_1 = GradientBoostingRegressor()\n",
        "model_2 = SVR()\n",
        "model_3 = RandomForestRegressor()\n",
        " \n",
        "# training all the model on the training dataset\n",
        "model_1.fit(X_train, y_train)\n",
        "model_2.fit(X_train, y_train)\n",
        "model_3.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the validation dataset\n",
        "pred_1 = model_1.predict(X_test)\n",
        "pred_2 = model_2.predict(X_test)\n",
        "pred_3 = model_3.predict(X_test)\n",
        " \n",
        "# final prediction after averaging on the prediction of all 3 models\n",
        "pred_final = (pred_1+pred_2+pred_3)/3.0\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "mod1 = (mean_absolute_error(pred_1, y_test))\n",
        "mod2 = (mean_absolute_error(pred_2, y_test))\n",
        "mod3 = (mean_absolute_error(pred_3, y_test))\n",
        "#print(mean_absolute_error(pred_final, y_test))  ##switch Y_pred and test\n",
        "#Averaging_model = (mod1+mod2+mod3)/3.0\n",
        "Avg_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Avg_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('AVERAGING')\n",
        "print(\"Averaging Model MAE:\", Avg_mod)\n",
        "print(\"Averaging Model MSE:\", Avg_mod1)\n",
        "\n",
        "############################################# Boosting Method\n",
        "#############################################\n",
        "\n",
        "# importing utility modules\n",
        "import pandas as pd\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        " \n",
        "# initializing the boosting module with default parameters\n",
        "model = GradientBoostingRegressor() #163\n",
        " \n",
        "# training the model on the train dataset\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Boost_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Boost_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('BOOSTING')\n",
        "print(\"Averaging Model MAE:\", Boost_mod)\n",
        "print(\"Averaging Model MSE:\", Boost_mod1)\n",
        "print('BAGGING')\n",
        "\n",
        "################################################# Bagging Method\n",
        "#################################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "import xgboost as xgb\n",
        " \n",
        "# importing bagging module\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "\n",
        "# initializing the bagging model using XGboost as base model with default parameters\n",
        "model = BaggingRegressor(base_estimator=xgb.XGBRegressor())\n",
        " \n",
        "# training model\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Bag_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Bag_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print(\"Bagging Model MAE:\", Bag_mod)\n",
        "print(\"Bagging Model MSE:\", Bag_mod1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98g7yyCstSuL",
        "outputId": "fc79f4fd-f44d-4c71-b7d9-d85a3321f459"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'sklearn.ensemble._forest.RandomForestRegressor'>\n",
            "0.0036709822084798026 \n",
            " 0.0437645280131826\n",
            "<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>\n",
            "0.003317995165331379 \n",
            " 0.04079699452006383\n",
            "<class 'sklearn.svm._classes.SVR'>\n",
            "0.005661555210601395 \n",
            " 0.06191366115496232\n",
            "AVERAGING\n",
            "Averaging Model MAE: 0.04730669762470511\n",
            "Averaging Model MSE: 0.003617313394911295\n",
            "BOOSTING\n",
            "Averaging Model MAE: 0.04079699452006381\n",
            "Averaging Model MSE: 0.003317995165331379\n",
            "BAGGING\n",
            "[02:02:38] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[02:02:42] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[02:02:45] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[02:02:48] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[02:02:52] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[02:02:55] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[02:02:58] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[02:03:02] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[02:03:05] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[02:03:08] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "Bagging Model MAE: 0.04032206484609601\n",
            "Bagging Model MSE: 0.0033046205060251847\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Fire size .25-1\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DNy2KsqXE-4Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "########## Spliting by size\n",
        "\n",
        "Size_sample1 = Fires_sample[(Fires_sample[\"FIRE_SIZE\"] > .25) & (Fires_sample[\"FIRE_SIZE\"] <= 1)]\n",
        "\n",
        "########### Spliting by month only needed when switching seasons\n",
        "Season_sample1 = Size_sample1[(Size_sample1[\"DISCOVERY_MONTH\"] > 3) & (Size_sample1[\"DISCOVERY_MONTH\"] <= 6)]\n",
        "\n",
        "################ Train test split\n",
        "\n",
        "Fires12 = Season_sample1[Season_sample1['FIRE_YEAR'] <= 2017].copy()\n",
        "Fires13 = Season_sample1[Season_sample1['FIRE_YEAR'] > 2017].copy()\n",
        "X_train = Fires12.loc[:, Fires12.columns != 'FIRE_SIZE']\n",
        "y_train = Fires12['FIRE_SIZE'].copy()\n",
        "X_test = Fires13.loc[:, Fires13.columns != 'FIRE_SIZE']\n",
        "y_test = Fires13['FIRE_SIZE'].copy()\n",
        "\n",
        "############################################ Testing best methods\n",
        "\n",
        "model_list = [RandomForestRegressor, GradientBoostingRegressor, SVR] #GaussianNB, \n",
        "model_scores = dict()\n",
        "for model in model_list:\n",
        "  # fit model to data\n",
        "  print(model)\n",
        "  regressor = model().fit(X_train,y_train)\n",
        "  y_pred = regressor.predict(X_test)\n",
        "  model_mse = mean_squared_error(y_pred,y_test)\n",
        "  model_mae = mean_absolute_error(y_pred,y_test)\n",
        "  model_scores[model] = (regressor,y_pred,model_mse,model_mae)\n",
        "  print(model_mse,'\\n',model_mae)\n",
        "\n",
        "############################################# Averaging Method\n",
        "#############################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import xgboost as xgb\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        " \n",
        "# initializing all the model objects with default parameters\n",
        "model_1 = GradientBoostingRegressor()\n",
        "model_2 = SVR()\n",
        "model_3 = RandomForestRegressor()\n",
        " \n",
        "# training all the model on the training dataset\n",
        "model_1.fit(X_train, y_train)\n",
        "model_2.fit(X_train, y_train)\n",
        "model_3.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the validation dataset\n",
        "pred_1 = model_1.predict(X_test)\n",
        "pred_2 = model_2.predict(X_test)\n",
        "pred_3 = model_3.predict(X_test)\n",
        " \n",
        "# final prediction after averaging on the prediction of all 3 models\n",
        "pred_final = (pred_1+pred_2+pred_3)/3.0\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "mod1 = (mean_absolute_error(pred_1, y_test))\n",
        "mod2 = (mean_absolute_error(pred_2, y_test))\n",
        "mod3 = (mean_absolute_error(pred_3, y_test))\n",
        "#print(mean_absolute_error(pred_final, y_test))  ##switch Y_pred and test\n",
        "#Averaging_model = (mod1+mod2+mod3)/3.0\n",
        "Avg_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Avg_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('AVERAGING')\n",
        "print(\"Averaging Model MAE:\", Avg_mod)\n",
        "print(\"Averaging Model MSE:\", Avg_mod1)\n",
        "\n",
        "############################################# Boosting Method\n",
        "#############################################\n",
        "\n",
        "# importing utility modules\n",
        "import pandas as pd\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        " \n",
        "# initializing the boosting module with default parameters\n",
        "model = GradientBoostingRegressor() #163\n",
        " \n",
        "# training the model on the train dataset\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Boost_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Boost_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('BOOSTING')\n",
        "print(\"Averaging Model MAE:\", Boost_mod)\n",
        "print(\"Averaging Model MSE:\", Boost_mod1)\n",
        "print('BAGGING')\n",
        "\n",
        "################################################# Bagging Method\n",
        "#################################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "import xgboost as xgb\n",
        " \n",
        "# importing bagging module\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "\n",
        "# initializing the bagging model using XGboost as base model with default parameters\n",
        "model = BaggingRegressor(base_estimator=xgb.XGBRegressor())\n",
        " \n",
        "# training model\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Bag_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Bag_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print(\"Bagging Model MAE:\", Bag_mod)\n",
        "print(\"Bagging Model MSE:\", Bag_mod1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bdie-wTABEyd",
        "outputId": "6e276cd6-d6c3-4ed3-c6e0-8454cf453584"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'sklearn.ensemble._forest.RandomForestRegressor'>\n",
            "0.08136899086194883 \n",
            " 0.2462859805794333\n",
            "<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>\n",
            "0.06940180322046133 \n",
            " 0.23940951552274528\n",
            "<class 'sklearn.svm._classes.SVR'>\n",
            "0.07120780080914729 \n",
            " 0.24477778083028576\n",
            "AVERAGING\n",
            "Averaging Model MAE: 0.23902342891990644\n",
            "Averaging Model MSE: 0.06765737572922881\n",
            "BOOSTING\n",
            "Averaging Model MAE: 0.23940951552274525\n",
            "Averaging Model MSE: 0.06940180322046133\n",
            "BAGGING\n",
            "[02:14:22] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[02:14:25] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[02:14:27] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[02:14:29] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[02:14:32] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[02:14:34] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[02:14:36] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[02:14:39] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[02:14:41] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[02:14:43] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "Bagging Model MAE: 0.24043793802113336\n",
            "Bagging Model MSE: 0.07003740121657126\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Fire size 1-5"
      ],
      "metadata": {
        "id": "Tr_OXlLBFXuM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "Size_sample10 = Fires_sample[(Fires_sample[\"FIRE_SIZE\"] > 1) & (Fires_sample[\"FIRE_SIZE\"] <= 5)]\n",
        "\n",
        "########### Spliting by month only needed when switching seasons\n",
        "Season_sample10 = Size_sample10[(Size_sample10[\"DISCOVERY_MONTH\"] > 3) & (Size_sample10[\"DISCOVERY_MONTH\"] <= 6)]\n",
        "\n",
        "################ Train test split\n",
        "\n",
        "Fires12 = Season_sample10[Season_sample10['FIRE_YEAR'] <= 2017].copy()\n",
        "Fires13 = Season_sample10[Season_sample10['FIRE_YEAR'] > 2017].copy()\n",
        "X_train = Fires12.loc[:, Fires12.columns != 'FIRE_SIZE']\n",
        "y_train = Fires12['FIRE_SIZE'].copy()\n",
        "X_test = Fires13.loc[:, Fires13.columns != 'FIRE_SIZE']\n",
        "y_test = Fires13['FIRE_SIZE'].copy()\n",
        "\n",
        "############################################ Testing best methods\n",
        "\n",
        "model_list = [RandomForestRegressor, GradientBoostingRegressor, SVR] #GaussianNB, \n",
        "model_scores = dict()\n",
        "for model in model_list:\n",
        "  # fit model to data\n",
        "  print(model)\n",
        "  regressor = model().fit(X_train,y_train)\n",
        "  y_pred = regressor.predict(X_test)\n",
        "  model_mse = mean_squared_error(y_pred,y_test)\n",
        "  model_mae = mean_absolute_error(y_pred,y_test)\n",
        "  model_scores[model] = (regressor,y_pred,model_mse,model_mae)\n",
        "  print(model_mse,'\\n',model_mae)\n",
        "\n",
        "############################################# Averaging Method\n",
        "#############################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import xgboost as xgb\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        " \n",
        "# initializing all the model objects with default parameters\n",
        "model_1 = GradientBoostingRegressor()\n",
        "model_2 = SVR()\n",
        "model_3 = RandomForestRegressor()\n",
        " \n",
        "# training all the model on the training dataset\n",
        "model_1.fit(X_train, y_train)\n",
        "model_2.fit(X_train, y_train)\n",
        "model_3.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the validation dataset\n",
        "pred_1 = model_1.predict(X_test)\n",
        "pred_2 = model_2.predict(X_test)\n",
        "pred_3 = model_3.predict(X_test)\n",
        " \n",
        "# final prediction after averaging on the prediction of all 3 models\n",
        "pred_final = (pred_1+pred_2+pred_3)/3.0\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "mod1 = (mean_absolute_error(pred_1, y_test))\n",
        "mod2 = (mean_absolute_error(pred_2, y_test))\n",
        "mod3 = (mean_absolute_error(pred_3, y_test))\n",
        "#print(mean_absolute_error(pred_final, y_test))  ##switch Y_pred and test\n",
        "#Averaging_model = (mod1+mod2+mod3)/3.0\n",
        "Avg_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Avg_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('AVERAGING')\n",
        "print(\"Averaging Model MAE:\", Avg_mod)\n",
        "print(\"Averaging Model MSE:\", Avg_mod1)\n",
        "\n",
        "############################################# Boosting Method\n",
        "#############################################\n",
        "\n",
        "# importing utility modules\n",
        "import pandas as pd\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        " \n",
        "# initializing the boosting module with default parameters\n",
        "model = GradientBoostingRegressor() #163\n",
        " \n",
        "# training the model on the train dataset\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Boost_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Boost_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('BOOSTING')\n",
        "print(\"Averaging Model MAE:\", Boost_mod)\n",
        "print(\"Averaging Model MSE:\", Boost_mod1)\n",
        "print('BAGGING')\n",
        "\n",
        "################################################# Bagging Method\n",
        "#################################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "import xgboost as xgb\n",
        " \n",
        "# importing bagging module\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "\n",
        "# initializing the bagging model using XGboost as base model with default parameters\n",
        "model = BaggingRegressor(base_estimator=xgb.XGBRegressor())\n",
        " \n",
        "# training model\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Bag_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Bag_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print(\"Bagging Model MAE:\", Bag_mod)\n",
        "print(\"Bagging Model MSE:\", Bag_mod1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVZklRnyBE_o",
        "outputId": "96e5d486-55b1-49d2-b94f-ca29f1bc2728"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'sklearn.ensemble._forest.RandomForestRegressor'>\n",
            "1.4943430628065908 \n",
            " 1.0298864322949117\n",
            "<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>\n",
            "1.3878969482867507 \n",
            " 1.00826601429895\n",
            "<class 'sklearn.svm._classes.SVR'>\n",
            "1.779584640158579 \n",
            " 0.9975064129396338\n",
            "AVERAGING\n",
            "Averaging Model MAE: 0.9833631103439054\n",
            "Averaging Model MSE: 1.4082379416117472\n",
            "BOOSTING\n",
            "Averaging Model MAE: 1.00826601429895\n",
            "Averaging Model MSE: 1.3878969482867507\n",
            "BAGGING\n",
            "[02:18:54] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[02:18:56] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[02:18:57] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[02:18:59] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[02:19:01] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[02:19:03] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[02:19:05] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[02:19:06] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[02:19:08] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[02:19:10] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "Bagging Model MAE: 1.0052692740476021\n",
            "Bagging Model MSE: 1.3810157399461633\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Fire size 5-20"
      ],
      "metadata": {
        "id": "DPybrnWtFZkJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "######### Spliting by size\n",
        "\n",
        "Size_sample20 = Fires_sample[(Fires_sample[\"FIRE_SIZE\"] > 5) & (Fires_sample[\"FIRE_SIZE\"] <= 20)]\n",
        "\n",
        "########### Spliting by month only needed when switching seasons\n",
        "Season_sample20 = Size_sample20[(Size_sample20[\"DISCOVERY_MONTH\"] > 3) & (Size_sample20[\"DISCOVERY_MONTH\"] <= 6)]\n",
        "\n",
        "################ Train test split\n",
        "\n",
        "Fires12 = Season_sample20[Season_sample20['FIRE_YEAR'] <= 2017].copy()\n",
        "Fires13 = Season_sample20[Season_sample20['FIRE_YEAR'] > 2017].copy()\n",
        "X_train = Fires12.loc[:, Fires12.columns != 'FIRE_SIZE']\n",
        "y_train = Fires12['FIRE_SIZE'].copy()\n",
        "X_test = Fires13.loc[:, Fires13.columns != 'FIRE_SIZE']\n",
        "y_test = Fires13['FIRE_SIZE'].copy()\n",
        "\n",
        "############################################ Testing best methods\n",
        "\n",
        "model_list = [RandomForestRegressor, GradientBoostingRegressor, SVR] #GaussianNB, \n",
        "model_scores = dict()\n",
        "for model in model_list:\n",
        "  # fit model to data\n",
        "  print(model)\n",
        "  regressor = model().fit(X_train,y_train)\n",
        "  y_pred = regressor.predict(X_test)\n",
        "  model_mse = mean_squared_error(y_pred,y_test)\n",
        "  model_mae = mean_absolute_error(y_pred,y_test)\n",
        "  model_scores[model] = (regressor,y_pred,model_mse,model_mae)\n",
        "  print(model_mse,'\\n',model_mae)\n",
        "\n",
        "############################################# Averaging Method\n",
        "#############################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import xgboost as xgb\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        " \n",
        "# initializing all the model objects with default parameters\n",
        "model_1 = GradientBoostingRegressor()\n",
        "model_2 = SVR()\n",
        "model_3 = RandomForestRegressor()\n",
        " \n",
        "# training all the model on the training dataset\n",
        "model_1.fit(X_train, y_train)\n",
        "model_2.fit(X_train, y_train)\n",
        "model_3.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the validation dataset\n",
        "pred_1 = model_1.predict(X_test)\n",
        "pred_2 = model_2.predict(X_test)\n",
        "pred_3 = model_3.predict(X_test)\n",
        " \n",
        "# final prediction after averaging on the prediction of all 3 models\n",
        "pred_final = (pred_1+pred_2+pred_3)/3.0\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "mod1 = (mean_absolute_error(pred_1, y_test))\n",
        "mod2 = (mean_absolute_error(pred_2, y_test))\n",
        "mod3 = (mean_absolute_error(pred_3, y_test))\n",
        "#print(mean_absolute_error(pred_final, y_test))  ##switch Y_pred and test\n",
        "#Averaging_model = (mod1+mod2+mod3)/3.0\n",
        "Avg_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Avg_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('AVERAGING')\n",
        "print(\"Averaging Model MAE:\", Avg_mod)\n",
        "print(\"Averaging Model MSE:\", Avg_mod1)\n",
        "\n",
        "############################################# Boosting Method\n",
        "#############################################\n",
        "\n",
        "# importing utility modules\n",
        "import pandas as pd\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        " \n",
        "# initializing the boosting module with default parameters\n",
        "model = GradientBoostingRegressor() #163\n",
        " \n",
        "# training the model on the train dataset\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Boost_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Boost_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('BOOSTING')\n",
        "print(\"Averaging Model MAE:\", Boost_mod)\n",
        "print(\"Averaging Model MSE:\", Boost_mod1)\n",
        "print('BAGGING')\n",
        "\n",
        "################################################# Bagging Method\n",
        "#################################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "import xgboost as xgb\n",
        " \n",
        "# importing bagging module\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "\n",
        "# initializing the bagging model using XGboost as base model with default parameters\n",
        "model = BaggingRegressor(base_estimator=xgb.XGBRegressor())\n",
        " \n",
        "# training model\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Bag_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Bag_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print(\"Bagging Model MAE:\", Bag_mod)\n",
        "print(\"Bagging Model MSE:\", Bag_mod1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15rHaN6eFOup",
        "outputId": "1ed29949-5a1d-4f03-e838-3a16187255e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'sklearn.ensemble._forest.RandomForestRegressor'>\n",
            "19.9199275171968 \n",
            " 3.7547236614121506\n",
            "<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>\n",
            "18.11683366070714 \n",
            " 3.619851926175402\n",
            "<class 'sklearn.svm._classes.SVR'>\n",
            "20.49173465872183 \n",
            " 3.527411026617758\n",
            "AVERAGING\n",
            "Averaging Model MAE: 3.598430351974079\n",
            "Averaging Model MSE: 18.471030087164085\n",
            "BOOSTING\n",
            "Averaging Model MAE: 3.6198519261754027\n",
            "Averaging Model MSE: 18.116833660707144\n",
            "BAGGING\n",
            "[02:20:04] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[02:20:05] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[02:20:06] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[02:20:06] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[02:20:07] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[02:20:08] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[02:20:09] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[02:20:09] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[02:20:10] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[02:20:11] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "Bagging Model MAE: 3.625909102829807\n",
            "Bagging Model MSE: 18.193506330257254\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Fire size 20-50"
      ],
      "metadata": {
        "id": "JJiYrJ7JFcjp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "########## Spliting by size\n",
        "\n",
        "Size_sample30 = Fires_sample[(Fires_sample[\"FIRE_SIZE\"] > 20) & (Fires_sample[\"FIRE_SIZE\"] <= 50)]\n",
        "\n",
        "########### Spliting by month only needed when switching seasons\n",
        "Season_sample30 = Size_sample30[(Size_sample30[\"DISCOVERY_MONTH\"] > 3) & (Size_sample30[\"DISCOVERY_MONTH\"] <= 6)]\n",
        "\n",
        "################ Train test split\n",
        "\n",
        "Fires12 = Season_sample30[Season_sample30['FIRE_YEAR'] <= 2017].copy()\n",
        "Fires13 = Season_sample30[Season_sample30['FIRE_YEAR'] > 2017].copy()\n",
        "X_train = Fires12.loc[:, Fires12.columns != 'FIRE_SIZE']\n",
        "y_train = Fires12['FIRE_SIZE'].copy()\n",
        "X_test = Fires13.loc[:, Fires13.columns != 'FIRE_SIZE']\n",
        "y_test = Fires13['FIRE_SIZE'].copy()\n",
        "\n",
        "############################################ Testing best methods\n",
        "\n",
        "model_list = [RandomForestRegressor, GradientBoostingRegressor, SVR] #GaussianNB, \n",
        "model_scores = dict()\n",
        "for model in model_list:\n",
        "  # fit model to data\n",
        "  print(model)\n",
        "  regressor = model().fit(X_train,y_train)\n",
        "  y_pred = regressor.predict(X_test)\n",
        "  model_mse = mean_squared_error(y_pred,y_test)\n",
        "  model_mae = mean_absolute_error(y_pred,y_test)\n",
        "  model_scores[model] = (regressor,y_pred,model_mse,model_mae)\n",
        "  print(model_mse,'\\n',model_mae)\n",
        "\n",
        "############################################# Averaging Method\n",
        "#############################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import xgboost as xgb\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        " \n",
        "# initializing all the model objects with default parameters\n",
        "model_1 = GradientBoostingRegressor()\n",
        "model_2 = SVR()\n",
        "model_3 = RandomForestRegressor()\n",
        " \n",
        "# training all the model on the training dataset\n",
        "model_1.fit(X_train, y_train)\n",
        "model_2.fit(X_train, y_train)\n",
        "model_3.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the validation dataset\n",
        "pred_1 = model_1.predict(X_test)\n",
        "pred_2 = model_2.predict(X_test)\n",
        "pred_3 = model_3.predict(X_test)\n",
        " \n",
        "# final prediction after averaging on the prediction of all 3 models\n",
        "pred_final = (pred_1+pred_2+pred_3)/3.0\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "mod1 = (mean_absolute_error(pred_1, y_test))\n",
        "mod2 = (mean_absolute_error(pred_2, y_test))\n",
        "mod3 = (mean_absolute_error(pred_3, y_test))\n",
        "#print(mean_absolute_error(pred_final, y_test))  ##switch Y_pred and test\n",
        "#Averaging_model = (mod1+mod2+mod3)/3.0\n",
        "Avg_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Avg_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('AVERAGING')\n",
        "print(\"Averaging Model MAE:\", Avg_mod)\n",
        "print(\"Averaging Model MSE:\", Avg_mod1)\n",
        "\n",
        "############################################# Boosting Method\n",
        "#############################################\n",
        "\n",
        "# importing utility modules\n",
        "import pandas as pd\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        " \n",
        "# initializing the boosting module with default parameters\n",
        "model = GradientBoostingRegressor() #163\n",
        " \n",
        "# training the model on the train dataset\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Boost_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Boost_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('BOOSTING')\n",
        "print(\"Averaging Model MAE:\", Boost_mod)\n",
        "print(\"Averaging Model MSE:\", Boost_mod1)\n",
        "print('BAGGING')\n",
        "\n",
        "################################################# Bagging Method\n",
        "#################################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "import xgboost as xgb\n",
        " \n",
        "# importing bagging module\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "\n",
        "# initializing the bagging model using XGboost as base model with default parameters\n",
        "model = BaggingRegressor(base_estimator=xgb.XGBRegressor())\n",
        " \n",
        "# training model\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Bag_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Bag_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print(\"Bagging Model MAE:\", Bag_mod)\n",
        "print(\"Bagging Model MSE:\", Bag_mod1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MaIiuIzCFOnv",
        "outputId": "c3f0bbbc-975b-4659-8833-afae6fe2f083"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'sklearn.ensemble._forest.RandomForestRegressor'>\n",
            "85.56307352995886 \n",
            " 7.579183539094649\n",
            "<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>\n",
            "82.27941313819622 \n",
            " 7.565882545728207\n",
            "<class 'sklearn.svm._classes.SVR'>\n",
            "95.38949815330739 \n",
            " 7.7473416303846365\n",
            "AVERAGING\n",
            "Averaging Model MAE: 7.491943911162066\n",
            "Averaging Model MSE: 81.87004927365875\n",
            "BOOSTING\n",
            "Averaging Model MAE: 7.565882545728208\n",
            "Averaging Model MSE: 82.27941313819622\n",
            "BAGGING\n",
            "[02:20:26] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[02:20:26] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[02:20:26] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[02:20:27] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[02:20:27] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[02:20:27] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[02:20:28] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[02:20:28] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[02:20:28] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[02:20:29] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "Bagging Model MAE: 7.565318093821836\n",
            "Bagging Model MSE: 80.81438981318526\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Fire size 50-200"
      ],
      "metadata": {
        "id": "foJpdFdlFavn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "########## Spliting by size\n",
        "\n",
        "Size_sample40 = Fires_sample[(Fires_sample[\"FIRE_SIZE\"] > 50) & (Fires_sample[\"FIRE_SIZE\"] <= 200)]\n",
        "\n",
        "########### Spliting by month only needed when switching seasons\n",
        "Season_sample40 = Size_sample40[(Size_sample40[\"DISCOVERY_MONTH\"] > 3) & (Size_sample40[\"DISCOVERY_MONTH\"] <= 6)]\n",
        "\n",
        "################ Train test split\n",
        "\n",
        "Fires12 = Season_sample40[Season_sample40['FIRE_YEAR'] <= 2017].copy()\n",
        "Fires13 = Season_sample40[Season_sample40['FIRE_YEAR'] > 2017].copy()\n",
        "X_train = Fires12.loc[:, Fires12.columns != 'FIRE_SIZE']\n",
        "y_train = Fires12['FIRE_SIZE'].copy()\n",
        "X_test = Fires13.loc[:, Fires13.columns != 'FIRE_SIZE']\n",
        "y_test = Fires13['FIRE_SIZE'].copy()\n",
        "\n",
        "############################################ Testing best methods\n",
        "\n",
        "model_list = [RandomForestRegressor, GradientBoostingRegressor, SVR] #GaussianNB, \n",
        "model_scores = dict()\n",
        "for model in model_list:\n",
        "  # fit model to data\n",
        "  print(model)\n",
        "  regressor = model().fit(X_train,y_train)\n",
        "  y_pred = regressor.predict(X_test)\n",
        "  model_mse = mean_squared_error(y_pred,y_test)\n",
        "  model_mae = mean_absolute_error(y_pred,y_test)\n",
        "  model_scores[model] = (regressor,y_pred,model_mse,model_mae)\n",
        "  print(model_mse,'\\n',model_mae)\n",
        "\n",
        "############################################# Averaging Method\n",
        "#############################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import xgboost as xgb\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        " \n",
        "# initializing all the model objects with default parameters\n",
        "model_1 = GradientBoostingRegressor()\n",
        "model_2 = SVR()\n",
        "model_3 = RandomForestRegressor()\n",
        " \n",
        "# training all the model on the training dataset\n",
        "model_1.fit(X_train, y_train)\n",
        "model_2.fit(X_train, y_train)\n",
        "model_3.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the validation dataset\n",
        "pred_1 = model_1.predict(X_test)\n",
        "pred_2 = model_2.predict(X_test)\n",
        "pred_3 = model_3.predict(X_test)\n",
        " \n",
        "# final prediction after averaging on the prediction of all 3 models\n",
        "pred_final = (pred_1+pred_2+pred_3)/3.0\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "mod1 = (mean_absolute_error(pred_1, y_test))\n",
        "mod2 = (mean_absolute_error(pred_2, y_test))\n",
        "mod3 = (mean_absolute_error(pred_3, y_test))\n",
        "#print(mean_absolute_error(pred_final, y_test))  ##switch Y_pred and test\n",
        "#Averaging_model = (mod1+mod2+mod3)/3.0\n",
        "Avg_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Avg_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('AVERAGING')\n",
        "print(\"Averaging Model MAE:\", Avg_mod)\n",
        "print(\"Averaging Model MSE:\", Avg_mod1)\n",
        "\n",
        "############################################# Boosting Method\n",
        "#############################################\n",
        "\n",
        "# importing utility modules\n",
        "import pandas as pd\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        " \n",
        "# initializing the boosting module with default parameters\n",
        "model = GradientBoostingRegressor() #163\n",
        " \n",
        "# training the model on the train dataset\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Boost_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Boost_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('BOOSTING')\n",
        "print(\"Averaging Model MAE:\", Boost_mod)\n",
        "print(\"Averaging Model MSE:\", Boost_mod1)\n",
        "print('BAGGING')\n",
        "\n",
        "################################################# Bagging Method\n",
        "#################################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "import xgboost as xgb\n",
        " \n",
        "# importing bagging module\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "\n",
        "# initializing the bagging model using XGboost as base model with default parameters\n",
        "model = BaggingRegressor(base_estimator=xgb.XGBRegressor())\n",
        " \n",
        "# training model\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Bag_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Bag_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print(\"Bagging Model MAE:\", Bag_mod)\n",
        "print(\"Bagging Model MSE:\", Bag_mod1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lcuB5evJFOgC",
        "outputId": "595fc1ab-6de1-4166-f562-c805ed741a73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'sklearn.ensemble._forest.RandomForestRegressor'>\n",
            "1616.7334030675572 \n",
            " 32.02590640295567\n",
            "<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>\n",
            "1559.0731693730515 \n",
            " 30.819149952015746\n",
            "<class 'sklearn.svm._classes.SVR'>\n",
            "1682.8550767799006 \n",
            " 31.092811183327488\n",
            "AVERAGING\n",
            "Averaging Model MAE: 30.835204381492076\n",
            "Averaging Model MSE: 1544.1910519781488\n",
            "BOOSTING\n",
            "Averaging Model MAE: 30.816304276096172\n",
            "Averaging Model MSE: 1559.0412614837173\n",
            "BAGGING\n",
            "[02:20:38] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[02:20:38] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[02:20:38] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[02:20:38] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[02:20:39] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[02:20:39] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[02:20:39] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[02:20:39] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[02:20:39] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[02:20:40] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "Bagging Model MAE: 30.76168835599552\n",
            "Bagging Model MSE: 1538.6890343409123\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Fire size 200-1000"
      ],
      "metadata": {
        "id": "hBTgCPKMFfDO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "######### Spliting by size\n",
        "\n",
        "Size_sample50 = Fires_sample[(Fires_sample[\"FIRE_SIZE\"] > 200) & (Fires_sample[\"FIRE_SIZE\"] <= 1000)]\n",
        "\n",
        "########### Spliting by month only needed when switching seasons\n",
        "Season_sample50 = Size_sample50[(Size_sample50[\"DISCOVERY_MONTH\"] > 3) & (Size_sample50[\"DISCOVERY_MONTH\"] <= 6)]\n",
        "\n",
        "################ Train test split\n",
        "\n",
        "Fires12 = Season_sample50[Season_sample50['FIRE_YEAR'] <= 2017].copy()\n",
        "Fires13 = Season_sample50[Season_sample50['FIRE_YEAR'] > 2017].copy()\n",
        "X_train = Fires12.loc[:, Fires12.columns != 'FIRE_SIZE']\n",
        "y_train = Fires12['FIRE_SIZE'].copy()\n",
        "X_test = Fires13.loc[:, Fires13.columns != 'FIRE_SIZE']\n",
        "y_test = Fires13['FIRE_SIZE'].copy()\n",
        "\n",
        "############################################ Testing best methods\n",
        "\n",
        "model_list = [RandomForestRegressor, GradientBoostingRegressor, SVR] #GaussianNB, \n",
        "model_scores = dict()\n",
        "for model in model_list:\n",
        "  # fit model to data\n",
        "  print(model)\n",
        "  regressor = model().fit(X_train,y_train)\n",
        "  y_pred = regressor.predict(X_test)\n",
        "  model_mse = mean_squared_error(y_pred,y_test)\n",
        "  model_mae = mean_absolute_error(y_pred,y_test)\n",
        "  model_scores[model] = (regressor,y_pred,model_mse,model_mae)\n",
        "  print(model_mse,'\\n',model_mae)\n",
        "\n",
        "############################################# Averaging Method\n",
        "#############################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import xgboost as xgb\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        " \n",
        "# initializing all the model objects with default parameters\n",
        "model_1 = GradientBoostingRegressor()\n",
        "model_2 = SVR()\n",
        "model_3 = RandomForestRegressor()\n",
        " \n",
        "# training all the model on the training dataset\n",
        "model_1.fit(X_train, y_train)\n",
        "model_2.fit(X_train, y_train)\n",
        "model_3.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the validation dataset\n",
        "pred_1 = model_1.predict(X_test)\n",
        "pred_2 = model_2.predict(X_test)\n",
        "pred_3 = model_3.predict(X_test)\n",
        " \n",
        "# final prediction after averaging on the prediction of all 3 models\n",
        "pred_final = (pred_1+pred_2+pred_3)/3.0\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "mod1 = (mean_absolute_error(pred_1, y_test))\n",
        "mod2 = (mean_absolute_error(pred_2, y_test))\n",
        "mod3 = (mean_absolute_error(pred_3, y_test))\n",
        "#print(mean_absolute_error(pred_final, y_test))  ##switch Y_pred and test\n",
        "#Averaging_model = (mod1+mod2+mod3)/3.0\n",
        "Avg_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Avg_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('AVERAGING')\n",
        "print(\"Averaging Model MAE:\", Avg_mod)\n",
        "print(\"Averaging Model MSE:\", Avg_mod1)\n",
        "\n",
        "############################################# Boosting Method\n",
        "#############################################\n",
        "\n",
        "# importing utility modules\n",
        "import pandas as pd\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        " \n",
        "# initializing the boosting module with default parameters\n",
        "model = GradientBoostingRegressor() #163\n",
        " \n",
        "# training the model on the train dataset\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Boost_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Boost_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('BOOSTING')\n",
        "print(\"Averaging Model MAE:\", Boost_mod)\n",
        "print(\"Averaging Model MSE:\", Boost_mod1)\n",
        "print('BAGGING')\n",
        "\n",
        "################################################# Bagging Method\n",
        "#################################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "import xgboost as xgb\n",
        " \n",
        "# importing bagging module\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "\n",
        "# initializing the bagging model using XGboost as base model with default parameters\n",
        "model = BaggingRegressor(base_estimator=xgb.XGBRegressor())\n",
        " \n",
        "# training model\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Bag_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Bag_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print(\"Bagging Model MAE:\", Bag_mod)\n",
        "print(\"Bagging Model MSE:\", Bag_mod1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ReaDv8_HFOTg",
        "outputId": "858ac53b-502d-43b7-ecae-4ffc3b8555ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'sklearn.ensemble._forest.RandomForestRegressor'>\n",
            "42427.71757622331 \n",
            " 178.41130203775512\n",
            "<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>\n",
            "37842.15214902143 \n",
            " 164.78697784199093\n",
            "<class 'sklearn.svm._classes.SVR'>\n",
            "38082.327488982904 \n",
            " 144.90140231980422\n",
            "AVERAGING\n",
            "Averaging Model MAE: 158.85510079487375\n",
            "Averaging Model MSE: 36430.381972956944\n",
            "BOOSTING\n",
            "Averaging Model MAE: 165.40550975642202\n",
            "Averaging Model MSE: 38181.04325641447\n",
            "BAGGING\n",
            "[02:20:50] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[02:20:50] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[02:20:50] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[02:20:50] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[02:20:50] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[02:20:50] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[02:20:51] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[02:20:51] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[02:20:51] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[02:20:51] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "Bagging Model MAE: 163.8661010368503\n",
            "Bagging Model MSE: 37325.72330110476\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Fire size above 1000"
      ],
      "metadata": {
        "id": "0BTQy4OAFjqR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "######## Spliting by size\n",
        "\n",
        "Size_sampleinf = Fires_sample[Fires_sample['FIRE_SIZE'] > 1000]\n",
        "\n",
        "########### Spliting by month only needed when switching seasons\n",
        "Season_sampleinf = Size_sampleinf[(Size_sampleinf[\"DISCOVERY_MONTH\"] > 3) & (Size_sampleinf[\"DISCOVERY_MONTH\"] <= 6)]\n",
        "\n",
        "################ Train test split\n",
        "\n",
        "Fires12 = Season_sampleinf[Season_sampleinf['FIRE_YEAR'] <= 2017].copy()\n",
        "Fires13 = Season_sampleinf[Season_sampleinf['FIRE_YEAR'] > 2017].copy()\n",
        "X_train = Fires12.loc[:, Fires12.columns != 'FIRE_SIZE']\n",
        "y_train = Fires12['FIRE_SIZE'].copy()\n",
        "X_test = Fires13.loc[:, Fires13.columns != 'FIRE_SIZE']\n",
        "y_test = Fires13['FIRE_SIZE'].copy()\n",
        "\n",
        "############################################ Testing best methods\n",
        "\n",
        "model_list = [RandomForestRegressor, GradientBoostingRegressor, SVR] #GaussianNB, \n",
        "model_scores = dict()\n",
        "for model in model_list:\n",
        "  # fit model to data\n",
        "  print(model)\n",
        "  regressor = model().fit(X_train,y_train)\n",
        "  y_pred = regressor.predict(X_test)\n",
        "  model_mse = mean_squared_error(y_pred,y_test)\n",
        "  model_mae = mean_absolute_error(y_pred,y_test)\n",
        "  model_scores[model] = (regressor,y_pred,model_mse,model_mae)\n",
        "  print(model_mse,'\\n',model_mae)\n",
        "\n",
        "############################################# Averaging Method\n",
        "#############################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import xgboost as xgb\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        " \n",
        "# initializing all the model objects with default parameters\n",
        "model_1 = GradientBoostingRegressor()\n",
        "model_2 = SVR()\n",
        "model_3 = RandomForestRegressor()\n",
        " \n",
        "# training all the model on the training dataset\n",
        "model_1.fit(X_train, y_train)\n",
        "model_2.fit(X_train, y_train)\n",
        "model_3.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the validation dataset\n",
        "pred_1 = model_1.predict(X_test)\n",
        "pred_2 = model_2.predict(X_test)\n",
        "pred_3 = model_3.predict(X_test)\n",
        " \n",
        "# final prediction after averaging on the prediction of all 3 models\n",
        "pred_final = (pred_1+pred_2+pred_3)/3.0\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "mod1 = (mean_absolute_error(pred_1, y_test))\n",
        "mod2 = (mean_absolute_error(pred_2, y_test))\n",
        "mod3 = (mean_absolute_error(pred_3, y_test))\n",
        "#print(mean_absolute_error(pred_final, y_test))  ##switch Y_pred and test\n",
        "#Averaging_model = (mod1+mod2+mod3)/3.0\n",
        "Avg_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Avg_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('AVERAGING')\n",
        "print(\"Averaging Model MAE:\", Avg_mod)\n",
        "print(\"Averaging Model MSE:\", Avg_mod1)\n",
        "\n",
        "############################################# Boosting Method\n",
        "#############################################\n",
        "\n",
        "# importing utility modules\n",
        "import pandas as pd\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        " \n",
        "# initializing the boosting module with default parameters\n",
        "model = GradientBoostingRegressor() #163\n",
        " \n",
        "# training the model on the train dataset\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Boost_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Boost_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('BOOSTING')\n",
        "print(\"Averaging Model MAE:\", Boost_mod)\n",
        "print(\"Averaging Model MSE:\", Boost_mod1)\n",
        "print('BAGGING')\n",
        "\n",
        "################################################# Bagging Method\n",
        "#################################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "import xgboost as xgb\n",
        " \n",
        "# importing bagging module\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "\n",
        "# initializing the bagging model using XGboost as base model with default parameters\n",
        "model = BaggingRegressor(base_estimator=xgb.XGBRegressor())\n",
        " \n",
        "# training model\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Bag_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Bag_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print(\"Bagging Model MAE:\", Bag_mod)\n",
        "print(\"Bagging Model MSE:\", Bag_mod1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ksJ0u9sFlkV",
        "outputId": "bbd867a2-7bd8-48a8-c0fa-fc2dd92541c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'sklearn.ensemble._forest.RandomForestRegressor'>\n",
            "1465785473.72815 \n",
            " 15417.581825563637\n",
            "<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>\n",
            "2078513528.1478865 \n",
            " 15240.786767683985\n",
            "<class 'sklearn.svm._classes.SVR'>\n",
            "308151991.1001293 \n",
            " 7111.407858769375\n",
            "AVERAGING\n",
            "Averaging Model MAE: 11648.528231895098\n",
            "Averaging Model MSE: 937235519.4459884\n",
            "BOOSTING\n",
            "Averaging Model MAE: 15240.786767683987\n",
            "Averaging Model MSE: 2078513528.1478865\n",
            "BAGGING\n",
            "[02:21:01] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[02:21:01] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[02:21:02] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[02:21:02] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[02:21:02] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[02:21:02] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[02:21:02] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[02:21:02] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[02:21:02] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[02:21:02] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "Bagging Model MAE: 12387.539931747158\n",
            "Bagging Model MSE: 778148363.9459593\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Season 7-9"
      ],
      "metadata": {
        "id": "OiaXQWOF9YLI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Fire size 0-.25"
      ],
      "metadata": {
        "id": "z-ZMPJbn-oTv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "########## Spliting by size\n",
        "\n",
        "Size_sample1 = Fires_sample[Fires_sample[\"FIRE_SIZE\"] <= .25]\n",
        "\n",
        "########### Spliting by month only needed when switching seasons\n",
        "Season_sample1 = Size_sample1[(Size_sample1[\"DISCOVERY_MONTH\"] > 6) & (Size_sample1[\"DISCOVERY_MONTH\"] <= 9)]\n",
        "\n",
        "################ Train test split\n",
        "\n",
        "Fires12 = Season_sample1[Season_sample1['FIRE_YEAR'] <= 2017].copy()\n",
        "Fires13 = Season_sample1[Season_sample1['FIRE_YEAR'] > 2017].copy()\n",
        "X_train = Fires12.loc[:, Fires12.columns != 'FIRE_SIZE']\n",
        "y_train = Fires12['FIRE_SIZE'].copy()\n",
        "X_test = Fires13.loc[:, Fires13.columns != 'FIRE_SIZE']\n",
        "y_test = Fires13['FIRE_SIZE'].copy()\n",
        "\n",
        "############################################ Testing best methods\n",
        "\n",
        "model_list = [RandomForestRegressor, GradientBoostingRegressor, SVR] #GaussianNB, \n",
        "model_scores = dict()\n",
        "for model in model_list:\n",
        "  # fit model to data\n",
        "  print(model)\n",
        "  regressor = model().fit(X_train,y_train)\n",
        "  y_pred = regressor.predict(X_test)\n",
        "  model_mse = mean_squared_error(y_pred,y_test)\n",
        "  model_mae = mean_absolute_error(y_pred,y_test)\n",
        "  model_scores[model] = (regressor,y_pred,model_mse,model_mae)\n",
        "  print(model_mse,'\\n',model_mae)\n",
        "\n",
        "############################################# Averaging Method\n",
        "#############################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import xgboost as xgb\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        " \n",
        "# initializing all the model objects with default parameters\n",
        "model_1 = GradientBoostingRegressor()\n",
        "model_2 = SVR()\n",
        "model_3 = RandomForestRegressor()\n",
        " \n",
        "# training all the model on the training dataset\n",
        "model_1.fit(X_train, y_train)\n",
        "model_2.fit(X_train, y_train)\n",
        "model_3.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the validation dataset\n",
        "pred_1 = model_1.predict(X_test)\n",
        "pred_2 = model_2.predict(X_test)\n",
        "pred_3 = model_3.predict(X_test)\n",
        " \n",
        "# final prediction after averaging on the prediction of all 3 models\n",
        "pred_final = (pred_1+pred_2+pred_3)/3.0\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "mod1 = (mean_absolute_error(pred_1, y_test))\n",
        "mod2 = (mean_absolute_error(pred_2, y_test))\n",
        "mod3 = (mean_absolute_error(pred_3, y_test))\n",
        "#print(mean_absolute_error(pred_final, y_test))  ##switch Y_pred and test\n",
        "#Averaging_model = (mod1+mod2+mod3)/3.0\n",
        "Avg_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Avg_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('AVERAGING')\n",
        "print(\"Averaging Model MAE:\", Avg_mod)\n",
        "print(\"Averaging Model MSE:\", Avg_mod1)\n",
        "\n",
        "############################################# Boosting Method\n",
        "#############################################\n",
        "\n",
        "# importing utility modules\n",
        "import pandas as pd\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        " \n",
        "# initializing the boosting module with default parameters\n",
        "model = GradientBoostingRegressor() #163\n",
        " \n",
        "# training the model on the train dataset\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Boost_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Boost_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('BOOSTING')\n",
        "print(\"Averaging Model MAE:\", Boost_mod)\n",
        "print(\"Averaging Model MSE:\", Boost_mod1)\n",
        "print('BAGGING')\n",
        "\n",
        "################################################# Bagging Method\n",
        "#################################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "import xgboost as xgb\n",
        " \n",
        "# importing bagging module\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "\n",
        "# initializing the bagging model using XGboost as base model with default parameters\n",
        "model = BaggingRegressor(base_estimator=xgb.XGBRegressor())\n",
        " \n",
        "# training model\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Bag_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Bag_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print(\"Bagging Model MAE:\", Bag_mod)\n",
        "print(\"Bagging Model MSE:\", Bag_mod1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hEmTXYn9gOc",
        "outputId": "b5d1f8a1-2f29-4faf-f20f-6fdbd769b577"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'sklearn.ensemble._forest.RandomForestRegressor'>\n",
            "0.0032324479858306422 \n",
            " 0.03866290413718203\n",
            "<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>\n",
            "0.0031523309797918736 \n",
            " 0.03752129673439204\n",
            "<class 'sklearn.svm._classes.SVR'>\n",
            "0.004485974571748214 \n",
            " 0.04989930307147221\n",
            "AVERAGING\n",
            "Averaging Model MAE: 0.040109987122549894\n",
            "Averaging Model MSE: 0.0032441628329813186\n",
            "BOOSTING\n",
            "Averaging Model MAE: 0.03752129673439206\n",
            "Averaging Model MSE: 0.0031523309797918744\n",
            "BAGGING\n",
            "[02:59:11] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[02:59:16] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[02:59:21] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[02:59:25] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[02:59:30] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[02:59:35] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[02:59:40] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[02:59:44] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[02:59:49] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[02:59:54] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "Bagging Model MAE: 0.037437262244355216\n",
            "Bagging Model MSE: 0.0031582826399277264\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Fire size .25-1"
      ],
      "metadata": {
        "id": "XBvlWSKQMWMG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "####### Spliting by size\n",
        "\n",
        "Size_sample20 = Fires_sample[(Fires_sample[\"FIRE_SIZE\"] > .25) & (Fires_sample[\"FIRE_SIZE\"] <= 1)]\n",
        "\n",
        "########### Spliting by month only needed when switching seasons\n",
        "Season_sample20 = Size_sample20[(Size_sample20[\"DISCOVERY_MONTH\"] > 6) & (Size_sample20[\"DISCOVERY_MONTH\"] <= 9)]\n",
        "\n",
        "################ Train test split\n",
        "\n",
        "Fires12 = Season_sample20[Season_sample20['FIRE_YEAR'] <= 2017].copy()\n",
        "Fires13 = Season_sample20[Season_sample20['FIRE_YEAR'] > 2017].copy()\n",
        "X_train = Fires12.loc[:, Fires12.columns != 'FIRE_SIZE']\n",
        "y_train = Fires12['FIRE_SIZE'].copy()\n",
        "X_test = Fires13.loc[:, Fires13.columns != 'FIRE_SIZE']\n",
        "y_test = Fires13['FIRE_SIZE'].copy()\n",
        "\n",
        "############################################ Testing best methods\n",
        "\n",
        "model_list = [RandomForestRegressor, GradientBoostingRegressor, SVR] #GaussianNB, \n",
        "model_scores = dict()\n",
        "for model in model_list:\n",
        "  # fit model to data\n",
        "  print(model)\n",
        "  regressor = model().fit(X_train,y_train)\n",
        "  y_pred = regressor.predict(X_test)\n",
        "  model_mse = mean_squared_error(y_pred,y_test)\n",
        "  model_mae = mean_absolute_error(y_pred,y_test)\n",
        "  model_scores[model] = (regressor,y_pred,model_mse,model_mae)\n",
        "  print(model_mse,'\\n',model_mae)\n",
        "\n",
        "############################################# Averaging Method\n",
        "#############################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import xgboost as xgb\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        " \n",
        "# initializing all the model objects with default parameters\n",
        "model_1 = GradientBoostingRegressor()\n",
        "model_2 = SVR()\n",
        "model_3 = RandomForestRegressor()\n",
        " \n",
        "# training all the model on the training dataset\n",
        "model_1.fit(X_train, y_train)\n",
        "model_2.fit(X_train, y_train)\n",
        "model_3.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the validation dataset\n",
        "pred_1 = model_1.predict(X_test)\n",
        "pred_2 = model_2.predict(X_test)\n",
        "pred_3 = model_3.predict(X_test)\n",
        " \n",
        "# final prediction after averaging on the prediction of all 3 models\n",
        "pred_final = (pred_1+pred_2+pred_3)/3.0\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "mod1 = (mean_absolute_error(pred_1, y_test))\n",
        "mod2 = (mean_absolute_error(pred_2, y_test))\n",
        "mod3 = (mean_absolute_error(pred_3, y_test))\n",
        "#print(mean_absolute_error(pred_final, y_test))  ##switch Y_pred and test\n",
        "#Averaging_model = (mod1+mod2+mod3)/3.0\n",
        "Avg_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Avg_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('AVERAGING')\n",
        "print(\"Averaging Model MAE:\", Avg_mod)\n",
        "print(\"Averaging Model MSE:\", Avg_mod1)\n",
        "\n",
        "############################################# Boosting Method\n",
        "#############################################\n",
        "\n",
        "# importing utility modules\n",
        "import pandas as pd\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        " \n",
        "# initializing the boosting module with default parameters\n",
        "model = GradientBoostingRegressor() #163\n",
        " \n",
        "# training the model on the train dataset\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Boost_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Boost_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('BOOSTING')\n",
        "print(\"Averaging Model MAE:\", Boost_mod)\n",
        "print(\"Averaging Model MSE:\", Boost_mod1)\n",
        "print('BAGGING')\n",
        "\n",
        "################################################# Bagging Method\n",
        "#################################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "import xgboost as xgb\n",
        " \n",
        "# importing bagging module\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "\n",
        "# initializing the bagging model using XGboost as base model with default parameters\n",
        "model = BaggingRegressor(base_estimator=xgb.XGBRegressor())\n",
        " \n",
        "# training model\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Bag_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Bag_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print(\"Bagging Model MAE:\", Bag_mod)\n",
        "print(\"Bagging Model MSE:\", Bag_mod1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izu-ZCIUMY2K",
        "outputId": "5b428b03-6e1b-44bd-b126-89d39f2b5c65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'sklearn.ensemble._forest.RandomForestRegressor'>\n",
            "0.07461184748639721 \n",
            " 0.230121755624958\n",
            "<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>\n",
            "0.0709153795013837 \n",
            " 0.2375790385089624\n",
            "<class 'sklearn.svm._classes.SVR'>\n",
            "0.07529756518089802 \n",
            " 0.23556824912292\n",
            "AVERAGING\n",
            "Averaging Model MAE: 0.2305475207927201\n",
            "Averaging Model MSE: 0.06549312806745085\n",
            "BOOSTING\n",
            "Averaging Model MAE: 0.23757903850896236\n",
            "Averaging Model MSE: 0.07091537950138367\n",
            "BAGGING\n",
            "[03:09:01] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:09:03] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:09:05] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:09:07] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:09:10] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:09:12] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:09:14] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:09:16] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:09:18] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:09:20] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "Bagging Model MAE: 0.23681169042849573\n",
            "Bagging Model MSE: 0.07045294076734414\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Fire size 1-5"
      ],
      "metadata": {
        "id": "pJVLIV70-6Un"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "######## Spliting by size\n",
        "\n",
        "Size_sample20 = Fires_sample[(Fires_sample[\"FIRE_SIZE\"] > 1) & (Fires_sample[\"FIRE_SIZE\"] <= 5)]\n",
        "\n",
        "########### Spliting by month only needed when switching seasons\n",
        "Season_sample20 = Size_sample20[(Size_sample20[\"DISCOVERY_MONTH\"] > 6) & (Size_sample20[\"DISCOVERY_MONTH\"] <= 9)]\n",
        "\n",
        "################ Train test split\n",
        "\n",
        "Fires12 = Season_sample20[Season_sample20['FIRE_YEAR'] <= 2017].copy()\n",
        "Fires13 = Season_sample20[Season_sample20['FIRE_YEAR'] > 2017].copy()\n",
        "X_train = Fires12.loc[:, Fires12.columns != 'FIRE_SIZE']\n",
        "y_train = Fires12['FIRE_SIZE'].copy()\n",
        "X_test = Fires13.loc[:, Fires13.columns != 'FIRE_SIZE']\n",
        "y_test = Fires13['FIRE_SIZE'].copy()\n",
        "\n",
        "############################################ Testing best methods\n",
        "\n",
        "model_list = [RandomForestRegressor, GradientBoostingRegressor, SVR] #GaussianNB, \n",
        "model_scores = dict()\n",
        "for model in model_list:\n",
        "  # fit model to data\n",
        "  print(model)\n",
        "  regressor = model().fit(X_train,y_train)\n",
        "  y_pred = regressor.predict(X_test)\n",
        "  model_mse = mean_squared_error(y_pred,y_test)\n",
        "  model_mae = mean_absolute_error(y_pred,y_test)\n",
        "  model_scores[model] = (regressor,y_pred,model_mse,model_mae)\n",
        "  print(model_mse,'\\n',model_mae)\n",
        "\n",
        "############################################# Averaging Method\n",
        "#############################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import xgboost as xgb\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        " \n",
        "# initializing all the model objects with default parameters\n",
        "model_1 = GradientBoostingRegressor()\n",
        "model_2 = SVR()\n",
        "model_3 = RandomForestRegressor()\n",
        " \n",
        "# training all the model on the training dataset\n",
        "model_1.fit(X_train, y_train)\n",
        "model_2.fit(X_train, y_train)\n",
        "model_3.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the validation dataset\n",
        "pred_1 = model_1.predict(X_test)\n",
        "pred_2 = model_2.predict(X_test)\n",
        "pred_3 = model_3.predict(X_test)\n",
        " \n",
        "# final prediction after averaging on the prediction of all 3 models\n",
        "pred_final = (pred_1+pred_2+pred_3)/3.0\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "mod1 = (mean_absolute_error(pred_1, y_test))\n",
        "mod2 = (mean_absolute_error(pred_2, y_test))\n",
        "mod3 = (mean_absolute_error(pred_3, y_test))\n",
        "#print(mean_absolute_error(pred_final, y_test))  ##switch Y_pred and test\n",
        "#Averaging_model = (mod1+mod2+mod3)/3.0\n",
        "Avg_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Avg_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('AVERAGING')\n",
        "print(\"Averaging Model MAE:\", Avg_mod)\n",
        "print(\"Averaging Model MSE:\", Avg_mod1)\n",
        "\n",
        "############################################# Boosting Method\n",
        "#############################################\n",
        "\n",
        "# importing utility modules\n",
        "import pandas as pd\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        " \n",
        "# initializing the boosting module with default parameters\n",
        "model = GradientBoostingRegressor() #163\n",
        " \n",
        "# training the model on the train dataset\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Boost_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Boost_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('BOOSTING')\n",
        "print(\"Averaging Model MAE:\", Boost_mod)\n",
        "print(\"Averaging Model MSE:\", Boost_mod1)\n",
        "print('BAGGING')\n",
        "\n",
        "################################################# Bagging Method\n",
        "#################################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "import xgboost as xgb\n",
        " \n",
        "# importing bagging module\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "\n",
        "# initializing the bagging model using XGboost as base model with default parameters\n",
        "model = BaggingRegressor(base_estimator=xgb.XGBRegressor())\n",
        " \n",
        "# training model\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Bag_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Bag_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print(\"Bagging Model MAE:\", Bag_mod)\n",
        "print(\"Bagging Model MSE:\", Bag_mod1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPHlXIGY-8dd",
        "outputId": "4b0c788b-76e0-43ba-8f5f-5c9adea5c88b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'sklearn.ensemble._forest.RandomForestRegressor'>\n",
            "1.5778247576725215 \n",
            " 1.0654536889469104\n",
            "<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>\n",
            "1.4754411210643716 \n",
            " 1.0423407698303178\n",
            "<class 'sklearn.svm._classes.SVR'>\n",
            "1.602350448474377 \n",
            " 1.0416657143381105\n",
            "AVERAGING\n",
            "Averaging Model MAE: 1.0283192754683188\n",
            "Averaging Model MSE: 1.4667026453709129\n",
            "BOOSTING\n",
            "Averaging Model MAE: 1.0421743867797923\n",
            "Averaging Model MSE: 1.4752593442356747\n",
            "BAGGING\n",
            "[03:11:34] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:11:35] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:11:36] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:11:38] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:11:39] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:11:40] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:11:41] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:11:43] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:11:44] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:11:45] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "Bagging Model MAE: 1.043983565674127\n",
            "Bagging Model MSE: 1.4745127513307024\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Fire size 5-20"
      ],
      "metadata": {
        "id": "rDnZNgca_GL3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "######### Spliting by size\n",
        "\n",
        "Size_sample30 = Fires_sample[(Fires_sample[\"FIRE_SIZE\"] > 5) & (Fires_sample[\"FIRE_SIZE\"] <= 20)]\n",
        "\n",
        "########### Spliting by month only needed when switching seasons\n",
        "Season_sample30 = Size_sample30[(Size_sample30[\"DISCOVERY_MONTH\"] > 6) & (Size_sample30[\"DISCOVERY_MONTH\"] <= 9)]\n",
        "\n",
        "################ Train test split\n",
        "\n",
        "Fires12 = Season_sample30[Season_sample30['FIRE_YEAR'] <= 2017].copy()\n",
        "Fires13 = Season_sample30[Season_sample30['FIRE_YEAR'] > 2017].copy()\n",
        "X_train = Fires12.loc[:, Fires12.columns != 'FIRE_SIZE']\n",
        "y_train = Fires12['FIRE_SIZE'].copy()\n",
        "X_test = Fires13.loc[:, Fires13.columns != 'FIRE_SIZE']\n",
        "y_test = Fires13['FIRE_SIZE'].copy()\n",
        "\n",
        "############################################ Testing best methods\n",
        "\n",
        "model_list = [RandomForestRegressor, GradientBoostingRegressor, SVR] #GaussianNB, \n",
        "model_scores = dict()\n",
        "for model in model_list:\n",
        "  # fit model to data\n",
        "  print(model)\n",
        "  regressor = model().fit(X_train,y_train)\n",
        "  y_pred = regressor.predict(X_test)\n",
        "  model_mse = mean_squared_error(y_pred,y_test)\n",
        "  model_mae = mean_absolute_error(y_pred,y_test)\n",
        "  model_scores[model] = (regressor,y_pred,model_mse,model_mae)\n",
        "  print(model_mse,'\\n',model_mae)\n",
        "\n",
        "############################################# Averaging Method\n",
        "#############################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import xgboost as xgb\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        " \n",
        "# initializing all the model objects with default parameters\n",
        "model_1 = GradientBoostingRegressor()\n",
        "model_2 = SVR()\n",
        "model_3 = RandomForestRegressor()\n",
        " \n",
        "# training all the model on the training dataset\n",
        "model_1.fit(X_train, y_train)\n",
        "model_2.fit(X_train, y_train)\n",
        "model_3.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the validation dataset\n",
        "pred_1 = model_1.predict(X_test)\n",
        "pred_2 = model_2.predict(X_test)\n",
        "pred_3 = model_3.predict(X_test)\n",
        " \n",
        "# final prediction after averaging on the prediction of all 3 models\n",
        "pred_final = (pred_1+pred_2+pred_3)/3.0\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "mod1 = (mean_absolute_error(pred_1, y_test))\n",
        "mod2 = (mean_absolute_error(pred_2, y_test))\n",
        "mod3 = (mean_absolute_error(pred_3, y_test))\n",
        "#print(mean_absolute_error(pred_final, y_test))  ##switch Y_pred and test\n",
        "#Averaging_model = (mod1+mod2+mod3)/3.0\n",
        "Avg_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Avg_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('AVERAGING')\n",
        "print(\"Averaging Model MAE:\", Avg_mod)\n",
        "print(\"Averaging Model MSE:\", Avg_mod1)\n",
        "\n",
        "############################################# Boosting Method\n",
        "#############################################\n",
        "\n",
        "# importing utility modules\n",
        "import pandas as pd\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        " \n",
        "# initializing the boosting module with default parameters\n",
        "model = GradientBoostingRegressor() #163\n",
        " \n",
        "# training the model on the train dataset\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Boost_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Boost_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('BOOSTING')\n",
        "print(\"Averaging Model MAE:\", Boost_mod)\n",
        "print(\"Averaging Model MSE:\", Boost_mod1)\n",
        "print('BAGGING')\n",
        "\n",
        "################################################# Bagging Method\n",
        "#################################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "import xgboost as xgb\n",
        " \n",
        "# importing bagging module\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "\n",
        "# initializing the bagging model using XGboost as base model with default parameters\n",
        "model = BaggingRegressor(base_estimator=xgb.XGBRegressor())\n",
        " \n",
        "# training model\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Bag_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Bag_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print(\"Bagging Model MAE:\", Bag_mod)\n",
        "print(\"Bagging Model MSE:\", Bag_mod1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHB-NNpT_H9f",
        "outputId": "606fd87a-ed95-409c-9364-989a94142e19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'sklearn.ensemble._forest.RandomForestRegressor'>\n",
            "21.681270411916575 \n",
            " 3.999574438502674\n",
            "<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>\n",
            "20.27836951681767 \n",
            " 3.835228848908317\n",
            "<class 'sklearn.svm._classes.SVR'>\n",
            "22.498957611406272 \n",
            " 3.719346107690018\n",
            "AVERAGING\n",
            "Averaging Model MAE: 3.8255969836435857\n",
            "Averaging Model MSE: 20.567301757913945\n",
            "BOOSTING\n",
            "Averaging Model MAE: 3.835228848908317\n",
            "Averaging Model MSE: 20.27836951681767\n",
            "BAGGING\n",
            "[03:12:23] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:12:24] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:12:24] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:12:25] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:12:26] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:12:26] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:12:27] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:12:27] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:12:28] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:12:29] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "Bagging Model MAE: 3.814828853301186\n",
            "Bagging Model MSE: 20.15247684202792\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Fire size 20-50"
      ],
      "metadata": {
        "id": "al3TZetE_OgS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "########## Spliting by size\n",
        "\n",
        "Size_sample40 = Fires_sample[(Fires_sample[\"FIRE_SIZE\"] > 20) & (Fires_sample[\"FIRE_SIZE\"] <= 50)]\n",
        "\n",
        "########### Spliting by month only needed when switching seasons\n",
        "Season_sample40 = Size_sample40[(Size_sample40[\"DISCOVERY_MONTH\"] > 6) & (Size_sample40[\"DISCOVERY_MONTH\"] <= 9)]\n",
        "\n",
        "################ Train test split\n",
        "\n",
        "Fires12 = Season_sample40[Season_sample40['FIRE_YEAR'] <= 2017].copy()\n",
        "Fires13 = Season_sample40[Season_sample40['FIRE_YEAR'] > 2017].copy()\n",
        "X_train = Fires12.loc[:, Fires12.columns != 'FIRE_SIZE']\n",
        "y_train = Fires12['FIRE_SIZE'].copy()\n",
        "X_test = Fires13.loc[:, Fires13.columns != 'FIRE_SIZE']\n",
        "y_test = Fires13['FIRE_SIZE'].copy()\n",
        "\n",
        "############################################ Testing best methods\n",
        "\n",
        "model_list = [RandomForestRegressor, GradientBoostingRegressor, SVR] #GaussianNB, \n",
        "model_scores = dict()\n",
        "for model in model_list:\n",
        "  # fit model to data\n",
        "  print(model)\n",
        "  regressor = model().fit(X_train,y_train)\n",
        "  y_pred = regressor.predict(X_test)\n",
        "  model_mse = mean_squared_error(y_pred,y_test)\n",
        "  model_mae = mean_absolute_error(y_pred,y_test)\n",
        "  model_scores[model] = (regressor,y_pred,model_mse,model_mae)\n",
        "  print(model_mse,'\\n',model_mae)\n",
        "\n",
        "############################################# Averaging Method\n",
        "#############################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import xgboost as xgb\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        " \n",
        "# initializing all the model objects with default parameters\n",
        "model_1 = GradientBoostingRegressor()\n",
        "model_2 = SVR()\n",
        "model_3 = RandomForestRegressor()\n",
        " \n",
        "# training all the model on the training dataset\n",
        "model_1.fit(X_train, y_train)\n",
        "model_2.fit(X_train, y_train)\n",
        "model_3.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the validation dataset\n",
        "pred_1 = model_1.predict(X_test)\n",
        "pred_2 = model_2.predict(X_test)\n",
        "pred_3 = model_3.predict(X_test)\n",
        " \n",
        "# final prediction after averaging on the prediction of all 3 models\n",
        "pred_final = (pred_1+pred_2+pred_3)/3.0\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "mod1 = (mean_absolute_error(pred_1, y_test))\n",
        "mod2 = (mean_absolute_error(pred_2, y_test))\n",
        "mod3 = (mean_absolute_error(pred_3, y_test))\n",
        "#print(mean_absolute_error(pred_final, y_test))  ##switch Y_pred and test\n",
        "#Averaging_model = (mod1+mod2+mod3)/3.0\n",
        "Avg_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Avg_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('AVERAGING')\n",
        "print(\"Averaging Model MAE:\", Avg_mod)\n",
        "print(\"Averaging Model MSE:\", Avg_mod1)\n",
        "\n",
        "############################################# Boosting Method\n",
        "#############################################\n",
        "\n",
        "# importing utility modules\n",
        "import pandas as pd\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        " \n",
        "# initializing the boosting module with default parameters\n",
        "model = GradientBoostingRegressor() #163\n",
        " \n",
        "# training the model on the train dataset\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Boost_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Boost_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('BOOSTING')\n",
        "print(\"Averaging Model MAE:\", Boost_mod)\n",
        "print(\"Averaging Model MSE:\", Boost_mod1)\n",
        "print('BAGGING')\n",
        "\n",
        "################################################# Bagging Method\n",
        "#################################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "import xgboost as xgb\n",
        " \n",
        "# importing bagging module\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "\n",
        "# initializing the bagging model using XGboost as base model with default parameters\n",
        "model = BaggingRegressor(base_estimator=xgb.XGBRegressor())\n",
        " \n",
        "# training model\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Bag_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Bag_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print(\"Bagging Model MAE:\", Bag_mod)\n",
        "print(\"Bagging Model MSE:\", Bag_mod1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLW2sp7C_Qeu",
        "outputId": "b4bdf88b-de72-4544-9a05-a6a19920bbe2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'sklearn.ensemble._forest.RandomForestRegressor'>\n",
            "75.69701992371796 \n",
            " 7.489803846153848\n",
            "<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>\n",
            "77.20578784356242 \n",
            " 7.785562071418979\n",
            "<class 'sklearn.svm._classes.SVR'>\n",
            "94.36092081322306 \n",
            " 8.005302632710126\n",
            "AVERAGING\n",
            "Averaging Model MAE: 7.639033005925894\n",
            "Averaging Model MSE: 77.05514604800577\n",
            "BOOSTING\n",
            "Averaging Model MAE: 7.78831935540633\n",
            "Averaging Model MSE: 77.24122032217264\n",
            "BAGGING\n",
            "[03:12:41] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:12:41] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:12:41] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:12:41] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:12:42] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:12:42] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:12:42] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:12:42] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:12:43] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:12:43] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "Bagging Model MAE: 7.737032391083546\n",
            "Bagging Model MSE: 76.25593846768746\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Fire size 50-200"
      ],
      "metadata": {
        "id": "GNoOaFJ9_Qvf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "######### Spliting by size\n",
        "\n",
        "Size_sample50 = Fires_sample[(Fires_sample[\"FIRE_SIZE\"] > 50) & (Fires_sample[\"FIRE_SIZE\"] <= 200)]\n",
        "\n",
        "########### Spliting by month only needed when switching seasons\n",
        "Season_sample50 = Size_sample50[(Size_sample50[\"DISCOVERY_MONTH\"] > 6) & (Size_sample50[\"DISCOVERY_MONTH\"] <= 9)]\n",
        "\n",
        "################ Train test split\n",
        "\n",
        "Fires12 = Season_sample50[Season_sample50['FIRE_YEAR'] <= 2017].copy()\n",
        "Fires13 = Season_sample50[Season_sample50['FIRE_YEAR'] > 2017].copy()\n",
        "X_train = Fires12.loc[:, Fires12.columns != 'FIRE_SIZE']\n",
        "y_train = Fires12['FIRE_SIZE'].copy()\n",
        "X_test = Fires13.loc[:, Fires13.columns != 'FIRE_SIZE']\n",
        "y_test = Fires13['FIRE_SIZE'].copy()\n",
        "\n",
        "############################################ Testing best methods\n",
        "\n",
        "model_list = [RandomForestRegressor, GradientBoostingRegressor, SVR] #GaussianNB, \n",
        "model_scores = dict()\n",
        "for model in model_list:\n",
        "  # fit model to data\n",
        "  print(model)\n",
        "  regressor = model().fit(X_train,y_train)\n",
        "  y_pred = regressor.predict(X_test)\n",
        "  model_mse = mean_squared_error(y_pred,y_test)\n",
        "  model_mae = mean_absolute_error(y_pred,y_test)\n",
        "  model_scores[model] = (regressor,y_pred,model_mse,model_mae)\n",
        "  print(model_mse,'\\n',model_mae)\n",
        "\n",
        "############################################# Averaging Method\n",
        "#############################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import xgboost as xgb\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        " \n",
        "# initializing all the model objects with default parameters\n",
        "model_1 = GradientBoostingRegressor()\n",
        "model_2 = SVR()\n",
        "model_3 = RandomForestRegressor()\n",
        " \n",
        "# training all the model on the training dataset\n",
        "model_1.fit(X_train, y_train)\n",
        "model_2.fit(X_train, y_train)\n",
        "model_3.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the validation dataset\n",
        "pred_1 = model_1.predict(X_test)\n",
        "pred_2 = model_2.predict(X_test)\n",
        "pred_3 = model_3.predict(X_test)\n",
        " \n",
        "# final prediction after averaging on the prediction of all 3 models\n",
        "pred_final = (pred_1+pred_2+pred_3)/3.0\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "mod1 = (mean_absolute_error(pred_1, y_test))\n",
        "mod2 = (mean_absolute_error(pred_2, y_test))\n",
        "mod3 = (mean_absolute_error(pred_3, y_test))\n",
        "#print(mean_absolute_error(pred_final, y_test))  ##switch Y_pred and test\n",
        "#Averaging_model = (mod1+mod2+mod3)/3.0\n",
        "Avg_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Avg_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('AVERAGING')\n",
        "print(\"Averaging Model MAE:\", Avg_mod)\n",
        "print(\"Averaging Model MSE:\", Avg_mod1)\n",
        "\n",
        "############################################# Boosting Method\n",
        "#############################################\n",
        "\n",
        "# importing utility modules\n",
        "import pandas as pd\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        " \n",
        "# initializing the boosting module with default parameters\n",
        "model = GradientBoostingRegressor() #163\n",
        " \n",
        "# training the model on the train dataset\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Boost_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Boost_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('BOOSTING')\n",
        "print(\"Averaging Model MAE:\", Boost_mod)\n",
        "print(\"Averaging Model MSE:\", Boost_mod1)\n",
        "print('BAGGING')\n",
        "\n",
        "################################################# Bagging Method\n",
        "#################################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "import xgboost as xgb\n",
        " \n",
        "# importing bagging module\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "\n",
        "# initializing the bagging model using XGboost as base model with default parameters\n",
        "model = BaggingRegressor(base_estimator=xgb.XGBRegressor())\n",
        " \n",
        "# training model\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Bag_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Bag_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print(\"Bagging Model MAE:\", Bag_mod)\n",
        "print(\"Bagging Model MSE:\", Bag_mod1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BvN6vnbr_Sua",
        "outputId": "489e622f-44ef-4f65-f000-33f40dd26abf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'sklearn.ensemble._forest.RandomForestRegressor'>\n",
            "2105.163656645655 \n",
            " 39.18799172413793\n",
            "<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>\n",
            "2026.7688037436424 \n",
            " 37.24657641711261\n",
            "<class 'sklearn.svm._classes.SVR'>\n",
            "2097.975218000009 \n",
            " 35.97728522516937\n",
            "AVERAGING\n",
            "Averaging Model MAE: 37.16956920225639\n",
            "Averaging Model MSE: 1997.1500760321705\n",
            "BOOSTING\n",
            "Averaging Model MAE: 37.21462988624744\n",
            "Averaging Model MSE: 2026.5630060068645\n",
            "BAGGING\n",
            "[03:12:52] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:12:52] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:12:53] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:12:53] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:12:53] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:12:53] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:12:54] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:12:54] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:12:54] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:12:54] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "Bagging Model MAE: 37.8232215984476\n",
            "Bagging Model MSE: 2037.8181137608983\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Fire size 200-1000"
      ],
      "metadata": {
        "id": "WgcDGL9B_S_l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "######### Spliting by size\n",
        "\n",
        "Size_sample100 = Fires_sample[(Fires_sample[\"FIRE_SIZE\"] > 200) & (Fires_sample[\"FIRE_SIZE\"] <= 1000)]\n",
        "\n",
        "########### Spliting by month only needed when switching seasons\n",
        "Season_sample100 = Size_sample100[(Size_sample100[\"DISCOVERY_MONTH\"] > 6) & (Size_sample100[\"DISCOVERY_MONTH\"] <= 9)]\n",
        "\n",
        "################ Train test split\n",
        "\n",
        "Fires12 = Season_sample100[Season_sample100['FIRE_YEAR'] <= 2017].copy()\n",
        "Fires13 = Season_sample100[Season_sample100['FIRE_YEAR'] > 2017].copy()\n",
        "X_train = Fires12.loc[:, Fires12.columns != 'FIRE_SIZE']\n",
        "y_train = Fires12['FIRE_SIZE'].copy()\n",
        "X_test = Fires13.loc[:, Fires13.columns != 'FIRE_SIZE']\n",
        "y_test = Fires13['FIRE_SIZE'].copy()\n",
        "\n",
        "############################################ Testing best methods\n",
        "\n",
        "model_list = [RandomForestRegressor, GradientBoostingRegressor, SVR] #GaussianNB, \n",
        "model_scores = dict()\n",
        "for model in model_list:\n",
        "  # fit model to data\n",
        "  print(model)\n",
        "  regressor = model().fit(X_train,y_train)\n",
        "  y_pred = regressor.predict(X_test)\n",
        "  model_mse = mean_squared_error(y_pred,y_test)\n",
        "  model_mae = mean_absolute_error(y_pred,y_test)\n",
        "  model_scores[model] = (regressor,y_pred,model_mse,model_mae)\n",
        "  print(model_mse,'\\n',model_mae)\n",
        "\n",
        "############################################# Averaging Method\n",
        "#############################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import xgboost as xgb\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        " \n",
        "# initializing all the model objects with default parameters\n",
        "model_1 = GradientBoostingRegressor()\n",
        "model_2 = SVR()\n",
        "model_3 = RandomForestRegressor()\n",
        " \n",
        "# training all the model on the training dataset\n",
        "model_1.fit(X_train, y_train)\n",
        "model_2.fit(X_train, y_train)\n",
        "model_3.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the validation dataset\n",
        "pred_1 = model_1.predict(X_test)\n",
        "pred_2 = model_2.predict(X_test)\n",
        "pred_3 = model_3.predict(X_test)\n",
        " \n",
        "# final prediction after averaging on the prediction of all 3 models\n",
        "pred_final = (pred_1+pred_2+pred_3)/3.0\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "mod1 = (mean_absolute_error(pred_1, y_test))\n",
        "mod2 = (mean_absolute_error(pred_2, y_test))\n",
        "mod3 = (mean_absolute_error(pred_3, y_test))\n",
        "#print(mean_absolute_error(pred_final, y_test))  ##switch Y_pred and test\n",
        "#Averaging_model = (mod1+mod2+mod3)/3.0\n",
        "Avg_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Avg_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('AVERAGING')\n",
        "print(\"Averaging Model MAE:\", Avg_mod)\n",
        "print(\"Averaging Model MSE:\", Avg_mod1)\n",
        "\n",
        "############################################# Boosting Method\n",
        "#############################################\n",
        "\n",
        "# importing utility modules\n",
        "import pandas as pd\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        " \n",
        "# initializing the boosting module with default parameters\n",
        "model = GradientBoostingRegressor() #163\n",
        " \n",
        "# training the model on the train dataset\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Boost_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Boost_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('BOOSTING')\n",
        "print(\"Averaging Model MAE:\", Boost_mod)\n",
        "print(\"Averaging Model MSE:\", Boost_mod1)\n",
        "print('BAGGING')\n",
        "\n",
        "################################################# Bagging Method\n",
        "#################################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "import xgboost as xgb\n",
        " \n",
        "# importing bagging module\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "\n",
        "# initializing the bagging model using XGboost as base model with default parameters\n",
        "model = BaggingRegressor(base_estimator=xgb.XGBRegressor())\n",
        " \n",
        "# training model\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Bag_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Bag_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print(\"Bagging Model MAE:\", Bag_mod)\n",
        "print(\"Bagging Model MSE:\", Bag_mod1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qi2wkCOY_YSb",
        "outputId": "36acd059-f011-42ba-dbce-d412217ac4fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'sklearn.ensemble._forest.RandomForestRegressor'>\n",
            "52674.01052940265 \n",
            " 188.7786021728261\n",
            "<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>\n",
            "55501.85213136812 \n",
            " 198.08176302579727\n",
            "<class 'sklearn.svm._classes.SVR'>\n",
            "67768.98766122873 \n",
            " 206.70460510416547\n",
            "AVERAGING\n",
            "Averaging Model MAE: 193.74695671094105\n",
            "Averaging Model MSE: 55058.777510847794\n",
            "BOOSTING\n",
            "Averaging Model MAE: 198.17636453346242\n",
            "Averaging Model MSE: 55441.60496331457\n",
            "BAGGING\n",
            "[03:12:59] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:12:59] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:12:59] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:12:59] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:12:59] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:13:00] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:13:00] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:13:00] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:13:00] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:13:00] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "Bagging Model MAE: 201.27520653766132\n",
            "Bagging Model MSE: 58240.213945422365\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Fire size 1000+"
      ],
      "metadata": {
        "id": "ZzFWWfF4_fRS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "######## Spliting by size\n",
        "\n",
        "Size_sampleinf = Fires_sample[Fires_sample['FIRE_SIZE'] > 1000]\n",
        "\n",
        "########### Spliting by month only needed when switching seasons\n",
        "Season_sampleinf = Size_sampleinf[(Size_sampleinf[\"DISCOVERY_MONTH\"] > 6) & (Size_sampleinf[\"DISCOVERY_MONTH\"] <= 9)]\n",
        "\n",
        "################ Train test split\n",
        "\n",
        "Fires12 = Season_sampleinf[Season_sampleinf['FIRE_YEAR'] <= 2017].copy()\n",
        "Fires13 = Season_sampleinf[Season_sampleinf['FIRE_YEAR'] > 2017].copy()\n",
        "X_train = Fires12.loc[:, Fires12.columns != 'FIRE_SIZE']\n",
        "y_train = Fires12['FIRE_SIZE'].copy()\n",
        "X_test = Fires13.loc[:, Fires13.columns != 'FIRE_SIZE']\n",
        "y_test = Fires13['FIRE_SIZE'].copy()\n",
        "\n",
        "############################################ Testing best methods\n",
        "\n",
        "model_list = [RandomForestRegressor, GradientBoostingRegressor, SVR] #GaussianNB, \n",
        "model_scores = dict()\n",
        "for model in model_list:\n",
        "  # fit model to data\n",
        "  print(model)\n",
        "  regressor = model().fit(X_train,y_train)\n",
        "  y_pred = regressor.predict(X_test)\n",
        "  model_mse = mean_squared_error(y_pred,y_test)\n",
        "  model_mae = mean_absolute_error(y_pred,y_test)\n",
        "  model_scores[model] = (regressor,y_pred,model_mse,model_mae)\n",
        "  print(model_mse,'\\n',model_mae)\n",
        "\n",
        "############################################# Averaging Method\n",
        "#############################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import xgboost as xgb\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        " \n",
        "# initializing all the model objects with default parameters\n",
        "model_1 = GradientBoostingRegressor()\n",
        "model_2 = SVR()\n",
        "model_3 = RandomForestRegressor()\n",
        " \n",
        "# training all the model on the training dataset\n",
        "model_1.fit(X_train, y_train)\n",
        "model_2.fit(X_train, y_train)\n",
        "model_3.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the validation dataset\n",
        "pred_1 = model_1.predict(X_test)\n",
        "pred_2 = model_2.predict(X_test)\n",
        "pred_3 = model_3.predict(X_test)\n",
        " \n",
        "# final prediction after averaging on the prediction of all 3 models\n",
        "pred_final = (pred_1+pred_2+pred_3)/3.0\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "mod1 = (mean_absolute_error(pred_1, y_test))\n",
        "mod2 = (mean_absolute_error(pred_2, y_test))\n",
        "mod3 = (mean_absolute_error(pred_3, y_test))\n",
        "#print(mean_absolute_error(pred_final, y_test))  ##switch Y_pred and test\n",
        "#Averaging_model = (mod1+mod2+mod3)/3.0\n",
        "Avg_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Avg_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('AVERAGING')\n",
        "print(\"Averaging Model MAE:\", Avg_mod)\n",
        "print(\"Averaging Model MSE:\", Avg_mod1)\n",
        "\n",
        "############################################# Boosting Method\n",
        "#############################################\n",
        "\n",
        "# importing utility modules\n",
        "import pandas as pd\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        " \n",
        "# initializing the boosting module with default parameters\n",
        "model = GradientBoostingRegressor() #163\n",
        " \n",
        "# training the model on the train dataset\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Boost_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Boost_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('BOOSTING')\n",
        "print(\"Averaging Model MAE:\", Boost_mod)\n",
        "print(\"Averaging Model MSE:\", Boost_mod1)\n",
        "print('BAGGING')\n",
        "\n",
        "################################################# Bagging Method\n",
        "#################################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "import xgboost as xgb\n",
        " \n",
        "# importing bagging module\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "\n",
        "# initializing the bagging model using XGboost as base model with default parameters\n",
        "model = BaggingRegressor(base_estimator=xgb.XGBRegressor())\n",
        " \n",
        "# training model\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Bag_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Bag_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print(\"Bagging Model MAE:\", Bag_mod)\n",
        "print(\"Bagging Model MSE:\", Bag_mod1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ax6VncLWAWNM",
        "outputId": "a4c4a088-0a9b-4195-8d7d-a505c3a22093"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'sklearn.ensemble._forest.RandomForestRegressor'>\n",
            "347722020.84280556 \n",
            " 12853.290071657142\n",
            "<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>\n",
            "286805903.9914925 \n",
            " 11710.967110891379\n",
            "<class 'sklearn.svm._classes.SVR'>\n",
            "371272296.96822786 \n",
            " 10429.090494737726\n",
            "AVERAGING\n",
            "Averaging Model MAE: 10999.605929164836\n",
            "Averaging Model MSE: 264608311.48600107\n",
            "BOOSTING\n",
            "Averaging Model MAE: 11999.814138810272\n",
            "Averaging Model MSE: 302907138.42652404\n",
            "BAGGING\n",
            "[03:13:04] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:13:04] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:13:04] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:13:04] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:13:04] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:13:04] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:13:04] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:13:04] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:13:05] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:13:05] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "Bagging Model MAE: 11402.65338982515\n",
            "Bagging Model MSE: 255317787.9021682\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Seasons 10-12"
      ],
      "metadata": {
        "id": "6xjYLKTUNjkU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Fire size 0-.25"
      ],
      "metadata": {
        "id": "H0SqQ8v_NmSL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "######### Spliting by size\n",
        "\n",
        "Size_sample1 = Fires_sample[Fires_sample['FIRE_SIZE'] <= .25]\n",
        "\n",
        "########### Spliting by month only needed when switching seasons\n",
        "Season_sample1 = Size_sample1[Size_sample1['DISCOVERY_MONTH'] >= 10]\n",
        "\n",
        "################ Train test split\n",
        "\n",
        "Fires12 = Season_sample1[Season_sample1['FIRE_YEAR'] <= 2017].copy()\n",
        "Fires13 = Season_sample1[Season_sample1['FIRE_YEAR'] > 2017].copy()\n",
        "X_train = Fires12.loc[:, Fires12.columns != 'FIRE_SIZE']\n",
        "y_train = Fires12['FIRE_SIZE'].copy()\n",
        "X_test = Fires13.loc[:, Fires13.columns != 'FIRE_SIZE']\n",
        "y_test = Fires13['FIRE_SIZE'].copy()\n",
        "\n",
        "############################################ Testing best methods\n",
        "\n",
        "model_list = [RandomForestRegressor, GradientBoostingRegressor, SVR] #GaussianNB, \n",
        "model_scores = dict()\n",
        "for model in model_list:\n",
        "  # fit model to data\n",
        "  print(model)\n",
        "  regressor = model().fit(X_train,y_train)\n",
        "  y_pred = regressor.predict(X_test)\n",
        "  model_mse = mean_squared_error(y_pred,y_test)\n",
        "  model_mae = mean_absolute_error(y_pred,y_test)\n",
        "  model_scores[model] = (regressor,y_pred,model_mse,model_mae)\n",
        "  print(model_mse,'\\n',model_mae)\n",
        "\n",
        "############################################# Averaging Method\n",
        "#############################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import xgboost as xgb\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        " \n",
        "# initializing all the model objects with default parameters\n",
        "model_1 = GradientBoostingRegressor()\n",
        "model_2 = SVR()\n",
        "model_3 = RandomForestRegressor()\n",
        " \n",
        "# training all the model on the training dataset\n",
        "model_1.fit(X_train, y_train)\n",
        "model_2.fit(X_train, y_train)\n",
        "model_3.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the validation dataset\n",
        "pred_1 = model_1.predict(X_test)\n",
        "pred_2 = model_2.predict(X_test)\n",
        "pred_3 = model_3.predict(X_test)\n",
        " \n",
        "# final prediction after averaging on the prediction of all 3 models\n",
        "pred_final = (pred_1+pred_2+pred_3)/3.0\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "mod1 = (mean_absolute_error(pred_1, y_test))\n",
        "mod2 = (mean_absolute_error(pred_2, y_test))\n",
        "mod3 = (mean_absolute_error(pred_3, y_test))\n",
        "#print(mean_absolute_error(pred_final, y_test))  ##switch Y_pred and test\n",
        "#Averaging_model = (mod1+mod2+mod3)/3.0\n",
        "Avg_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Avg_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('AVERAGING')\n",
        "print(\"Averaging Model MAE:\", Avg_mod)\n",
        "print(\"Averaging Model MSE:\", Avg_mod1)\n",
        "\n",
        "############################################# Boosting Method\n",
        "#############################################\n",
        "\n",
        "# importing utility modules\n",
        "import pandas as pd\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        " \n",
        "# initializing the boosting module with default parameters\n",
        "model = GradientBoostingRegressor() #163\n",
        " \n",
        "# training the model on the train dataset\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Boost_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Boost_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('BOOSTING')\n",
        "print(\"Averaging Model MAE:\", Boost_mod)\n",
        "print(\"Averaging Model MSE:\", Boost_mod1)\n",
        "print('BAGGING')\n",
        "\n",
        "################################################# Bagging Method\n",
        "#################################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "import xgboost as xgb\n",
        " \n",
        "# importing bagging module\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "\n",
        "# initializing the bagging model using XGboost as base model with default parameters\n",
        "model = BaggingRegressor(base_estimator=xgb.XGBRegressor())\n",
        " \n",
        "# training model\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Bag_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Bag_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print(\"Bagging Model MAE:\", Bag_mod)\n",
        "print(\"Bagging Model MSE:\", Bag_mod1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0oTJHM-O6HF",
        "outputId": "cb592fae-e643-42dc-8526-c11966123437"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'sklearn.ensemble._forest.RandomForestRegressor'>\n",
            "0.0031407622751331785 \n",
            " 0.03637226407914755\n",
            "<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>\n",
            "0.0030379153934418776 \n",
            " 0.03646290371416098\n",
            "<class 'sklearn.svm._classes.SVR'>\n",
            "0.004939317018153144 \n",
            " 0.052709070702900145\n",
            "AVERAGING\n",
            "Averaging Model MAE: 0.03971089118717715\n",
            "Averaging Model MSE: 0.0032484340284229937\n",
            "BOOSTING\n",
            "Averaging Model MAE: 0.036462903714160964\n",
            "Averaging Model MSE: 0.0030379153934418776\n",
            "BAGGING\n",
            "[03:14:06] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:14:08] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:14:09] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:14:10] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:14:12] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:14:13] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:14:15] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:14:16] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:14:18] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:14:19] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "Bagging Model MAE: 0.03643222494420668\n",
            "Bagging Model MSE: 0.003019962603768222\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### .25-1"
      ],
      "metadata": {
        "id": "kVaOzCJtO6f6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "######### Spliting by size\n",
        "\n",
        "Size_sample10 = Fires_sample[(Fires_sample[\"FIRE_SIZE\"] > .25) & (Fires_sample[\"FIRE_SIZE\"] <= 1)]\n",
        "\n",
        "########### Spliting by month only needed when switching seasons\n",
        "Season_sample10 = Size_sample10[Size_sample10['DISCOVERY_MONTH'] >= 10]\n",
        "# Season_sample50 = Size_sample50[(Size_sample50[\"DISCOVERY_MONTH\"] > 1) & (Size_sample10[\"DISCOVERY_MONTH\"] <= 3)]\n",
        "\n",
        "################ Train test split\n",
        "\n",
        "Fires12 = Season_sample10[Season_sample10['FIRE_YEAR'] <= 2017].copy()\n",
        "Fires13 = Season_sample10[Season_sample10['FIRE_YEAR'] > 2017].copy()\n",
        "X_train = Fires12.loc[:, Fires12.columns != 'FIRE_SIZE']\n",
        "y_train = Fires12['FIRE_SIZE'].copy()\n",
        "X_test = Fires13.loc[:, Fires13.columns != 'FIRE_SIZE']\n",
        "y_test = Fires13['FIRE_SIZE'].copy()\n",
        "\n",
        "############################################ Testing best methods\n",
        "\n",
        "model_list = [RandomForestRegressor, GradientBoostingRegressor, SVR] #GaussianNB, \n",
        "model_scores = dict()\n",
        "for model in model_list:\n",
        "  # fit model to data\n",
        "  print(model)\n",
        "  regressor = model().fit(X_train,y_train)\n",
        "  y_pred = regressor.predict(X_test)\n",
        "  model_mse = mean_squared_error(y_pred,y_test)\n",
        "  model_mae = mean_absolute_error(y_pred,y_test)\n",
        "  model_scores[model] = (regressor,y_pred,model_mse,model_mae)\n",
        "  print(model_mse,'\\n',model_mae)\n",
        "\n",
        "############################################# Averaging Method\n",
        "#############################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import xgboost as xgb\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        " \n",
        "# initializing all the model objects with default parameters\n",
        "model_1 = GradientBoostingRegressor()\n",
        "model_2 = SVR()\n",
        "model_3 = RandomForestRegressor()\n",
        " \n",
        "# training all the model on the training dataset\n",
        "model_1.fit(X_train, y_train)\n",
        "model_2.fit(X_train, y_train)\n",
        "model_3.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the validation dataset\n",
        "pred_1 = model_1.predict(X_test)\n",
        "pred_2 = model_2.predict(X_test)\n",
        "pred_3 = model_3.predict(X_test)\n",
        " \n",
        "# final prediction after averaging on the prediction of all 3 models\n",
        "pred_final = (pred_1+pred_2+pred_3)/3.0\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "mod1 = (mean_absolute_error(pred_1, y_test))\n",
        "mod2 = (mean_absolute_error(pred_2, y_test))\n",
        "mod3 = (mean_absolute_error(pred_3, y_test))\n",
        "#print(mean_absolute_error(pred_final, y_test))  ##switch Y_pred and test\n",
        "#Averaging_model = (mod1+mod2+mod3)/3.0\n",
        "Avg_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Avg_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('AVERAGING')\n",
        "print(\"Averaging Model MAE:\", Avg_mod)\n",
        "print(\"Averaging Model MSE:\", Avg_mod1)\n",
        "\n",
        "############################################# Boosting Method\n",
        "#############################################\n",
        "\n",
        "# importing utility modules\n",
        "import pandas as pd\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        " \n",
        "# initializing the boosting module with default parameters\n",
        "model = GradientBoostingRegressor() #163\n",
        " \n",
        "# training the model on the train dataset\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Boost_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Boost_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('BOOSTING')\n",
        "print(\"Averaging Model MAE:\", Boost_mod)\n",
        "print(\"Averaging Model MSE:\", Boost_mod1)\n",
        "print('BAGGING')\n",
        "\n",
        "################################################# Bagging Method\n",
        "#################################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "import xgboost as xgb\n",
        " \n",
        "# importing bagging module\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "\n",
        "# initializing the bagging model using XGboost as base model with default parameters\n",
        "model = BaggingRegressor(base_estimator=xgb.XGBRegressor())\n",
        " \n",
        "# training model\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Bag_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Bag_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print(\"Bagging Model MAE:\", Bag_mod)\n",
        "print(\"Bagging Model MSE:\", Bag_mod1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZExk7qHO8I3",
        "outputId": "ac3e34bd-67f5-454d-ba21-07af11c450c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'sklearn.ensemble._forest.RandomForestRegressor'>\n",
            "0.0770247881013889 \n",
            " 0.2367867483660131\n",
            "<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>\n",
            "0.07247667289718404 \n",
            " 0.2436974379283289\n",
            "<class 'sklearn.svm._classes.SVR'>\n",
            "0.08453518308838893 \n",
            " 0.24716685049033646\n",
            "AVERAGING\n",
            "Averaging Model MAE: 0.23866622517413272\n",
            "Averaging Model MSE: 0.0694774787346881\n",
            "BOOSTING\n",
            "Averaging Model MAE: 0.24369743792832896\n",
            "Averaging Model MSE: 0.07247667289718404\n",
            "BAGGING\n",
            "[03:15:53] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:15:54] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:15:56] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:15:57] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:15:58] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:15:59] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:16:00] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:16:01] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:16:02] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:16:03] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "Bagging Model MAE: 0.24532371986333062\n",
            "Bagging Model MSE: 0.07307811328032023\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### fire size 1-5"
      ],
      "metadata": {
        "id": "sIjTNanSO8XW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "######### Spliting by size\n",
        "\n",
        "Size_sample20 = Fires_sample[(Fires_sample[\"FIRE_SIZE\"] > 1) & (Fires_sample[\"FIRE_SIZE\"] <= 5)]\n",
        "\n",
        "########### Spliting by month only needed when switching seasons\n",
        "Season_sample20 = Size_sample20[Size_sample20['DISCOVERY_MONTH'] >= 10]\n",
        "# Season_sample50 = Size_sample50[(Size_sample50[\"DISCOVERY_MONTH\"] > 1) & (Size_sample10[\"DISCOVERY_MONTH\"] <= 3)]\n",
        "\n",
        "################ Train test split\n",
        "\n",
        "Fires12 = Season_sample20[Season_sample20['FIRE_YEAR'] <= 2017].copy()\n",
        "Fires13 = Season_sample20[Season_sample20['FIRE_YEAR'] > 2017].copy()\n",
        "X_train = Fires12.loc[:, Fires12.columns != 'FIRE_SIZE']\n",
        "y_train = Fires12['FIRE_SIZE'].copy()\n",
        "X_test = Fires13.loc[:, Fires13.columns != 'FIRE_SIZE']\n",
        "y_test = Fires13['FIRE_SIZE'].copy()\n",
        "\n",
        "############################################ Testing best methods\n",
        "\n",
        "model_list = [RandomForestRegressor, GradientBoostingRegressor, SVR] #GaussianNB, \n",
        "model_scores = dict()\n",
        "for model in model_list:\n",
        "  # fit model to data\n",
        "  print(model)\n",
        "  regressor = model().fit(X_train,y_train)\n",
        "  y_pred = regressor.predict(X_test)\n",
        "  model_mse = mean_squared_error(y_pred,y_test)\n",
        "  model_mae = mean_absolute_error(y_pred,y_test)\n",
        "  model_scores[model] = (regressor,y_pred,model_mse,model_mae)\n",
        "  print(model_mse,'\\n',model_mae)\n",
        "\n",
        "############################################# Averaging Method\n",
        "#############################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import xgboost as xgb\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        " \n",
        "# initializing all the model objects with default parameters\n",
        "model_1 = GradientBoostingRegressor()\n",
        "model_2 = SVR()\n",
        "model_3 = RandomForestRegressor()\n",
        " \n",
        "# training all the model on the training dataset\n",
        "model_1.fit(X_train, y_train)\n",
        "model_2.fit(X_train, y_train)\n",
        "model_3.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the validation dataset\n",
        "pred_1 = model_1.predict(X_test)\n",
        "pred_2 = model_2.predict(X_test)\n",
        "pred_3 = model_3.predict(X_test)\n",
        " \n",
        "# final prediction after averaging on the prediction of all 3 models\n",
        "pred_final = (pred_1+pred_2+pred_3)/3.0\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "mod1 = (mean_absolute_error(pred_1, y_test))\n",
        "mod2 = (mean_absolute_error(pred_2, y_test))\n",
        "mod3 = (mean_absolute_error(pred_3, y_test))\n",
        "#print(mean_absolute_error(pred_final, y_test))  ##switch Y_pred and test\n",
        "#Averaging_model = (mod1+mod2+mod3)/3.0\n",
        "Avg_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Avg_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('AVERAGING')\n",
        "print(\"Averaging Model MAE:\", Avg_mod)\n",
        "print(\"Averaging Model MSE:\", Avg_mod1)\n",
        "\n",
        "############################################# Boosting Method\n",
        "#############################################\n",
        "\n",
        "# importing utility modules\n",
        "import pandas as pd\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        " \n",
        "# initializing the boosting module with default parameters\n",
        "model = GradientBoostingRegressor() #163\n",
        " \n",
        "# training the model on the train dataset\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Boost_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Boost_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('BOOSTING')\n",
        "print(\"Averaging Model MAE:\", Boost_mod)\n",
        "print(\"Averaging Model MSE:\", Boost_mod1)\n",
        "print('BAGGING')\n",
        "\n",
        "################################################# Bagging Method\n",
        "#################################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "import xgboost as xgb\n",
        " \n",
        "# importing bagging module\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "\n",
        "# initializing the bagging model using XGboost as base model with default parameters\n",
        "model = BaggingRegressor(base_estimator=xgb.XGBRegressor())\n",
        " \n",
        "# training model\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Bag_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Bag_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print(\"Bagging Model MAE:\", Bag_mod)\n",
        "print(\"Bagging Model MSE:\", Bag_mod1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbSWjkeCO8pa",
        "outputId": "0df2d110-a321-4470-f5b9-e8fd0518cf6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'sklearn.ensemble._forest.RandomForestRegressor'>\n",
            "1.46911285504651 \n",
            " 1.0100797222222222\n",
            "<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>\n",
            "1.3046047150725282 \n",
            " 0.9483101992380509\n",
            "<class 'sklearn.svm._classes.SVR'>\n",
            "1.4247283543457763 \n",
            " 0.9659622533606248\n",
            "AVERAGING\n",
            "Averaging Model MAE: 0.9379480108889373\n",
            "Averaging Model MSE: 1.3108072756744036\n",
            "BOOSTING\n",
            "Averaging Model MAE: 0.9506983105392889\n",
            "Averaging Model MSE: 1.3065631191434808\n",
            "BAGGING\n",
            "[03:17:14] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:17:15] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:17:15] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:17:16] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:17:17] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:17:18] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:17:19] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:17:20] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:17:21] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:17:22] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "Bagging Model MAE: 0.9454304453042838\n",
            "Bagging Model MSE: 1.2934357982995677\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Fire size 5-20"
      ],
      "metadata": {
        "id": "ZlelHv60O81H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "######### Spliting by size\n",
        "\n",
        "Size_sample30 = Fires_sample[(Fires_sample[\"FIRE_SIZE\"] > 5) & (Fires_sample[\"FIRE_SIZE\"] <= 20)]\n",
        "\n",
        "########### Spliting by month only needed when switching seasons\n",
        "Season_sample30 = Size_sample30[Size_sample30['DISCOVERY_MONTH'] >= 10]\n",
        "# Season_sample50 = Size_sample50[(Size_sample50[\"DISCOVERY_MONTH\"] > 1) & (Size_sample10[\"DISCOVERY_MONTH\"] <= 3)]\n",
        "\n",
        "################ Train test split\n",
        "\n",
        "Fires12 = Season_sample30[Season_sample30['FIRE_YEAR'] <= 2017].copy()\n",
        "Fires13 = Season_sample30[Season_sample30['FIRE_YEAR'] > 2017].copy()\n",
        "X_train = Fires12.loc[:, Fires12.columns != 'FIRE_SIZE']\n",
        "y_train = Fires12['FIRE_SIZE'].copy()\n",
        "X_test = Fires13.loc[:, Fires13.columns != 'FIRE_SIZE']\n",
        "y_test = Fires13['FIRE_SIZE'].copy()\n",
        "\n",
        "############################################ Testing best methods\n",
        "\n",
        "model_list = [RandomForestRegressor, GradientBoostingRegressor, SVR] #GaussianNB, \n",
        "model_scores = dict()\n",
        "for model in model_list:\n",
        "  # fit model to data\n",
        "  print(model)\n",
        "  regressor = model().fit(X_train,y_train)\n",
        "  y_pred = regressor.predict(X_test)\n",
        "  model_mse = mean_squared_error(y_pred,y_test)\n",
        "  model_mae = mean_absolute_error(y_pred,y_test)\n",
        "  model_scores[model] = (regressor,y_pred,model_mse,model_mae)\n",
        "  print(model_mse,'\\n',model_mae)\n",
        "\n",
        "############################################# Averaging Method\n",
        "#############################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import xgboost as xgb\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        " \n",
        "# initializing all the model objects with default parameters\n",
        "model_1 = GradientBoostingRegressor()\n",
        "model_2 = SVR()\n",
        "model_3 = RandomForestRegressor()\n",
        " \n",
        "# training all the model on the training dataset\n",
        "model_1.fit(X_train, y_train)\n",
        "model_2.fit(X_train, y_train)\n",
        "model_3.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the validation dataset\n",
        "pred_1 = model_1.predict(X_test)\n",
        "pred_2 = model_2.predict(X_test)\n",
        "pred_3 = model_3.predict(X_test)\n",
        " \n",
        "# final prediction after averaging on the prediction of all 3 models\n",
        "pred_final = (pred_1+pred_2+pred_3)/3.0\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "mod1 = (mean_absolute_error(pred_1, y_test))\n",
        "mod2 = (mean_absolute_error(pred_2, y_test))\n",
        "mod3 = (mean_absolute_error(pred_3, y_test))\n",
        "#print(mean_absolute_error(pred_final, y_test))  ##switch Y_pred and test\n",
        "#Averaging_model = (mod1+mod2+mod3)/3.0\n",
        "Avg_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Avg_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('AVERAGING')\n",
        "print(\"Averaging Model MAE:\", Avg_mod)\n",
        "print(\"Averaging Model MSE:\", Avg_mod1)\n",
        "\n",
        "############################################# Boosting Method\n",
        "#############################################\n",
        "\n",
        "# importing utility modules\n",
        "import pandas as pd\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        " \n",
        "# initializing the boosting module with default parameters\n",
        "model = GradientBoostingRegressor() #163\n",
        " \n",
        "# training the model on the train dataset\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Boost_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Boost_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('BOOSTING')\n",
        "print(\"Averaging Model MAE:\", Boost_mod)\n",
        "print(\"Averaging Model MSE:\", Boost_mod1)\n",
        "print('BAGGING')\n",
        "\n",
        "################################################# Bagging Method\n",
        "#################################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "import xgboost as xgb\n",
        " \n",
        "# importing bagging module\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "\n",
        "# initializing the bagging model using XGboost as base model with default parameters\n",
        "model = BaggingRegressor(base_estimator=xgb.XGBRegressor())\n",
        " \n",
        "# training model\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Bag_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Bag_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print(\"Bagging Model MAE:\", Bag_mod)\n",
        "print(\"Bagging Model MSE:\", Bag_mod1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzMI33QJPBAH",
        "outputId": "d9f4be4e-b8bf-4f5d-e906-b9ab87a6f4d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'sklearn.ensemble._forest.RandomForestRegressor'>\n",
            "28.84102558687391 \n",
            " 4.648311086956522\n",
            "<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>\n",
            "22.434337605399527 \n",
            " 4.07842696512831\n",
            "<class 'sklearn.svm._classes.SVR'>\n",
            "21.143956047500843 \n",
            " 3.4916614602602665\n",
            "AVERAGING\n",
            "Averaging Model MAE: 4.029384683722652\n",
            "Averaging Model MSE: 22.742543198502837\n",
            "BOOSTING\n",
            "Averaging Model MAE: 4.088548915964197\n",
            "Averaging Model MSE: 22.480142687347048\n",
            "BAGGING\n",
            "[03:17:44] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:17:45] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:17:45] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:17:46] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:17:46] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:17:47] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:17:47] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:17:47] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:17:48] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:17:48] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "Bagging Model MAE: 4.068855280751768\n",
            "Bagging Model MSE: 22.09410529087618\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Fire size 20-50"
      ],
      "metadata": {
        "id": "Aya88ynNPBMl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "########## Spliting by size\n",
        "\n",
        "Size_sample40 = Fires_sample[(Fires_sample[\"FIRE_SIZE\"] > 20) & (Fires_sample[\"FIRE_SIZE\"] <= 50)]\n",
        "\n",
        "########### Spliting by month only needed when switching seasons\n",
        "Season_sample40 = Size_sample40[Size_sample40['DISCOVERY_MONTH'] >= 10]\n",
        "# Season_sample50 = Size_sample50[(Size_sample50[\"DISCOVERY_MONTH\"] > 1) & (Size_sample10[\"DISCOVERY_MONTH\"] <= 3)]\n",
        "\n",
        "################ Train test split\n",
        "\n",
        "Fires12 = Season_sample40[Season_sample40['FIRE_YEAR'] <= 2017].copy()\n",
        "Fires13 = Season_sample40[Season_sample40['FIRE_YEAR'] > 2017].copy()\n",
        "X_train = Fires12.loc[:, Fires12.columns != 'FIRE_SIZE']\n",
        "y_train = Fires12['FIRE_SIZE'].copy()\n",
        "X_test = Fires13.loc[:, Fires13.columns != 'FIRE_SIZE']\n",
        "y_test = Fires13['FIRE_SIZE'].copy()\n",
        "\n",
        "############################################ Testing best methods\n",
        "\n",
        "model_list = [RandomForestRegressor, GradientBoostingRegressor, SVR] #GaussianNB, \n",
        "model_scores = dict()\n",
        "for model in model_list:\n",
        "  # fit model to data\n",
        "  print(model)\n",
        "  regressor = model().fit(X_train,y_train)\n",
        "  y_pred = regressor.predict(X_test)\n",
        "  model_mse = mean_squared_error(y_pred,y_test)\n",
        "  model_mae = mean_absolute_error(y_pred,y_test)\n",
        "  model_scores[model] = (regressor,y_pred,model_mse,model_mae)\n",
        "  print(model_mse,'\\n',model_mae)\n",
        "\n",
        "############################################# Averaging Method\n",
        "#############################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import xgboost as xgb\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        " \n",
        "# initializing all the model objects with default parameters\n",
        "model_1 = GradientBoostingRegressor()\n",
        "model_2 = SVR()\n",
        "model_3 = RandomForestRegressor()\n",
        " \n",
        "# training all the model on the training dataset\n",
        "model_1.fit(X_train, y_train)\n",
        "model_2.fit(X_train, y_train)\n",
        "model_3.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the validation dataset\n",
        "pred_1 = model_1.predict(X_test)\n",
        "pred_2 = model_2.predict(X_test)\n",
        "pred_3 = model_3.predict(X_test)\n",
        " \n",
        "# final prediction after averaging on the prediction of all 3 models\n",
        "pred_final = (pred_1+pred_2+pred_3)/3.0\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "mod1 = (mean_absolute_error(pred_1, y_test))\n",
        "mod2 = (mean_absolute_error(pred_2, y_test))\n",
        "mod3 = (mean_absolute_error(pred_3, y_test))\n",
        "#print(mean_absolute_error(pred_final, y_test))  ##switch Y_pred and test\n",
        "#Averaging_model = (mod1+mod2+mod3)/3.0\n",
        "Avg_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Avg_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('AVERAGING')\n",
        "print(\"Averaging Model MAE:\", Avg_mod)\n",
        "print(\"Averaging Model MSE:\", Avg_mod1)\n",
        "\n",
        "############################################# Boosting Method\n",
        "#############################################\n",
        "\n",
        "# importing utility modules\n",
        "import pandas as pd\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        " \n",
        "# initializing the boosting module with default parameters\n",
        "model = GradientBoostingRegressor() #163\n",
        " \n",
        "# training the model on the train dataset\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Boost_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Boost_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('BOOSTING')\n",
        "print(\"Averaging Model MAE:\", Boost_mod)\n",
        "print(\"Averaging Model MSE:\", Boost_mod1)\n",
        "print('BAGGING')\n",
        "\n",
        "################################################# Bagging Method\n",
        "#################################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "import xgboost as xgb\n",
        " \n",
        "# importing bagging module\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "\n",
        "# initializing the bagging model using XGboost as base model with default parameters\n",
        "model = BaggingRegressor(base_estimator=xgb.XGBRegressor())\n",
        " \n",
        "# training model\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Bag_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Bag_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print(\"Bagging Model MAE:\", Bag_mod)\n",
        "print(\"Bagging Model MSE:\", Bag_mod1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQualB0IPC-f",
        "outputId": "0dcc3b7c-0f18-4959-a3f7-e98ed0455c25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'sklearn.ensemble._forest.RandomForestRegressor'>\n",
            "81.84684228187498 \n",
            " 7.658437499999999\n",
            "<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>\n",
            "65.30679057113304 \n",
            " 7.053403890840588\n",
            "<class 'sklearn.svm._classes.SVR'>\n",
            "57.175116367390586 \n",
            " 6.174693444804559\n",
            "AVERAGING\n",
            "Averaging Model MAE: 6.7394298687697285\n",
            "Averaging Model MSE: 61.315613295291975\n",
            "BOOSTING\n",
            "Averaging Model MAE: 7.085262810744168\n",
            "Averaging Model MSE: 65.76331514214371\n",
            "BAGGING\n",
            "[03:17:55] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:17:55] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:17:56] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:17:56] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:17:56] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:17:56] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:17:56] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:17:56] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:17:57] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:17:57] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "Bagging Model MAE: 6.907906529903412\n",
            "Bagging Model MSE: 61.58431120325915\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Fire size 50-200"
      ],
      "metadata": {
        "id": "xWu3BkDNPDNk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "######### Spliting by size\n",
        "\n",
        "Size_sample50 = Fires_sample[(Fires_sample[\"FIRE_SIZE\"] > 50) & (Fires_sample[\"FIRE_SIZE\"] <= 200)]\n",
        "\n",
        "########### Spliting by month only needed when switching seasons\n",
        "Season_sample50 = Size_sample50[Size_sample50['DISCOVERY_MONTH'] >= 10]\n",
        "# Season_sample50 = Size_sample50[(Size_sample50[\"DISCOVERY_MONTH\"] > 1) & (Size_sample10[\"DISCOVERY_MONTH\"] <= 3)]\n",
        "\n",
        "################ Train test split\n",
        "\n",
        "Fires12 = Season_sample50[Season_sample50['FIRE_YEAR'] <= 2016].copy()\n",
        "Fires13 = Season_sample50[Season_sample50['FIRE_YEAR'] >= 2017].copy()\n",
        "X_train = Fires12.loc[:, Fires12.columns != 'FIRE_SIZE']\n",
        "y_train = Fires12['FIRE_SIZE'].copy()\n",
        "X_test = Fires13.loc[:, Fires13.columns != 'FIRE_SIZE']\n",
        "y_test = Fires13['FIRE_SIZE'].copy()\n",
        "\n",
        "############################################ Testing best methods\n",
        "\n",
        "model_list = [RandomForestRegressor, GradientBoostingRegressor, SVR] #GaussianNB, \n",
        "model_scores = dict()\n",
        "for model in model_list:\n",
        "  # fit model to data\n",
        "  print(model)\n",
        "  regressor = model().fit(X_train,y_train)\n",
        "  y_pred = regressor.predict(X_test)\n",
        "  model_mse = mean_squared_error(y_pred,y_test)\n",
        "  model_mae = mean_absolute_error(y_pred,y_test)\n",
        "  model_scores[model] = (regressor,y_pred,model_mse,model_mae)\n",
        "  print(model_mse,'\\n',model_mae)\n",
        "\n",
        "############################################# Averaging Method\n",
        "#############################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import xgboost as xgb\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        " \n",
        "# initializing all the model objects with default parameters\n",
        "model_1 = GradientBoostingRegressor()\n",
        "model_2 = SVR()\n",
        "model_3 = RandomForestRegressor()\n",
        " \n",
        "# training all the model on the training dataset\n",
        "model_1.fit(X_train, y_train)\n",
        "model_2.fit(X_train, y_train)\n",
        "model_3.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the validation dataset\n",
        "pred_1 = model_1.predict(X_test)\n",
        "pred_2 = model_2.predict(X_test)\n",
        "pred_3 = model_3.predict(X_test)\n",
        " \n",
        "# final prediction after averaging on the prediction of all 3 models\n",
        "pred_final = (pred_1+pred_2+pred_3)/3.0\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "mod1 = (mean_absolute_error(pred_1, y_test))\n",
        "mod2 = (mean_absolute_error(pred_2, y_test))\n",
        "mod3 = (mean_absolute_error(pred_3, y_test))\n",
        "#print(mean_absolute_error(pred_final, y_test))  ##switch Y_pred and test\n",
        "#Averaging_model = (mod1+mod2+mod3)/3.0\n",
        "Avg_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Avg_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('AVERAGING')\n",
        "print(\"Averaging Model MAE:\", Avg_mod)\n",
        "print(\"Averaging Model MSE:\", Avg_mod1)\n",
        "\n",
        "############################################# Boosting Method\n",
        "#############################################\n",
        "\n",
        "# importing utility modules\n",
        "import pandas as pd\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        " \n",
        "# initializing the boosting module with default parameters\n",
        "model = GradientBoostingRegressor() #163\n",
        " \n",
        "# training the model on the train dataset\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Boost_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Boost_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('BOOSTING')\n",
        "print(\"Averaging Model MAE:\", Boost_mod)\n",
        "print(\"Averaging Model MSE:\", Boost_mod1)\n",
        "print('BAGGING')\n",
        "\n",
        "################################################# Bagging Method\n",
        "#################################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "import xgboost as xgb\n",
        " \n",
        "# importing bagging module\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "\n",
        "# initializing the bagging model using XGboost as base model with default parameters\n",
        "model = BaggingRegressor(base_estimator=xgb.XGBRegressor())\n",
        " \n",
        "# training model\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Bag_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Bag_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print(\"Bagging Model MAE:\", Bag_mod)\n",
        "print(\"Bagging Model MSE:\", Bag_mod1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvwkhVz8PFAG",
        "outputId": "36feacee-e5e3-4e02-deee-38cb86c16517"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'sklearn.ensemble._forest.RandomForestRegressor'>\n",
            "2105.197224630103 \n",
            " 39.53464536082474\n",
            "<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>\n",
            "1874.062834234272 \n",
            " 36.64829419895883\n",
            "<class 'sklearn.svm._classes.SVR'>\n",
            "2128.1252473653713 \n",
            " 33.498838869375575\n",
            "AVERAGING\n",
            "Averaging Model MAE: 35.510207982887636\n",
            "Averaging Model MSE: 1855.9457478077636\n",
            "BOOSTING\n",
            "Averaging Model MAE: 36.66608968786993\n",
            "Averaging Model MSE: 1875.331971590912\n",
            "BAGGING\n",
            "[03:18:01] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:18:01] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:18:01] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:18:01] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:18:01] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:18:01] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:18:01] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:18:02] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:18:02] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:18:02] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "Bagging Model MAE: 36.801949126253426\n",
            "Bagging Model MSE: 1846.8502684965983\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Fire size 200-1000"
      ],
      "metadata": {
        "id": "5oILJ-_7PFNr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "########## Spliting by size\n",
        "\n",
        "Size_sample100 = Fires_sample[(Fires_sample[\"FIRE_SIZE\"] > 200) & (Fires_sample[\"FIRE_SIZE\"] <= 1000)]\n",
        "\n",
        "########### Spliting by month only needed when switching seasons\n",
        "Season_sample100 = Size_sample100[Size_sample100['DISCOVERY_MONTH'] >= 10]\n",
        "\n",
        "################ Train test split\n",
        "\n",
        "Fires12 = Season_sample100[Season_sample100['FIRE_YEAR'] <= 2017].copy()\n",
        "Fires13 = Season_sample100[Season_sample100['FIRE_YEAR'] > 2017].copy()\n",
        "X_train = Fires12.loc[:, Fires12.columns != 'FIRE_SIZE']\n",
        "y_train = Fires12['FIRE_SIZE'].copy()\n",
        "X_test = Fires13.loc[:, Fires13.columns != 'FIRE_SIZE']\n",
        "y_test = Fires13['FIRE_SIZE'].copy()\n",
        "\n",
        "############################################ Testing best methods\n",
        "\n",
        "model_list = [RandomForestRegressor, GradientBoostingRegressor, SVR] #GaussianNB, \n",
        "model_scores = dict()\n",
        "for model in model_list:\n",
        "  # fit model to data\n",
        "  print(model)\n",
        "  regressor = model().fit(X_train,y_train)\n",
        "  y_pred = regressor.predict(X_test)\n",
        "  model_mse = mean_squared_error(y_pred,y_test)\n",
        "  model_mae = mean_absolute_error(y_pred,y_test)\n",
        "  model_scores[model] = (regressor,y_pred,model_mse,model_mae)\n",
        "  print(model_mse,'\\n',model_mae)\n",
        "\n",
        "############################################# Averaging Method\n",
        "#############################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import xgboost as xgb\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        " \n",
        "# initializing all the model objects with default parameters\n",
        "model_1 = GradientBoostingRegressor()\n",
        "model_2 = SVR()\n",
        "model_3 = RandomForestRegressor()\n",
        " \n",
        "# training all the model on the training dataset\n",
        "model_1.fit(X_train, y_train)\n",
        "model_2.fit(X_train, y_train)\n",
        "model_3.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the validation dataset\n",
        "pred_1 = model_1.predict(X_test)\n",
        "pred_2 = model_2.predict(X_test)\n",
        "pred_3 = model_3.predict(X_test)\n",
        " \n",
        "# final prediction after averaging on the prediction of all 3 models\n",
        "pred_final = (pred_1+pred_2+pred_3)/3.0\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "mod1 = (mean_absolute_error(pred_1, y_test))\n",
        "mod2 = (mean_absolute_error(pred_2, y_test))\n",
        "mod3 = (mean_absolute_error(pred_3, y_test))\n",
        "#print(mean_absolute_error(pred_final, y_test))  ##switch Y_pred and test\n",
        "#Averaging_model = (mod1+mod2+mod3)/3.0\n",
        "Avg_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Avg_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('AVERAGING')\n",
        "print(\"Averaging Model MAE:\", Avg_mod)\n",
        "print(\"Averaging Model MSE:\", Avg_mod1)\n",
        "\n",
        "############################################# Boosting Method\n",
        "#############################################\n",
        "\n",
        "# importing utility modules\n",
        "import pandas as pd\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        " \n",
        "# initializing the boosting module with default parameters\n",
        "model = GradientBoostingRegressor() #163\n",
        " \n",
        "# training the model on the train dataset\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Boost_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Boost_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('BOOSTING')\n",
        "print(\"Averaging Model MAE:\", Boost_mod)\n",
        "print(\"Averaging Model MSE:\", Boost_mod1)\n",
        "print('BAGGING')\n",
        "\n",
        "################################################# Bagging Method\n",
        "#################################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "import xgboost as xgb\n",
        " \n",
        "# importing bagging module\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "\n",
        "# initializing the bagging model using XGboost as base model with default parameters\n",
        "model = BaggingRegressor(base_estimator=xgb.XGBRegressor())\n",
        " \n",
        "# training model\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Bag_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Bag_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print(\"Bagging Model MAE:\", Bag_mod)\n",
        "print(\"Bagging Model MSE:\", Bag_mod1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8K6x25TPGzB",
        "outputId": "9749e29e-bffa-49f9-dafa-597d80ddb671"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'sklearn.ensemble._forest.RandomForestRegressor'>\n",
            "23657.277799011812 \n",
            " 139.2564090909091\n",
            "<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>\n",
            "23373.25886512689 \n",
            " 131.74943062413539\n",
            "<class 'sklearn.svm._classes.SVR'>\n",
            "41620.23165684834 \n",
            " 182.98677225382772\n",
            "AVERAGING\n",
            "Averaging Model MAE: 142.7995251724635\n",
            "Averaging Model MSE: 25680.977985592897\n",
            "BOOSTING\n",
            "Averaging Model MAE: 131.74943062413539\n",
            "Averaging Model MSE: 23373.25886512689\n",
            "BAGGING\n",
            "[03:18:04] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:18:04] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:18:04] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:18:04] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:18:04] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:18:04] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:18:04] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:18:04] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:18:04] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:18:04] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "Bagging Model MAE: 143.28008145419034\n",
            "Bagging Model MSE: 25329.74571301421\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Fire size 1000+"
      ],
      "metadata": {
        "id": "3IVKcLT_PMsy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "######### Spliting by size\n",
        "\n",
        "Size_sampleinf = Fires_sample[Fires_sample['FIRE_SIZE'] > 1000]\n",
        "\n",
        "########### Spliting by month only needed when switching seasons\n",
        "Season_sampleinf = Size_sampleinf[Size_sampleinf['DISCOVERY_MONTH'] >= 10]\n",
        "# Season_sample50 = Size_sample50[(Size_sample50[\"DISCOVERY_MONTH\"] > 1) & (Size_sample10[\"DISCOVERY_MONTH\"] <= 3)]\n",
        "\n",
        "################ Train test split\n",
        "\n",
        "Fires12 = Season_sampleinf[Season_sampleinf['FIRE_YEAR'] <= 2017].copy()\n",
        "Fires13 = Season_sampleinf[Season_sampleinf['FIRE_YEAR'] > 2017].copy()\n",
        "X_train = Fires12.loc[:, Fires12.columns != 'FIRE_SIZE']\n",
        "y_train = Fires12['FIRE_SIZE'].copy()\n",
        "X_test = Fires13.loc[:, Fires13.columns != 'FIRE_SIZE']\n",
        "y_test = Fires13['FIRE_SIZE'].copy()\n",
        "\n",
        "############################################ Testing best methods\n",
        "\n",
        "model_list = [RandomForestRegressor, GradientBoostingRegressor, SVR] #GaussianNB, \n",
        "model_scores = dict()\n",
        "for model in model_list:\n",
        "  # fit model to data\n",
        "  print(model)\n",
        "  regressor = model().fit(X_train,y_train)\n",
        "  y_pred = regressor.predict(X_test)\n",
        "  model_mse = mean_squared_error(y_pred,y_test)\n",
        "  model_mae = mean_absolute_error(y_pred,y_test)\n",
        "  model_scores[model] = (regressor,y_pred,model_mse,model_mae)\n",
        "  print(model_mse,'\\n',model_mae)\n",
        "\n",
        "############################################# Averaging Method\n",
        "#############################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import xgboost as xgb\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        " \n",
        "# initializing all the model objects with default parameters\n",
        "model_1 = GradientBoostingRegressor()\n",
        "model_2 = SVR()\n",
        "model_3 = RandomForestRegressor()\n",
        " \n",
        "# training all the model on the training dataset\n",
        "model_1.fit(X_train, y_train)\n",
        "model_2.fit(X_train, y_train)\n",
        "model_3.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the validation dataset\n",
        "pred_1 = model_1.predict(X_test)\n",
        "pred_2 = model_2.predict(X_test)\n",
        "pred_3 = model_3.predict(X_test)\n",
        " \n",
        "# final prediction after averaging on the prediction of all 3 models\n",
        "pred_final = (pred_1+pred_2+pred_3)/3.0\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "mod1 = (mean_absolute_error(pred_1, y_test))\n",
        "mod2 = (mean_absolute_error(pred_2, y_test))\n",
        "mod3 = (mean_absolute_error(pred_3, y_test))\n",
        "#print(mean_absolute_error(pred_final, y_test))  ##switch Y_pred and test\n",
        "#Averaging_model = (mod1+mod2+mod3)/3.0\n",
        "Avg_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Avg_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('AVERAGING')\n",
        "print(\"Averaging Model MAE:\", Avg_mod)\n",
        "print(\"Averaging Model MSE:\", Avg_mod1)\n",
        "\n",
        "############################################# Boosting Method\n",
        "#############################################\n",
        "\n",
        "# importing utility modules\n",
        "import pandas as pd\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        " \n",
        "# initializing the boosting module with default parameters\n",
        "model = GradientBoostingRegressor() #163\n",
        " \n",
        "# training the model on the train dataset\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Boost_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Boost_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('BOOSTING')\n",
        "print(\"Averaging Model MAE:\", Boost_mod)\n",
        "print(\"Averaging Model MSE:\", Boost_mod1)\n",
        "print('BAGGING')\n",
        "\n",
        "################################################# Bagging Method\n",
        "#################################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "import xgboost as xgb\n",
        " \n",
        "# importing bagging module\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "\n",
        "# initializing the bagging model using XGboost as base model with default parameters\n",
        "model = BaggingRegressor(base_estimator=xgb.XGBRegressor())\n",
        " \n",
        "# training model\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Bag_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Bag_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print(\"Bagging Model MAE:\", Bag_mod)\n",
        "print(\"Bagging Model MSE:\", Bag_mod1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvJQbnhHPOWv",
        "outputId": "c273e74a-41eb-4196-91be-9b9161c08072"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'sklearn.ensemble._forest.RandomForestRegressor'>\n",
            "2165982303.5142274 \n",
            " 27538.4431875\n",
            "<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>\n",
            "1931744182.2240143 \n",
            " 23185.090616356465\n",
            "<class 'sklearn.svm._classes.SVR'>\n",
            "2863501794.8580704 \n",
            " 19467.23479166151\n",
            "AVERAGING\n",
            "Averaging Model MAE: 22060.865806501104\n",
            "Averaging Model MSE: 2185962778.457427\n",
            "BOOSTING\n",
            "Averaging Model MAE: 22344.496101403634\n",
            "Averaging Model MSE: 1962083649.8262053\n",
            "BAGGING\n",
            "[03:18:05] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:18:05] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:18:05] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:18:05] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:18:05] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:18:05] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:18:05] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:18:05] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:18:05] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:18:05] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "Bagging Model MAE: 33024.40297851562\n",
            "Bagging Model MSE: 2121682625.351288\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Geography"
      ],
      "metadata": {
        "id": "WLizc0d-Oa4o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "state_enc.classes_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xd9i6DkgQEXD",
        "outputId": "a885334e-b4c0-4395-93e8-42dd8cfe86a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['AK', 'AL', 'AR', 'AZ', 'CA', 'CO', 'CT', 'DC', 'DE', 'FL', 'GA',\n",
              "       'HI', 'IA', 'ID', 'IL', 'IN', 'KS', 'KY', 'LA', 'MA', 'MD', 'ME',\n",
              "       'MI', 'MN', 'MO', 'MS', 'MT', 'NC', 'ND', 'NE', 'NH', 'NJ', 'NM',\n",
              "       'NV', 'NY', 'OH', 'OK', 'OR', 'PA', 'PR', 'RI', 'SC', 'SD', 'TN',\n",
              "       'TX', 'UT', 'VA', 'VT', 'WA', 'WI', 'WV', 'WY'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### MidWest"
      ],
      "metadata": {
        "id": "6RFQEXfUIPa2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MidWest = Fires[(Fires['STATE_ORD'] == 15) | (Fires['STATE_ORD'] == 16) | (Fires['STATE_ORD'] == 13) | (Fires['STATE_ORD'] == 17) | (Fires['STATE_ORD'] == 23) | (Fires['STATE_ORD'] == 24) | (Fires['STATE_ORD'] == 25) | (Fires['STATE_ORD'] == 29) | (Fires['STATE_ORD'] == 30) | (Fires['STATE_ORD'] == 36) | (Fires['STATE_ORD'] == 43) | (Fires['STATE_ORD'] == 50)]"
      ],
      "metadata": {
        "id": "T7WtgoVrQGXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MidWest"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "id": "T6f0YlcyRsoG",
        "outputId": "12f9a406-51c3-477e-a220-1dc3922dc5c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           FIRE_YEAR  FIRE_SIZE   LATITUDE   LONGITUDE  FIPS_CODE  \\\n",
              "FOD_ID                                                              \n",
              "155             2005        0.1  44.488611 -111.256111      16043   \n",
              "172             2005        0.1  42.736389 -112.384444      16005   \n",
              "176             2005        0.1  42.839722 -112.176667      16005   \n",
              "177             2005        0.1  42.691389 -112.368611      16005   \n",
              "178             2005        0.1  42.736389 -112.072222      16005   \n",
              "...              ...        ...        ...         ...        ...   \n",
              "400482072       2017      150.0  36.591667  -95.362833      40131   \n",
              "400482073       2015      148.0  30.553889  -89.066111      28047   \n",
              "400482074       2016      146.0  35.328889  -97.105556      40125   \n",
              "400482078       2016      125.0  36.431667  -84.421667      47151   \n",
              "400482086       2015      109.0  34.794167  -94.958000      40079   \n",
              "\n",
              "           DISCOVERY_MONTH  NWCG_CAUSE_CLASSIFICATION_ORD  DAYS_TO_CONT  \\\n",
              "FOD_ID                                                                    \n",
              "155                      7                              9      0.000000   \n",
              "172                      7                             11      0.000000   \n",
              "176                      7                             11      0.000000   \n",
              "177                      7                             11      0.000000   \n",
              "178                      7                             11      0.000000   \n",
              "...                    ...                            ...           ...   \n",
              "400482072                3                              5      0.923788   \n",
              "400482073               10                              5      0.923788   \n",
              "400482074               12                              5      0.923788   \n",
              "400482078               10                              5      0.923788   \n",
              "400482086                7                              5      0.923788   \n",
              "\n",
              "           STATE_ORD  \n",
              "FOD_ID                \n",
              "155               13  \n",
              "172               13  \n",
              "176               13  \n",
              "177               13  \n",
              "178               13  \n",
              "...              ...  \n",
              "400482072         36  \n",
              "400482073         25  \n",
              "400482074         36  \n",
              "400482078         43  \n",
              "400482086         36  \n",
              "\n",
              "[361997 rows x 9 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-97227470-ee91-447f-94fa-054385220ac4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>FIRE_YEAR</th>\n",
              "      <th>FIRE_SIZE</th>\n",
              "      <th>LATITUDE</th>\n",
              "      <th>LONGITUDE</th>\n",
              "      <th>FIPS_CODE</th>\n",
              "      <th>DISCOVERY_MONTH</th>\n",
              "      <th>NWCG_CAUSE_CLASSIFICATION_ORD</th>\n",
              "      <th>DAYS_TO_CONT</th>\n",
              "      <th>STATE_ORD</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>FOD_ID</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>155</th>\n",
              "      <td>2005</td>\n",
              "      <td>0.1</td>\n",
              "      <td>44.488611</td>\n",
              "      <td>-111.256111</td>\n",
              "      <td>16043</td>\n",
              "      <td>7</td>\n",
              "      <td>9</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>172</th>\n",
              "      <td>2005</td>\n",
              "      <td>0.1</td>\n",
              "      <td>42.736389</td>\n",
              "      <td>-112.384444</td>\n",
              "      <td>16005</td>\n",
              "      <td>7</td>\n",
              "      <td>11</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>176</th>\n",
              "      <td>2005</td>\n",
              "      <td>0.1</td>\n",
              "      <td>42.839722</td>\n",
              "      <td>-112.176667</td>\n",
              "      <td>16005</td>\n",
              "      <td>7</td>\n",
              "      <td>11</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>177</th>\n",
              "      <td>2005</td>\n",
              "      <td>0.1</td>\n",
              "      <td>42.691389</td>\n",
              "      <td>-112.368611</td>\n",
              "      <td>16005</td>\n",
              "      <td>7</td>\n",
              "      <td>11</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>178</th>\n",
              "      <td>2005</td>\n",
              "      <td>0.1</td>\n",
              "      <td>42.736389</td>\n",
              "      <td>-112.072222</td>\n",
              "      <td>16005</td>\n",
              "      <td>7</td>\n",
              "      <td>11</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>400482072</th>\n",
              "      <td>2017</td>\n",
              "      <td>150.0</td>\n",
              "      <td>36.591667</td>\n",
              "      <td>-95.362833</td>\n",
              "      <td>40131</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>0.923788</td>\n",
              "      <td>36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>400482073</th>\n",
              "      <td>2015</td>\n",
              "      <td>148.0</td>\n",
              "      <td>30.553889</td>\n",
              "      <td>-89.066111</td>\n",
              "      <td>28047</td>\n",
              "      <td>10</td>\n",
              "      <td>5</td>\n",
              "      <td>0.923788</td>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>400482074</th>\n",
              "      <td>2016</td>\n",
              "      <td>146.0</td>\n",
              "      <td>35.328889</td>\n",
              "      <td>-97.105556</td>\n",
              "      <td>40125</td>\n",
              "      <td>12</td>\n",
              "      <td>5</td>\n",
              "      <td>0.923788</td>\n",
              "      <td>36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>400482078</th>\n",
              "      <td>2016</td>\n",
              "      <td>125.0</td>\n",
              "      <td>36.431667</td>\n",
              "      <td>-84.421667</td>\n",
              "      <td>47151</td>\n",
              "      <td>10</td>\n",
              "      <td>5</td>\n",
              "      <td>0.923788</td>\n",
              "      <td>43</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>400482086</th>\n",
              "      <td>2015</td>\n",
              "      <td>109.0</td>\n",
              "      <td>34.794167</td>\n",
              "      <td>-94.958000</td>\n",
              "      <td>40079</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>0.923788</td>\n",
              "      <td>36</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>361997 rows × 9 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-97227470-ee91-447f-94fa-054385220ac4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-97227470-ee91-447f-94fa-054385220ac4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-97227470-ee91-447f-94fa-054385220ac4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Fire Size 0-.25"
      ],
      "metadata": {
        "id": "VrefjPHkH0DW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MidWest1 = MidWest[MidWest['FIRE_SIZE'] <= .25]\n",
        "################ Train test split\n",
        "\n",
        "Fires12 = MidWest1[MidWest1['FIRE_YEAR'] <= 2017].copy()\n",
        "Fires13 = MidWest1[MidWest1['FIRE_YEAR'] > 2017].copy()\n",
        "X_train = Fires12.loc[:, Fires12.columns != 'FIRE_SIZE']\n",
        "y_train = Fires12['FIRE_SIZE'].copy()\n",
        "X_test = Fires13.loc[:, Fires13.columns != 'FIRE_SIZE']\n",
        "y_test = Fires13['FIRE_SIZE'].copy()\n",
        "\n",
        "############################################ Testing best methods\n",
        "\n",
        "model_list = [RandomForestRegressor, GradientBoostingRegressor, SVR] #GaussianNB, \n",
        "model_scores = dict()\n",
        "for model in model_list:\n",
        "  # fit model to data\n",
        "  print(model)\n",
        "  regressor = model().fit(X_train,y_train)\n",
        "  y_pred = regressor.predict(X_test)\n",
        "  model_mse = mean_squared_error(y_pred,y_test)\n",
        "  model_mae = mean_absolute_error(y_pred,y_test)\n",
        "  model_scores[model] = (regressor,y_pred,model_mse,model_mae)\n",
        "  print(model_mse,'\\n',model_mae)\n",
        "\n",
        "############################################# Averaging Method\n",
        "#############################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import xgboost as xgb\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        " \n",
        "# initializing all the model objects with default parameters\n",
        "model_1 = GradientBoostingRegressor()\n",
        "model_2 = SVR()\n",
        "model_3 = RandomForestRegressor()\n",
        " \n",
        "# training all the model on the training dataset\n",
        "model_1.fit(X_train, y_train)\n",
        "model_2.fit(X_train, y_train)\n",
        "model_3.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the validation dataset\n",
        "pred_1 = model_1.predict(X_test)\n",
        "pred_2 = model_2.predict(X_test)\n",
        "pred_3 = model_3.predict(X_test)\n",
        " \n",
        "# final prediction after averaging on the prediction of all 3 models\n",
        "pred_final = (pred_1+pred_2+pred_3)/3.0\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "mod1 = (mean_absolute_error(pred_1, y_test))\n",
        "mod2 = (mean_absolute_error(pred_2, y_test))\n",
        "mod3 = (mean_absolute_error(pred_3, y_test))\n",
        "#print(mean_absolute_error(pred_final, y_test))  ##switch Y_pred and test\n",
        "#Averaging_model = (mod1+mod2+mod3)/3.0\n",
        "Avg_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Avg_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('AVERAGING')\n",
        "print(\"Averaging Model MAE:\", Avg_mod)\n",
        "print(\"Averaging Model MSE:\", Avg_mod1)\n",
        "\n",
        "############################################# Boosting Method\n",
        "#############################################\n",
        "\n",
        "# importing utility modules\n",
        "import pandas as pd\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        " \n",
        "# initializing the boosting module with default parameters\n",
        "model = GradientBoostingRegressor() #163\n",
        " \n",
        "# training the model on the train dataset\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Boost_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Boost_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('BOOSTING')\n",
        "print(\"Averaging Model MAE:\", Boost_mod)\n",
        "print(\"Averaging Model MSE:\", Boost_mod1)\n",
        "print('BAGGING')\n",
        "\n",
        "################################################# Bagging Method\n",
        "#################################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "import xgboost as xgb\n",
        " \n",
        "# importing bagging module\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "\n",
        "# initializing the bagging model using XGboost as base model with default parameters\n",
        "model = BaggingRegressor(base_estimator=xgb.XGBRegressor())\n",
        " \n",
        "# training model\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Bag_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Bag_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print(\"Bagging Model MAE:\", Bag_mod)\n",
        "print(\"Bagging Model MSE:\", Bag_mod1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBl2BthRHDp1",
        "outputId": "999f3c32-6be3-44af-84c2-43dc206ffdb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'sklearn.ensemble._forest.RandomForestRegressor'>\n",
            "0.0031337377827200686 \n",
            " 0.03310867400881053\n",
            "<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>\n",
            "0.002658334289157491 \n",
            " 0.03208107660221757\n",
            "<class 'sklearn.svm._classes.SVR'>\n",
            "0.002637835881101033 \n",
            " 0.03433641831847565\n",
            "AVERAGING\n",
            "Averaging Model MAE: 0.03268635202429471\n",
            "Averaging Model MSE: 0.0026513605739305808\n",
            "BOOSTING\n",
            "Averaging Model MAE: 0.03208143824890133\n",
            "Averaging Model MSE: 0.0026583015760729316\n",
            "BAGGING\n",
            "[00:35:14] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[00:35:24] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[00:35:31] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[00:35:37] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[00:35:43] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[00:35:49] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[00:35:54] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[00:36:00] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[00:36:08] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[00:36:13] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "Bagging Model MAE: 0.03146995527655018\n",
            "Bagging Model MSE: 0.0026160912475101266\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Fire size .25-1\n"
      ],
      "metadata": {
        "id": "fiFhUJivILFR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "########## Spliting by size\n",
        "\n",
        "MidWest2 = MidWest[(MidWest[\"FIRE_SIZE\"] > .25) & (MidWest[\"FIRE_SIZE\"] <= 1)]\n",
        "\n",
        "Fires12 = MidWest2[MidWest2['FIRE_YEAR'] <= 2017].copy()\n",
        "Fires13 = MidWest2[MidWest2['FIRE_YEAR'] > 2017].copy()\n",
        "X_train = Fires12.loc[:, Fires12.columns != 'FIRE_SIZE']\n",
        "y_train = Fires12['FIRE_SIZE'].copy()\n",
        "X_test = Fires13.loc[:, Fires13.columns != 'FIRE_SIZE']\n",
        "y_test = Fires13['FIRE_SIZE'].copy()\n",
        "\n",
        "############################################ Testing best methods\n",
        "\n",
        "model_list = [RandomForestRegressor, GradientBoostingRegressor, SVR] #GaussianNB, \n",
        "model_scores = dict()\n",
        "for model in model_list:\n",
        "  # fit model to data\n",
        "  print(model)\n",
        "  regressor = model().fit(X_train,y_train)\n",
        "  y_pred = regressor.predict(X_test)\n",
        "  model_mse = mean_squared_error(y_pred,y_test)\n",
        "  model_mae = mean_absolute_error(y_pred,y_test)\n",
        "  model_scores[model] = (regressor,y_pred,model_mse,model_mae)\n",
        "  print(model_mse,'\\n',model_mae)\n",
        "\n",
        "############################################# Averaging Method\n",
        "#############################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import xgboost as xgb\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        " \n",
        "# initializing all the model objects with default parameters\n",
        "model_1 = GradientBoostingRegressor()\n",
        "model_2 = SVR()\n",
        "model_3 = RandomForestRegressor()\n",
        " \n",
        "# training all the model on the training dataset\n",
        "model_1.fit(X_train, y_train)\n",
        "model_2.fit(X_train, y_train)\n",
        "model_3.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the validation dataset\n",
        "pred_1 = model_1.predict(X_test)\n",
        "pred_2 = model_2.predict(X_test)\n",
        "pred_3 = model_3.predict(X_test)\n",
        " \n",
        "# final prediction after averaging on the prediction of all 3 models\n",
        "pred_final = (pred_1+pred_2+pred_3)/3.0\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "mod1 = (mean_absolute_error(pred_1, y_test))\n",
        "mod2 = (mean_absolute_error(pred_2, y_test))\n",
        "mod3 = (mean_absolute_error(pred_3, y_test))\n",
        "#print(mean_absolute_error(pred_final, y_test))  ##switch Y_pred and test\n",
        "#Averaging_model = (mod1+mod2+mod3)/3.0\n",
        "Avg_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Avg_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('AVERAGING')\n",
        "print(\"Averaging Model MAE:\", Avg_mod)\n",
        "print(\"Averaging Model MSE:\", Avg_mod1)\n",
        "\n",
        "############################################# Boosting Method\n",
        "#############################################\n",
        "\n",
        "# importing utility modules\n",
        "import pandas as pd\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        " \n",
        "# initializing the boosting module with default parameters\n",
        "model = GradientBoostingRegressor() #163\n",
        " \n",
        "# training the model on the train dataset\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Boost_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Boost_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('BOOSTING')\n",
        "print(\"Averaging Model MAE:\", Boost_mod)\n",
        "print(\"Averaging Model MSE:\", Boost_mod1)\n",
        "print('BAGGING')\n",
        "\n",
        "################################################# Bagging Method\n",
        "#################################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "import xgboost as xgb\n",
        " \n",
        "# importing bagging module\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "\n",
        "# initializing the bagging model using XGboost as base model with default parameters\n",
        "model = BaggingRegressor(base_estimator=xgb.XGBRegressor())\n",
        " \n",
        "# training model\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Bag_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Bag_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print(\"Bagging Model MAE:\", Bag_mod)\n",
        "print(\"Bagging Model MSE:\", Bag_mod1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UuAfq8D1H3Gd",
        "outputId": "e21ca76b-a3bb-4294-868e-54bf21b8bf76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'sklearn.ensemble._forest.RandomForestRegressor'>\n",
            "0.07187831980355339 \n",
            " 0.20719581573415105\n",
            "<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>\n",
            "0.07899822687775587 \n",
            " 0.2264518129820196\n",
            "<class 'sklearn.svm._classes.SVR'>\n",
            "0.11166385098258556 \n",
            " 0.2688910880246547\n",
            "AVERAGING\n",
            "Averaging Model MAE: 0.2297645854135131\n",
            "Averaging Model MSE: 0.07767750073735717\n",
            "BOOSTING\n",
            "Averaging Model MAE: 0.22645181298201947\n",
            "Averaging Model MSE: 0.07899822687775587\n",
            "BAGGING\n",
            "[01:13:00] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[01:13:04] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[01:13:08] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[01:13:11] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[01:13:15] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[01:13:19] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[01:13:22] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[01:13:26] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[01:13:30] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[01:13:33] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "Bagging Model MAE: 0.2273739466481771\n",
            "Bagging Model MSE: 0.07967804633653985\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Fire size 1-5"
      ],
      "metadata": {
        "id": "IO9YHW-PIdB1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "########## Spliting by size\n",
        "\n",
        "MidWest2 = MidWest[(MidWest[\"FIRE_SIZE\"] > 1) & (MidWest[\"FIRE_SIZE\"] <= 5)]\n",
        "\n",
        "Fires12 = MidWest2[MidWest2['FIRE_YEAR'] <= 2017].copy()\n",
        "Fires13 = MidWest2[MidWest2['FIRE_YEAR'] > 2017].copy()\n",
        "X_train = Fires12.loc[:, Fires12.columns != 'FIRE_SIZE']\n",
        "y_train = Fires12['FIRE_SIZE'].copy()\n",
        "X_test = Fires13.loc[:, Fires13.columns != 'FIRE_SIZE']\n",
        "y_test = Fires13['FIRE_SIZE'].copy()\n",
        "\n",
        "############################################ Testing best methods\n",
        "\n",
        "model_list = [RandomForestRegressor, GradientBoostingRegressor, SVR] #GaussianNB, \n",
        "model_scores = dict()\n",
        "for model in model_list:\n",
        "  # fit model to data\n",
        "  print(model)\n",
        "  regressor = model().fit(X_train,y_train)\n",
        "  y_pred = regressor.predict(X_test)\n",
        "  model_mse = mean_squared_error(y_pred,y_test)\n",
        "  model_mae = mean_absolute_error(y_pred,y_test)\n",
        "  model_scores[model] = (regressor,y_pred,model_mse,model_mae)\n",
        "  print(model_mse,'\\n',model_mae)\n",
        "\n",
        "############################################# Averaging Method\n",
        "#############################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import xgboost as xgb\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        " \n",
        "# initializing all the model objects with default parameters\n",
        "model_1 = GradientBoostingRegressor()\n",
        "model_2 = SVR()\n",
        "model_3 = RandomForestRegressor()\n",
        " \n",
        "# training all the model on the training dataset\n",
        "model_1.fit(X_train, y_train)\n",
        "model_2.fit(X_train, y_train)\n",
        "model_3.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the validation dataset\n",
        "pred_1 = model_1.predict(X_test)\n",
        "pred_2 = model_2.predict(X_test)\n",
        "pred_3 = model_3.predict(X_test)\n",
        " \n",
        "# final prediction after averaging on the prediction of all 3 models\n",
        "pred_final = (pred_1+pred_2+pred_3)/3.0\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "mod1 = (mean_absolute_error(pred_1, y_test))\n",
        "mod2 = (mean_absolute_error(pred_2, y_test))\n",
        "mod3 = (mean_absolute_error(pred_3, y_test))\n",
        "#print(mean_absolute_error(pred_final, y_test))  ##switch Y_pred and test\n",
        "#Averaging_model = (mod1+mod2+mod3)/3.0\n",
        "Avg_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Avg_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('AVERAGING')\n",
        "print(\"Averaging Model MAE:\", Avg_mod)\n",
        "print(\"Averaging Model MSE:\", Avg_mod1)\n",
        "\n",
        "############################################# Boosting Method\n",
        "#############################################\n",
        "\n",
        "# importing utility modules\n",
        "import pandas as pd\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        " \n",
        "# initializing the boosting module with default parameters\n",
        "model = GradientBoostingRegressor() #163\n",
        " \n",
        "# training the model on the train dataset\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Boost_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Boost_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('BOOSTING')\n",
        "print(\"Averaging Model MAE:\", Boost_mod)\n",
        "print(\"Averaging Model MSE:\", Boost_mod1)\n",
        "print('BAGGING')\n",
        "\n",
        "################################################# Bagging Method\n",
        "#################################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "import xgboost as xgb\n",
        " \n",
        "# importing bagging module\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "\n",
        "# initializing the bagging model using XGboost as base model with default parameters\n",
        "model = BaggingRegressor(base_estimator=xgb.XGBRegressor())\n",
        " \n",
        "# training model\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Bag_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Bag_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print(\"Bagging Model MAE:\", Bag_mod)\n",
        "print(\"Bagging Model MSE:\", Bag_mod1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCPuKf4gIevv",
        "outputId": "8b087054-616c-4f4a-8b4e-78cd70554ad4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'sklearn.ensemble._forest.RandomForestRegressor'>\n",
            "1.7075656677358437 \n",
            " 1.1286453256789577\n",
            "<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>\n",
            "1.559002523693019 \n",
            " 1.0876790334385256\n",
            "<class 'sklearn.svm._classes.SVR'>\n",
            "1.605874810541849 \n",
            " 1.077780745265469\n",
            "AVERAGING\n",
            "Averaging Model MAE: 1.0835148533146772\n",
            "Averaging Model MSE: 1.5674111367257995\n",
            "BOOSTING\n",
            "Averaging Model MAE: 1.0876790334385251\n",
            "Averaging Model MSE: 1.559002523693018\n",
            "BAGGING\n",
            "[01:29:29] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[01:29:33] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[01:29:37] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[01:29:41] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[01:29:45] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[01:29:49] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[01:29:53] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[01:29:57] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[01:30:01] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[01:30:04] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "Bagging Model MAE: 1.0866305957761762\n",
            "Bagging Model MSE: 1.5552769956372048\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Fire size 5-20"
      ],
      "metadata": {
        "id": "Cvey-MsnIjRm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "########## Spliting by size\n",
        "\n",
        "MidWest2 = MidWest[(MidWest[\"FIRE_SIZE\"] > 5) & (MidWest[\"FIRE_SIZE\"] <= 20)]\n",
        "\n",
        "Fires12 = MidWest2[MidWest2['FIRE_YEAR'] <= 2017].copy()\n",
        "Fires13 = MidWest2[MidWest2['FIRE_YEAR'] > 2017].copy()\n",
        "X_train = Fires12.loc[:, Fires12.columns != 'FIRE_SIZE']\n",
        "y_train = Fires12['FIRE_SIZE'].copy()\n",
        "X_test = Fires13.loc[:, Fires13.columns != 'FIRE_SIZE']\n",
        "y_test = Fires13['FIRE_SIZE'].copy()\n",
        "\n",
        "############################################ Testing best methods\n",
        "\n",
        "model_list = [RandomForestRegressor, GradientBoostingRegressor, SVR] #GaussianNB, \n",
        "model_scores = dict()\n",
        "for model in model_list:\n",
        "  # fit model to data\n",
        "  print(model)\n",
        "  regressor = model().fit(X_train,y_train)\n",
        "  y_pred = regressor.predict(X_test)\n",
        "  model_mse = mean_squared_error(y_pred,y_test)\n",
        "  model_mae = mean_absolute_error(y_pred,y_test)\n",
        "  model_scores[model] = (regressor,y_pred,model_mse,model_mae)\n",
        "  print(model_mse,'\\n',model_mae)\n",
        "\n",
        "############################################# Averaging Method\n",
        "#############################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import xgboost as xgb\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        " \n",
        "# initializing all the model objects with default parameters\n",
        "model_1 = GradientBoostingRegressor()\n",
        "model_2 = SVR()\n",
        "model_3 = RandomForestRegressor()\n",
        " \n",
        "# training all the model on the training dataset\n",
        "model_1.fit(X_train, y_train)\n",
        "model_2.fit(X_train, y_train)\n",
        "model_3.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the validation dataset\n",
        "pred_1 = model_1.predict(X_test)\n",
        "pred_2 = model_2.predict(X_test)\n",
        "pred_3 = model_3.predict(X_test)\n",
        " \n",
        "# final prediction after averaging on the prediction of all 3 models\n",
        "pred_final = (pred_1+pred_2+pred_3)/3.0\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "mod1 = (mean_absolute_error(pred_1, y_test))\n",
        "mod2 = (mean_absolute_error(pred_2, y_test))\n",
        "mod3 = (mean_absolute_error(pred_3, y_test))\n",
        "#print(mean_absolute_error(pred_final, y_test))  ##switch Y_pred and test\n",
        "#Averaging_model = (mod1+mod2+mod3)/3.0\n",
        "Avg_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Avg_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('AVERAGING')\n",
        "print(\"Averaging Model MAE:\", Avg_mod)\n",
        "print(\"Averaging Model MSE:\", Avg_mod1)\n",
        "\n",
        "############################################# Boosting Method\n",
        "#############################################\n",
        "\n",
        "# importing utility modules\n",
        "import pandas as pd\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        " \n",
        "# initializing the boosting module with default parameters\n",
        "model = GradientBoostingRegressor() #163\n",
        " \n",
        "# training the model on the train dataset\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Boost_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Boost_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('BOOSTING')\n",
        "print(\"Averaging Model MAE:\", Boost_mod)\n",
        "print(\"Averaging Model MSE:\", Boost_mod1)\n",
        "print('BAGGING')\n",
        "\n",
        "################################################# Bagging Method\n",
        "#################################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "import xgboost as xgb\n",
        " \n",
        "# importing bagging module\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "\n",
        "# initializing the bagging model using XGboost as base model with default parameters\n",
        "model = BaggingRegressor(base_estimator=xgb.XGBRegressor())\n",
        " \n",
        "# training model\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Bag_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Bag_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print(\"Bagging Model MAE:\", Bag_mod)\n",
        "print(\"Bagging Model MSE:\", Bag_mod1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bK8FGFAIluj",
        "outputId": "8ca84a69-c8d6-4e90-9841-3cac189efa3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'sklearn.ensemble._forest.RandomForestRegressor'>\n",
            "22.73179733421891 \n",
            " 4.068809534550662\n",
            "<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>\n",
            "20.09537365622613 \n",
            " 3.8741804755024276\n",
            "<class 'sklearn.svm._classes.SVR'>\n",
            "21.763142639772816 \n",
            " 3.570868880597381\n",
            "AVERAGING\n",
            "Averaging Model MAE: 3.7834604388811903\n",
            "Averaging Model MSE: 20.01362568032414\n",
            "BOOSTING\n",
            "Averaging Model MAE: 3.8741804755024276\n",
            "Averaging Model MSE: 20.095373656226126\n",
            "BAGGING\n",
            "[01:37:39] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[01:37:41] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[01:37:44] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[01:37:46] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[01:37:49] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[01:37:52] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[01:37:54] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[01:37:57] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[01:37:59] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[01:38:02] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "Bagging Model MAE: 3.873154315068907\n",
            "Bagging Model MSE: 20.052072537733224\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Fire size 20-50"
      ],
      "metadata": {
        "id": "9zeviIOYIoni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "########## Spliting by size\n",
        "\n",
        "MidWest2 = MidWest[(MidWest[\"FIRE_SIZE\"] > 20) & (MidWest[\"FIRE_SIZE\"] <= 50)]\n",
        "\n",
        "Fires12 = MidWest2[MidWest2['FIRE_YEAR'] <= 2017].copy()\n",
        "Fires13 = MidWest2[MidWest2['FIRE_YEAR'] > 2017].copy()\n",
        "X_train = Fires12.loc[:, Fires12.columns != 'FIRE_SIZE']\n",
        "y_train = Fires12['FIRE_SIZE'].copy()\n",
        "X_test = Fires13.loc[:, Fires13.columns != 'FIRE_SIZE']\n",
        "y_test = Fires13['FIRE_SIZE'].copy()\n",
        "\n",
        "############################################ Testing best methods\n",
        "\n",
        "model_list = [RandomForestRegressor, GradientBoostingRegressor, SVR] #GaussianNB, \n",
        "model_scores = dict()\n",
        "for model in model_list:\n",
        "  # fit model to data\n",
        "  print(model)\n",
        "  regressor = model().fit(X_train,y_train)\n",
        "  y_pred = regressor.predict(X_test)\n",
        "  model_mse = mean_squared_error(y_pred,y_test)\n",
        "  model_mae = mean_absolute_error(y_pred,y_test)\n",
        "  model_scores[model] = (regressor,y_pred,model_mse,model_mae)\n",
        "  print(model_mse,'\\n',model_mae)\n",
        "\n",
        "############################################# Averaging Method\n",
        "#############################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import xgboost as xgb\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        " \n",
        "# initializing all the model objects with default parameters\n",
        "model_1 = GradientBoostingRegressor()\n",
        "model_2 = SVR()\n",
        "model_3 = RandomForestRegressor()\n",
        " \n",
        "# training all the model on the training dataset\n",
        "model_1.fit(X_train, y_train)\n",
        "model_2.fit(X_train, y_train)\n",
        "model_3.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the validation dataset\n",
        "pred_1 = model_1.predict(X_test)\n",
        "pred_2 = model_2.predict(X_test)\n",
        "pred_3 = model_3.predict(X_test)\n",
        " \n",
        "# final prediction after averaging on the prediction of all 3 models\n",
        "pred_final = (pred_1+pred_2+pred_3)/3.0\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "mod1 = (mean_absolute_error(pred_1, y_test))\n",
        "mod2 = (mean_absolute_error(pred_2, y_test))\n",
        "mod3 = (mean_absolute_error(pred_3, y_test))\n",
        "#print(mean_absolute_error(pred_final, y_test))  ##switch Y_pred and test\n",
        "#Averaging_model = (mod1+mod2+mod3)/3.0\n",
        "Avg_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Avg_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('AVERAGING')\n",
        "print(\"Averaging Model MAE:\", Avg_mod)\n",
        "print(\"Averaging Model MSE:\", Avg_mod1)\n",
        "\n",
        "############################################# Boosting Method\n",
        "#############################################\n",
        "\n",
        "# importing utility modules\n",
        "import pandas as pd\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        " \n",
        "# initializing the boosting module with default parameters\n",
        "model = GradientBoostingRegressor() #163\n",
        " \n",
        "# training the model on the train dataset\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Boost_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Boost_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('BOOSTING')\n",
        "print(\"Averaging Model MAE:\", Boost_mod)\n",
        "print(\"Averaging Model MSE:\", Boost_mod1)\n",
        "print('BAGGING')\n",
        "\n",
        "################################################# Bagging Method\n",
        "#################################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "import xgboost as xgb\n",
        " \n",
        "# importing bagging module\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "\n",
        "# initializing the bagging model using XGboost as base model with default parameters\n",
        "model = BaggingRegressor(base_estimator=xgb.XGBRegressor())\n",
        " \n",
        "# training model\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Bag_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Bag_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print(\"Bagging Model MAE:\", Bag_mod)\n",
        "print(\"Bagging Model MSE:\", Bag_mod1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmrPE5klIqkB",
        "outputId": "fe8580bb-8055-4aff-8da0-816d199238e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'sklearn.ensemble._forest.RandomForestRegressor'>\n",
            "85.65070145349364 \n",
            " 7.935761027805085\n",
            "<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>\n",
            "77.36573964351047 \n",
            " 7.681866246956165\n",
            "<class 'sklearn.svm._classes.SVR'>\n",
            "85.83131155711342 \n",
            " 7.522111375647284\n",
            "AVERAGING\n",
            "Averaging Model MAE: 7.657026654430301\n",
            "Averaging Model MSE: 78.91320704119192\n",
            "BOOSTING\n",
            "Averaging Model MAE: 7.681866246956167\n",
            "Averaging Model MSE: 77.36573964351047\n",
            "BAGGING\n",
            "[01:40:09] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[01:40:10] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[01:40:11] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[01:40:13] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[01:40:14] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[01:40:15] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[01:40:16] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[01:40:17] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[01:40:19] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[01:40:20] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "Bagging Model MAE: 7.695798690710535\n",
            "Bagging Model MSE: 77.2947629039508\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Fire Size 50-200"
      ],
      "metadata": {
        "id": "cMom8ut_ItAE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "########## Spliting by size\n",
        "\n",
        "MidWest2 = MidWest[(MidWest[\"FIRE_SIZE\"] > 50) & (MidWest[\"FIRE_SIZE\"] <= 200)]\n",
        "\n",
        "Fires12 = MidWest2[MidWest2['FIRE_YEAR'] <= 2017].copy()\n",
        "Fires13 = MidWest2[MidWest2['FIRE_YEAR'] > 2017].copy()\n",
        "X_train = Fires12.loc[:, Fires12.columns != 'FIRE_SIZE']\n",
        "y_train = Fires12['FIRE_SIZE'].copy()\n",
        "X_test = Fires13.loc[:, Fires13.columns != 'FIRE_SIZE']\n",
        "y_test = Fires13['FIRE_SIZE'].copy()\n",
        "\n",
        "############################################ Testing best methods\n",
        "\n",
        "model_list = [RandomForestRegressor, GradientBoostingRegressor, SVR] #GaussianNB, \n",
        "model_scores = dict()\n",
        "for model in model_list:\n",
        "  # fit model to data\n",
        "  print(model)\n",
        "  regressor = model().fit(X_train,y_train)\n",
        "  y_pred = regressor.predict(X_test)\n",
        "  model_mse = mean_squared_error(y_pred,y_test)\n",
        "  model_mae = mean_absolute_error(y_pred,y_test)\n",
        "  model_scores[model] = (regressor,y_pred,model_mse,model_mae)\n",
        "  print(model_mse,'\\n',model_mae)\n",
        "\n",
        "############################################# Averaging Method\n",
        "#############################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import xgboost as xgb\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        " \n",
        "# initializing all the model objects with default parameters\n",
        "model_1 = GradientBoostingRegressor()\n",
        "model_2 = SVR()\n",
        "model_3 = RandomForestRegressor()\n",
        " \n",
        "# training all the model on the training dataset\n",
        "model_1.fit(X_train, y_train)\n",
        "model_2.fit(X_train, y_train)\n",
        "model_3.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the validation dataset\n",
        "pred_1 = model_1.predict(X_test)\n",
        "pred_2 = model_2.predict(X_test)\n",
        "pred_3 = model_3.predict(X_test)\n",
        " \n",
        "# final prediction after averaging on the prediction of all 3 models\n",
        "pred_final = (pred_1+pred_2+pred_3)/3.0\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "mod1 = (mean_absolute_error(pred_1, y_test))\n",
        "mod2 = (mean_absolute_error(pred_2, y_test))\n",
        "mod3 = (mean_absolute_error(pred_3, y_test))\n",
        "#print(mean_absolute_error(pred_final, y_test))  ##switch Y_pred and test\n",
        "#Averaging_model = (mod1+mod2+mod3)/3.0\n",
        "Avg_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Avg_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('AVERAGING')\n",
        "print(\"Averaging Model MAE:\", Avg_mod)\n",
        "print(\"Averaging Model MSE:\", Avg_mod1)\n",
        "\n",
        "############################################# Boosting Method\n",
        "#############################################\n",
        "\n",
        "# importing utility modules\n",
        "import pandas as pd\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        " \n",
        "# initializing the boosting module with default parameters\n",
        "model = GradientBoostingRegressor() #163\n",
        " \n",
        "# training the model on the train dataset\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Boost_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Boost_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('BOOSTING')\n",
        "print(\"Averaging Model MAE:\", Boost_mod)\n",
        "print(\"Averaging Model MSE:\", Boost_mod1)\n",
        "print('BAGGING')\n",
        "\n",
        "################################################# Bagging Method\n",
        "#################################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "import xgboost as xgb\n",
        " \n",
        "# importing bagging module\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "\n",
        "# initializing the bagging model using XGboost as base model with default parameters\n",
        "model = BaggingRegressor(base_estimator=xgb.XGBRegressor())\n",
        " \n",
        "# training model\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Bag_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Bag_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print(\"Bagging Model MAE:\", Bag_mod)\n",
        "print(\"Bagging Model MSE:\", Bag_mod1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hokR1K-8IvEc",
        "outputId": "dc875162-b77d-434e-8da3-1a3204ca99cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'sklearn.ensemble._forest.RandomForestRegressor'>\n",
            "1696.2813807659666 \n",
            " 33.486863\n",
            "<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>\n",
            "1603.4018843871106 \n",
            " 32.86021905474422\n",
            "<class 'sklearn.svm._classes.SVR'>\n",
            "1852.3138053693033 \n",
            " 30.966364112991705\n",
            "AVERAGING\n",
            "Averaging Model MAE: 31.4893538494593\n",
            "Averaging Model MSE: 1589.0737356322215\n",
            "BOOSTING\n",
            "Averaging Model MAE: 32.86021905474422\n",
            "Averaging Model MSE: 1603.4018843871104\n",
            "BAGGING\n",
            "[01:41:35] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[01:41:36] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[01:41:37] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[01:41:38] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[01:41:39] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[01:41:40] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[01:41:41] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[01:41:41] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[01:41:42] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[01:41:43] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "Bagging Model MAE: 32.865583709513345\n",
            "Bagging Model MSE: 1612.42680143177\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Fire size 200-1000"
      ],
      "metadata": {
        "id": "ulci0P_KI23K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "########## Spliting by size\n",
        "\n",
        "MidWest2 = MidWest[(MidWest[\"FIRE_SIZE\"] > 200) & (MidWest[\"FIRE_SIZE\"] <= 1000)]\n",
        "\n",
        "Fires12 = MidWest2[MidWest2['FIRE_YEAR'] <= 2017].copy()\n",
        "Fires13 = MidWest2[MidWest2['FIRE_YEAR'] > 2017].copy()\n",
        "X_train = Fires12.loc[:, Fires12.columns != 'FIRE_SIZE']\n",
        "y_train = Fires12['FIRE_SIZE'].copy()\n",
        "X_test = Fires13.loc[:, Fires13.columns != 'FIRE_SIZE']\n",
        "y_test = Fires13['FIRE_SIZE'].copy()\n",
        "\n",
        "############################################ Testing best methods\n",
        "\n",
        "model_list = [RandomForestRegressor, GradientBoostingRegressor, SVR] #GaussianNB, \n",
        "model_scores = dict()\n",
        "for model in model_list:\n",
        "  # fit model to data\n",
        "  print(model)\n",
        "  regressor = model().fit(X_train,y_train)\n",
        "  y_pred = regressor.predict(X_test)\n",
        "  model_mse = mean_squared_error(y_pred,y_test)\n",
        "  model_mae = mean_absolute_error(y_pred,y_test)\n",
        "  model_scores[model] = (regressor,y_pred,model_mse,model_mae)\n",
        "  print(model_mse,'\\n',model_mae)\n",
        "\n",
        "############################################# Averaging Method\n",
        "#############################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import xgboost as xgb\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        " \n",
        "# initializing all the model objects with default parameters\n",
        "model_1 = GradientBoostingRegressor()\n",
        "model_2 = SVR()\n",
        "model_3 = RandomForestRegressor()\n",
        " \n",
        "# training all the model on the training dataset\n",
        "model_1.fit(X_train, y_train)\n",
        "model_2.fit(X_train, y_train)\n",
        "model_3.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the validation dataset\n",
        "pred_1 = model_1.predict(X_test)\n",
        "pred_2 = model_2.predict(X_test)\n",
        "pred_3 = model_3.predict(X_test)\n",
        " \n",
        "# final prediction after averaging on the prediction of all 3 models\n",
        "pred_final = (pred_1+pred_2+pred_3)/3.0\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "mod1 = (mean_absolute_error(pred_1, y_test))\n",
        "mod2 = (mean_absolute_error(pred_2, y_test))\n",
        "mod3 = (mean_absolute_error(pred_3, y_test))\n",
        "#print(mean_absolute_error(pred_final, y_test))  ##switch Y_pred and test\n",
        "#Averaging_model = (mod1+mod2+mod3)/3.0\n",
        "Avg_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Avg_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('AVERAGING')\n",
        "print(\"Averaging Model MAE:\", Avg_mod)\n",
        "print(\"Averaging Model MSE:\", Avg_mod1)\n",
        "\n",
        "############################################# Boosting Method\n",
        "#############################################\n",
        "\n",
        "# importing utility modules\n",
        "import pandas as pd\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        " \n",
        "# initializing the boosting module with default parameters\n",
        "model = GradientBoostingRegressor() #163\n",
        " \n",
        "# training the model on the train dataset\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Boost_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Boost_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('BOOSTING')\n",
        "print(\"Averaging Model MAE:\", Boost_mod)\n",
        "print(\"Averaging Model MSE:\", Boost_mod1)\n",
        "print('BAGGING')\n",
        "\n",
        "################################################# Bagging Method\n",
        "#################################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "import xgboost as xgb\n",
        " \n",
        "# importing bagging module\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "\n",
        "# initializing the bagging model using XGboost as base model with default parameters\n",
        "model = BaggingRegressor(base_estimator=xgb.XGBRegressor())\n",
        " \n",
        "# training model\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Bag_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Bag_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print(\"Bagging Model MAE:\", Bag_mod)\n",
        "print(\"Bagging Model MSE:\", Bag_mod1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2o_VjzE_I5G3",
        "outputId": "cb740aaa-8b23-4ce6-96da-1e6a6c4c4366"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'sklearn.ensemble._forest.RandomForestRegressor'>\n",
            "43034.217522137544 \n",
            " 171.452727101084\n",
            "<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>\n",
            "41901.46222007928 \n",
            " 167.69220015662805\n",
            "<class 'sklearn.svm._classes.SVR'>\n",
            "52901.14077423242 \n",
            " 171.57161778111245\n",
            "AVERAGING\n",
            "Averaging Model MAE: 163.74262021932174\n",
            "Averaging Model MSE: 42148.39682638946\n",
            "BOOSTING\n",
            "Averaging Model MAE: 167.69220015662805\n",
            "Averaging Model MSE: 41901.46222007929\n",
            "BAGGING\n",
            "[05:34:56] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[05:34:57] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[05:34:58] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[05:34:58] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[05:34:59] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[05:34:59] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[05:35:00] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[05:35:00] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[05:35:01] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[05:35:01] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "Bagging Model MAE: 168.31380408707284\n",
            "Bagging Model MSE: 41664.75103490713\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Fire size 1000+"
      ],
      "metadata": {
        "id": "IidcBAA-I9z8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "########## Spliting by size\n",
        "\n",
        "MidWest2 = MidWest[(MidWest[\"FIRE_SIZE\"] > 1000)]\n",
        "\n",
        "Fires12 = MidWest2[MidWest2['FIRE_YEAR'] <= 2017].copy()\n",
        "Fires13 = MidWest2[MidWest2['FIRE_YEAR'] > 2017].copy()\n",
        "X_train = Fires12.loc[:, Fires12.columns != 'FIRE_SIZE']\n",
        "y_train = Fires12['FIRE_SIZE'].copy()\n",
        "X_test = Fires13.loc[:, Fires13.columns != 'FIRE_SIZE']\n",
        "y_test = Fires13['FIRE_SIZE'].copy()\n",
        "\n",
        "############################################ Testing best methods\n",
        "\n",
        "model_list = [RandomForestRegressor, GradientBoostingRegressor, SVR] #GaussianNB, \n",
        "model_scores = dict()\n",
        "for model in model_list:\n",
        "  # fit model to data\n",
        "  print(model)\n",
        "  regressor = model().fit(X_train,y_train)\n",
        "  y_pred = regressor.predict(X_test)\n",
        "  model_mse = mean_squared_error(y_pred,y_test)\n",
        "  model_mae = mean_absolute_error(y_pred,y_test)\n",
        "  model_scores[model] = (regressor,y_pred,model_mse,model_mae)\n",
        "  print(model_mse,'\\n',model_mae)\n",
        "\n",
        "############################################# Averaging Method\n",
        "#############################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import xgboost as xgb\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        " \n",
        "# initializing all the model objects with default parameters\n",
        "model_1 = GradientBoostingRegressor()\n",
        "model_2 = SVR()\n",
        "model_3 = RandomForestRegressor()\n",
        " \n",
        "# training all the model on the training dataset\n",
        "model_1.fit(X_train, y_train)\n",
        "model_2.fit(X_train, y_train)\n",
        "model_3.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the validation dataset\n",
        "pred_1 = model_1.predict(X_test)\n",
        "pred_2 = model_2.predict(X_test)\n",
        "pred_3 = model_3.predict(X_test)\n",
        " \n",
        "# final prediction after averaging on the prediction of all 3 models\n",
        "pred_final = (pred_1+pred_2+pred_3)/3.0\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "mod1 = (mean_absolute_error(pred_1, y_test))\n",
        "mod2 = (mean_absolute_error(pred_2, y_test))\n",
        "mod3 = (mean_absolute_error(pred_3, y_test))\n",
        "#print(mean_absolute_error(pred_final, y_test))  ##switch Y_pred and test\n",
        "#Averaging_model = (mod1+mod2+mod3)/3.0\n",
        "Avg_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Avg_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('AVERAGING')\n",
        "print(\"Averaging Model MAE:\", Avg_mod)\n",
        "print(\"Averaging Model MSE:\", Avg_mod1)\n",
        "\n",
        "############################################# Boosting Method\n",
        "#############################################\n",
        "\n",
        "# importing utility modules\n",
        "import pandas as pd\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        " \n",
        "# initializing the boosting module with default parameters\n",
        "model = GradientBoostingRegressor() #163\n",
        " \n",
        "# training the model on the train dataset\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Boost_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Boost_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('BOOSTING')\n",
        "print(\"Averaging Model MAE:\", Boost_mod)\n",
        "print(\"Averaging Model MSE:\", Boost_mod1)\n",
        "print('BAGGING')\n",
        "\n",
        "################################################# Bagging Method\n",
        "#################################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "import xgboost as xgb\n",
        " \n",
        "# importing bagging module\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "\n",
        "# initializing the bagging model using XGboost as base model with default parameters\n",
        "model = BaggingRegressor(base_estimator=xgb.XGBRegressor())\n",
        " \n",
        "# training model\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Bag_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Bag_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print(\"Bagging Model MAE:\", Bag_mod)\n",
        "print(\"Bagging Model MSE:\", Bag_mod1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWb1PPS2I_hh",
        "outputId": "908a6fa3-cc85-485f-ec8e-be9b6ce3826b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'sklearn.ensemble._forest.RandomForestRegressor'>\n",
            "1682373103.4269984 \n",
            " 15396.000225409836\n",
            "<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>\n",
            "968616261.855425 \n",
            " 12415.592716086307\n",
            "<class 'sklearn.svm._classes.SVR'>\n",
            "865678296.2861291 \n",
            " 7452.834761332886\n",
            "AVERAGING\n",
            "Averaging Model MAE: 11001.462421684562\n",
            "Averaging Model MSE: 955129550.2028732\n",
            "BOOSTING\n",
            "Averaging Model MAE: 13092.154418551558\n",
            "Averaging Model MSE: 1095273972.0317585\n",
            "BAGGING\n",
            "[01:42:05] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[01:42:05] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[01:42:05] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[01:42:05] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[01:42:05] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[01:42:05] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[01:42:06] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[01:42:06] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[01:42:06] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[01:42:06] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "Bagging Model MAE: 12907.813097752307\n",
            "Bagging Model MSE: 892088781.7368503\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### South East 7 states"
      ],
      "metadata": {
        "id": "cAqZOYaHaozF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "state_enc.classes_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bxwd1fSZatm6",
        "outputId": "99b7fc42-8fd2-4c87-e423-516d03642724"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['AK', 'AL', 'AR', 'AZ', 'CA', 'CO', 'CT', 'DC', 'DE', 'FL', 'GA',\n",
              "       'HI', 'IA', 'ID', 'IL', 'IN', 'KS', 'KY', 'LA', 'MA', 'MD', 'ME',\n",
              "       'MI', 'MN', 'MO', 'MS', 'MT', 'NC', 'ND', 'NE', 'NH', 'NJ', 'NM',\n",
              "       'NV', 'NY', 'OH', 'OK', 'OR', 'PA', 'PR', 'RI', 'SC', 'SD', 'TN',\n",
              "       'TX', 'UT', 'VA', 'VT', 'WA', 'WI', 'WV', 'WY'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SouthEast = Fires[(Fires['STATE_ORD'] == 2) | (Fires['STATE_ORD'] == 10) | (Fires['STATE_ORD'] == 11) | (Fires['STATE_ORD'] == 26) | (Fires['STATE_ORD'] == 28) | (Fires['STATE_ORD'] == 42) | (Fires['STATE_ORD'] == 44)]"
      ],
      "metadata": {
        "id": "ucA_5dzlaoYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SouthEast"
      ],
      "metadata": {
        "id": "aQduXW_XbGfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Fire size 0-.25"
      ],
      "metadata": {
        "id": "jLzXaecccxqn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MidWest1 = SouthEast[SouthEast['FIRE_SIZE'] <= .25]\n",
        "################ Train test split\n",
        "\n",
        "Fires12 = MidWest1[MidWest1['FIRE_YEAR'] <= 2017].copy()\n",
        "Fires13 = MidWest1[MidWest1['FIRE_YEAR'] > 2017].copy()\n",
        "X_train = Fires12.loc[:, Fires12.columns != 'FIRE_SIZE']\n",
        "y_train = Fires12['FIRE_SIZE'].copy()\n",
        "X_test = Fires13.loc[:, Fires13.columns != 'FIRE_SIZE']\n",
        "y_test = Fires13['FIRE_SIZE'].copy()\n",
        "\n",
        "############################################ Testing best methods\n",
        "\n",
        "model_list = [RandomForestRegressor, GradientBoostingRegressor, SVR] #GaussianNB, \n",
        "model_scores = dict()\n",
        "for model in model_list:\n",
        "  # fit model to data\n",
        "  print(model)\n",
        "  regressor = model().fit(X_train,y_train)\n",
        "  y_pred = regressor.predict(X_test)\n",
        "  model_mse = mean_squared_error(y_pred,y_test)\n",
        "  model_mae = mean_absolute_error(y_pred,y_test)\n",
        "  model_scores[model] = (regressor,y_pred,model_mse,model_mae)\n",
        "  print(model_mse,'\\n',model_mae)\n",
        "\n",
        "############################################# Averaging Method\n",
        "#############################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import xgboost as xgb\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        " \n",
        "# initializing all the model objects with default parameters\n",
        "model_1 = GradientBoostingRegressor()\n",
        "model_2 = SVR()\n",
        "model_3 = RandomForestRegressor()\n",
        " \n",
        "# training all the model on the training dataset\n",
        "model_1.fit(X_train, y_train)\n",
        "model_2.fit(X_train, y_train)\n",
        "model_3.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the validation dataset\n",
        "pred_1 = model_1.predict(X_test)\n",
        "pred_2 = model_2.predict(X_test)\n",
        "pred_3 = model_3.predict(X_test)\n",
        " \n",
        "# final prediction after averaging on the prediction of all 3 models\n",
        "pred_final = (pred_1+pred_2+pred_3)/3.0\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "mod1 = (mean_absolute_error(pred_1, y_test))\n",
        "mod2 = (mean_absolute_error(pred_2, y_test))\n",
        "mod3 = (mean_absolute_error(pred_3, y_test))\n",
        "#print(mean_absolute_error(pred_final, y_test))  ##switch Y_pred and test\n",
        "#Averaging_model = (mod1+mod2+mod3)/3.0\n",
        "Avg_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Avg_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('AVERAGING')\n",
        "print(\"Averaging Model MAE:\", Avg_mod)\n",
        "print(\"Averaging Model MSE:\", Avg_mod1)\n",
        "\n",
        "############################################# Boosting Method\n",
        "#############################################\n",
        "\n",
        "# importing utility modules\n",
        "import pandas as pd\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        " \n",
        "# initializing the boosting module with default parameters\n",
        "model = GradientBoostingRegressor() #163\n",
        " \n",
        "# training the model on the train dataset\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Boost_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Boost_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('BOOSTING')\n",
        "print(\"Averaging Model MAE:\", Boost_mod)\n",
        "print(\"Averaging Model MSE:\", Boost_mod1)\n",
        "print('BAGGING')\n",
        "\n",
        "################################################# Bagging Method\n",
        "#################################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "import xgboost as xgb\n",
        " \n",
        "# importing bagging module\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "\n",
        "# initializing the bagging model using XGboost as base model with default parameters\n",
        "model = BaggingRegressor(base_estimator=xgb.XGBRegressor())\n",
        " \n",
        "# training model\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Bag_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Bag_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print(\"Bagging Model MAE:\", Bag_mod)\n",
        "print(\"Bagging Model MSE:\", Bag_mod1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yD-nZKEGcxbO",
        "outputId": "ddf59b23-0253-479a-8f71-7f70c57431a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'sklearn.ensemble._forest.RandomForestRegressor'>\n",
            "0.005652643797037713 \n",
            " 0.057008567797483645\n",
            "<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>\n",
            "0.005433276155506149 \n",
            " 0.0573956550580077\n",
            "<class 'sklearn.svm._classes.SVR'>\n",
            "0.0059596796554213126 \n",
            " 0.06196945834759231\n",
            "AVERAGING\n",
            "Averaging Model MAE: 0.05793764739981341\n",
            "Averaging Model MSE: 0.005372705267110178\n",
            "BOOSTING\n",
            "Averaging Model MAE: 0.05739565505800771\n",
            "Averaging Model MSE: 0.00543327615550615\n",
            "BAGGING\n",
            "[03:05:35] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:05:41] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:05:47] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:05:52] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:05:58] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:06:04] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:06:10] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:06:16] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:06:22] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:06:28] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "Bagging Model MAE: 0.057532447656049895\n",
            "Bagging Model MSE: 0.005460005865105681\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Fire size .25-1"
      ],
      "metadata": {
        "id": "C-CZPI-cc_oD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "########## Spliting by size\n",
        "\n",
        "MidWest2 = SouthEast[(SouthEast[\"FIRE_SIZE\"] > .25) & (SouthEast[\"FIRE_SIZE\"] <= 1)]\n",
        "\n",
        "Fires12 = MidWest2[MidWest2['FIRE_YEAR'] <= 2017].copy()\n",
        "Fires13 = MidWest2[MidWest2['FIRE_YEAR'] > 2017].copy()\n",
        "X_train = Fires12.loc[:, Fires12.columns != 'FIRE_SIZE']\n",
        "y_train = Fires12['FIRE_SIZE'].copy()\n",
        "X_test = Fires13.loc[:, Fires13.columns != 'FIRE_SIZE']\n",
        "y_test = Fires13['FIRE_SIZE'].copy()\n",
        "\n",
        "############################################ Testing best methods\n",
        "\n",
        "model_list = [RandomForestRegressor, GradientBoostingRegressor, SVR] #GaussianNB, \n",
        "model_scores = dict()\n",
        "for model in model_list:\n",
        "  # fit model to data\n",
        "  print(model)\n",
        "  regressor = model().fit(X_train,y_train)\n",
        "  y_pred = regressor.predict(X_test)\n",
        "  model_mse = mean_squared_error(y_pred,y_test)\n",
        "  model_mae = mean_absolute_error(y_pred,y_test)\n",
        "  model_scores[model] = (regressor,y_pred,model_mse,model_mae)\n",
        "  print(model_mse,'\\n',model_mae)\n",
        "\n",
        "############################################# Averaging Method\n",
        "#############################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import xgboost as xgb\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        " \n",
        "# initializing all the model objects with default parameters\n",
        "model_1 = GradientBoostingRegressor()\n",
        "model_2 = SVR()\n",
        "model_3 = RandomForestRegressor()\n",
        " \n",
        "# training all the model on the training dataset\n",
        "model_1.fit(X_train, y_train)\n",
        "model_2.fit(X_train, y_train)\n",
        "model_3.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the validation dataset\n",
        "pred_1 = model_1.predict(X_test)\n",
        "pred_2 = model_2.predict(X_test)\n",
        "pred_3 = model_3.predict(X_test)\n",
        " \n",
        "# final prediction after averaging on the prediction of all 3 models\n",
        "pred_final = (pred_1+pred_2+pred_3)/3.0\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "mod1 = (mean_absolute_error(pred_1, y_test))\n",
        "mod2 = (mean_absolute_error(pred_2, y_test))\n",
        "mod3 = (mean_absolute_error(pred_3, y_test))\n",
        "#print(mean_absolute_error(pred_final, y_test))  ##switch Y_pred and test\n",
        "#Averaging_model = (mod1+mod2+mod3)/3.0\n",
        "Avg_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Avg_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('AVERAGING')\n",
        "print(\"Averaging Model MAE:\", Avg_mod)\n",
        "print(\"Averaging Model MSE:\", Avg_mod1)\n",
        "\n",
        "############################################# Boosting Method\n",
        "#############################################\n",
        "\n",
        "# importing utility modules\n",
        "import pandas as pd\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        " \n",
        "# initializing the boosting module with default parameters\n",
        "model = GradientBoostingRegressor() #163\n",
        " \n",
        "# training the model on the train dataset\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Boost_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Boost_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('BOOSTING')\n",
        "print(\"Averaging Model MAE:\", Boost_mod)\n",
        "print(\"Averaging Model MSE:\", Boost_mod1)\n",
        "print('BAGGING')\n",
        "\n",
        "################################################# Bagging Method\n",
        "#################################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "import xgboost as xgb\n",
        " \n",
        "# importing bagging module\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "\n",
        "# initializing the bagging model using XGboost as base model with default parameters\n",
        "model = BaggingRegressor(base_estimator=xgb.XGBRegressor())\n",
        " \n",
        "# training model\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Bag_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Bag_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print(\"Bagging Model MAE:\", Bag_mod)\n",
        "print(\"Bagging Model MSE:\", Bag_mod1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vEsZf0d5dCI1",
        "outputId": "09be5d12-03f9-4255-f791-da8d7b56322f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'sklearn.ensemble._forest.RandomForestRegressor'>\n",
            "0.05036644168317811 \n",
            " 0.172293017920782\n",
            "<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>\n",
            "0.04857041308963725 \n",
            " 0.18478913652767093\n",
            "<class 'sklearn.svm._classes.SVR'>\n",
            "0.05424426386131776 \n",
            " 0.19032794172353354\n",
            "AVERAGING\n",
            "Averaging Model MAE: 0.1814714716683377\n",
            "Averaging Model MSE: 0.046787981311835036\n",
            "BOOSTING\n",
            "Averaging Model MAE: 0.1847891365276709\n",
            "Averaging Model MSE: 0.04857041308963725\n",
            "BAGGING\n",
            "[03:55:40] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:55:47] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:55:53] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:56:00] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:56:07] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:56:14] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:56:20] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:56:27] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:56:34] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[03:56:41] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "Bagging Model MAE: 0.184290973049752\n",
            "Bagging Model MSE: 0.048399231464754056\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fire Size 1-5"
      ],
      "metadata": {
        "id": "CEmHp7XudJJq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "########## Spliting by size\n",
        "\n",
        "MidWest2 = SouthEast[(SouthEast[\"FIRE_SIZE\"] > 1) & (SouthEast[\"FIRE_SIZE\"] <= 5)]\n",
        "\n",
        "Fires12 = MidWest2[MidWest2['FIRE_YEAR'] <= 2017].copy()\n",
        "Fires13 = MidWest2[MidWest2['FIRE_YEAR'] > 2017].copy()\n",
        "X_train = Fires12.loc[:, Fires12.columns != 'FIRE_SIZE']\n",
        "y_train = Fires12['FIRE_SIZE'].copy()\n",
        "X_test = Fires13.loc[:, Fires13.columns != 'FIRE_SIZE']\n",
        "y_test = Fires13['FIRE_SIZE'].copy()\n",
        "\n",
        "############################################ Testing best methods\n",
        "\n",
        "model_list = [RandomForestRegressor, GradientBoostingRegressor, SVR] #GaussianNB, \n",
        "model_scores = dict()\n",
        "for model in model_list:\n",
        "  # fit model to data\n",
        "  print(model)\n",
        "  regressor = model().fit(X_train,y_train)\n",
        "  y_pred = regressor.predict(X_test)\n",
        "  model_mse = mean_squared_error(y_pred,y_test)\n",
        "  model_mae = mean_absolute_error(y_pred,y_test)\n",
        "  model_scores[model] = (regressor,y_pred,model_mse,model_mae)\n",
        "  print(model_mse,'\\n',model_mae)\n",
        "\n",
        "############################################# Averaging Method\n",
        "#############################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import xgboost as xgb\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        " \n",
        "# initializing all the model objects with default parameters\n",
        "model_1 = GradientBoostingRegressor()\n",
        "model_2 = SVR()\n",
        "model_3 = RandomForestRegressor()\n",
        " \n",
        "# training all the model on the training dataset\n",
        "model_1.fit(X_train, y_train)\n",
        "model_2.fit(X_train, y_train)\n",
        "model_3.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the validation dataset\n",
        "pred_1 = model_1.predict(X_test)\n",
        "pred_2 = model_2.predict(X_test)\n",
        "pred_3 = model_3.predict(X_test)\n",
        " \n",
        "# final prediction after averaging on the prediction of all 3 models\n",
        "pred_final = (pred_1+pred_2+pred_3)/3.0\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "mod1 = (mean_absolute_error(pred_1, y_test))\n",
        "mod2 = (mean_absolute_error(pred_2, y_test))\n",
        "mod3 = (mean_absolute_error(pred_3, y_test))\n",
        "#print(mean_absolute_error(pred_final, y_test))  ##switch Y_pred and test\n",
        "#Averaging_model = (mod1+mod2+mod3)/3.0\n",
        "Avg_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Avg_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('AVERAGING')\n",
        "print(\"Averaging Model MAE:\", Avg_mod)\n",
        "print(\"Averaging Model MSE:\", Avg_mod1)\n",
        "\n",
        "############################################# Boosting Method\n",
        "#############################################\n",
        "\n",
        "# importing utility modules\n",
        "import pandas as pd\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        " \n",
        "# initializing the boosting module with default parameters\n",
        "model = GradientBoostingRegressor() #163\n",
        " \n",
        "# training the model on the train dataset\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Boost_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Boost_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('BOOSTING')\n",
        "print(\"Averaging Model MAE:\", Boost_mod)\n",
        "print(\"Averaging Model MSE:\", Boost_mod1)\n",
        "print('BAGGING')\n",
        "\n",
        "################################################# Bagging Method\n",
        "#################################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "import xgboost as xgb\n",
        " \n",
        "# importing bagging module\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "\n",
        "# initializing the bagging model using XGboost as base model with default parameters\n",
        "model = BaggingRegressor(base_estimator=xgb.XGBRegressor())\n",
        " \n",
        "# training model\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Bag_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Bag_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print(\"Bagging Model MAE:\", Bag_mod)\n",
        "print(\"Bagging Model MSE:\", Bag_mod1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGfdHC0YdKSF",
        "outputId": "efe119dd-45b0-4ca5-db83-5852a68c641a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'sklearn.ensemble._forest.RandomForestRegressor'>\n",
            "1.5362182304264578 \n",
            " 1.063448967350697\n",
            "<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>\n",
            "1.447044545188369 \n",
            " 1.038822502530652\n",
            "<class 'sklearn.svm._classes.SVR'>\n",
            "1.5638171047823812 \n",
            " 1.0296970066406015\n",
            "AVERAGING\n",
            "Averaging Model MAE: 1.028597651628823\n",
            "Averaging Model MSE: 1.4536716493404045\n",
            "BOOSTING\n",
            "Averaging Model MAE: 1.0388225025306517\n",
            "Averaging Model MSE: 1.447044545188369\n",
            "BAGGING\n",
            "[05:56:18] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[05:56:24] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[05:56:29] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[05:56:35] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[05:56:41] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[05:56:47] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[05:56:52] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[05:56:58] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[05:57:04] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[05:57:09] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "Bagging Model MAE: 1.0389943611936527\n",
            "Bagging Model MSE: 1.446111226984381\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Fire size 5-20"
      ],
      "metadata": {
        "id": "0ThGhVrvdMQZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "########## Spliting by size\n",
        "\n",
        "MidWest2 = SouthEast[(SouthEast[\"FIRE_SIZE\"] > 5) & (SouthEast[\"FIRE_SIZE\"] <= 20)]\n",
        "\n",
        "Fires12 = MidWest2[MidWest2['FIRE_YEAR'] <= 2017].copy()\n",
        "Fires13 = MidWest2[MidWest2['FIRE_YEAR'] > 2017].copy()\n",
        "X_train = Fires12.loc[:, Fires12.columns != 'FIRE_SIZE']\n",
        "y_train = Fires12['FIRE_SIZE'].copy()\n",
        "X_test = Fires13.loc[:, Fires13.columns != 'FIRE_SIZE']\n",
        "y_test = Fires13['FIRE_SIZE'].copy()\n",
        "\n",
        "############################################ Testing best methods\n",
        "\n",
        "model_list = [RandomForestRegressor, GradientBoostingRegressor, SVR] #GaussianNB, \n",
        "model_scores = dict()\n",
        "for model in model_list:\n",
        "  # fit model to data\n",
        "  print(model)\n",
        "  regressor = model().fit(X_train,y_train)\n",
        "  y_pred = regressor.predict(X_test)\n",
        "  model_mse = mean_squared_error(y_pred,y_test)\n",
        "  model_mae = mean_absolute_error(y_pred,y_test)\n",
        "  model_scores[model] = (regressor,y_pred,model_mse,model_mae)\n",
        "  print(model_mse,'\\n',model_mae)\n",
        "\n",
        "############################################# Averaging Method\n",
        "#############################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import xgboost as xgb\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        " \n",
        "# initializing all the model objects with default parameters\n",
        "model_1 = GradientBoostingRegressor()\n",
        "model_2 = SVR()\n",
        "model_3 = RandomForestRegressor()\n",
        " \n",
        "# training all the model on the training dataset\n",
        "model_1.fit(X_train, y_train)\n",
        "model_2.fit(X_train, y_train)\n",
        "model_3.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the validation dataset\n",
        "pred_1 = model_1.predict(X_test)\n",
        "pred_2 = model_2.predict(X_test)\n",
        "pred_3 = model_3.predict(X_test)\n",
        " \n",
        "# final prediction after averaging on the prediction of all 3 models\n",
        "pred_final = (pred_1+pred_2+pred_3)/3.0\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "mod1 = (mean_absolute_error(pred_1, y_test))\n",
        "mod2 = (mean_absolute_error(pred_2, y_test))\n",
        "mod3 = (mean_absolute_error(pred_3, y_test))\n",
        "#print(mean_absolute_error(pred_final, y_test))  ##switch Y_pred and test\n",
        "#Averaging_model = (mod1+mod2+mod3)/3.0\n",
        "Avg_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Avg_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('AVERAGING')\n",
        "print(\"Averaging Model MAE:\", Avg_mod)\n",
        "print(\"Averaging Model MSE:\", Avg_mod1)\n",
        "\n",
        "############################################# Boosting Method\n",
        "#############################################\n",
        "\n",
        "# importing utility modules\n",
        "import pandas as pd\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        " \n",
        "# initializing the boosting module with default parameters\n",
        "model = GradientBoostingRegressor() #163\n",
        " \n",
        "# training the model on the train dataset\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Boost_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Boost_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('BOOSTING')\n",
        "print(\"Averaging Model MAE:\", Boost_mod)\n",
        "print(\"Averaging Model MSE:\", Boost_mod1)\n",
        "print('BAGGING')\n",
        "\n",
        "################################################# Bagging Method\n",
        "#################################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "import xgboost as xgb\n",
        " \n",
        "# importing bagging module\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "\n",
        "# initializing the bagging model using XGboost as base model with default parameters\n",
        "model = BaggingRegressor(base_estimator=xgb.XGBRegressor())\n",
        " \n",
        "# training model\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Bag_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Bag_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print(\"Bagging Model MAE:\", Bag_mod)\n",
        "print(\"Bagging Model MSE:\", Bag_mod1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AW_jAwE8dOet",
        "outputId": "22db651c-56b4-401b-aac1-b62cfb7ae27a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'sklearn.ensemble._forest.RandomForestRegressor'>\n",
            "21.433331568766118 \n",
            " 3.8643540955884337\n",
            "<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>\n",
            "19.64887144279731 \n",
            " 3.7704214606070816\n",
            "<class 'sklearn.svm._classes.SVR'>\n",
            "23.382770116314877 \n",
            " 3.632846085344041\n",
            "AVERAGING\n",
            "Averaging Model MAE: 3.7175119613677974\n",
            "Averaging Model MSE: 20.420386633955886\n",
            "BOOSTING\n",
            "Averaging Model MAE: 3.7705946760236717\n",
            "Averaging Model MSE: 19.65040280126884\n",
            "BAGGING\n",
            "[06:12:52] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[06:12:55] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[06:12:57] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[06:13:00] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[06:13:02] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[06:13:05] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[06:13:08] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[06:13:10] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[06:13:13] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[06:13:15] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "Bagging Model MAE: 3.7664570962229083\n",
            "Bagging Model MSE: 19.696659389359723\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Fire size 20-50"
      ],
      "metadata": {
        "id": "lYGIM8S2dTpg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "########## Spliting by size\n",
        "\n",
        "MidWest2 = SouthEast[(SouthEast[\"FIRE_SIZE\"] > 20) & (SouthEast[\"FIRE_SIZE\"] <= 50)]\n",
        "\n",
        "Fires12 = MidWest2[MidWest2['FIRE_YEAR'] <= 2017].copy()\n",
        "Fires13 = MidWest2[MidWest2['FIRE_YEAR'] > 2017].copy()\n",
        "X_train = Fires12.loc[:, Fires12.columns != 'FIRE_SIZE']\n",
        "y_train = Fires12['FIRE_SIZE'].copy()\n",
        "X_test = Fires13.loc[:, Fires13.columns != 'FIRE_SIZE']\n",
        "y_test = Fires13['FIRE_SIZE'].copy()\n",
        "\n",
        "############################################ Testing best methods\n",
        "\n",
        "model_list = [RandomForestRegressor, GradientBoostingRegressor, SVR] #GaussianNB, \n",
        "model_scores = dict()\n",
        "for model in model_list:\n",
        "  # fit model to data\n",
        "  print(model)\n",
        "  regressor = model().fit(X_train,y_train)\n",
        "  y_pred = regressor.predict(X_test)\n",
        "  model_mse = mean_squared_error(y_pred,y_test)\n",
        "  model_mae = mean_absolute_error(y_pred,y_test)\n",
        "  model_scores[model] = (regressor,y_pred,model_mse,model_mae)\n",
        "  print(model_mse,'\\n',model_mae)\n",
        "\n",
        "############################################# Averaging Method\n",
        "#############################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import xgboost as xgb\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        " \n",
        "# initializing all the model objects with default parameters\n",
        "model_1 = GradientBoostingRegressor()\n",
        "model_2 = SVR()\n",
        "model_3 = RandomForestRegressor()\n",
        " \n",
        "# training all the model on the training dataset\n",
        "model_1.fit(X_train, y_train)\n",
        "model_2.fit(X_train, y_train)\n",
        "model_3.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the validation dataset\n",
        "pred_1 = model_1.predict(X_test)\n",
        "pred_2 = model_2.predict(X_test)\n",
        "pred_3 = model_3.predict(X_test)\n",
        " \n",
        "# final prediction after averaging on the prediction of all 3 models\n",
        "pred_final = (pred_1+pred_2+pred_3)/3.0\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "mod1 = (mean_absolute_error(pred_1, y_test))\n",
        "mod2 = (mean_absolute_error(pred_2, y_test))\n",
        "mod3 = (mean_absolute_error(pred_3, y_test))\n",
        "#print(mean_absolute_error(pred_final, y_test))  ##switch Y_pred and test\n",
        "#Averaging_model = (mod1+mod2+mod3)/3.0\n",
        "Avg_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Avg_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('AVERAGING')\n",
        "print(\"Averaging Model MAE:\", Avg_mod)\n",
        "print(\"Averaging Model MSE:\", Avg_mod1)\n",
        "\n",
        "############################################# Boosting Method\n",
        "#############################################\n",
        "\n",
        "# importing utility modules\n",
        "import pandas as pd\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        " \n",
        "# initializing the boosting module with default parameters\n",
        "model = GradientBoostingRegressor() #163\n",
        " \n",
        "# training the model on the train dataset\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Boost_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Boost_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('BOOSTING')\n",
        "print(\"Averaging Model MAE:\", Boost_mod)\n",
        "print(\"Averaging Model MSE:\", Boost_mod1)\n",
        "print('BAGGING')\n",
        "\n",
        "################################################# Bagging Method\n",
        "#################################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "import xgboost as xgb\n",
        " \n",
        "# importing bagging module\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "\n",
        "# initializing the bagging model using XGboost as base model with default parameters\n",
        "model = BaggingRegressor(base_estimator=xgb.XGBRegressor())\n",
        " \n",
        "# training model\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Bag_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Bag_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print(\"Bagging Model MAE:\", Bag_mod)\n",
        "print(\"Bagging Model MSE:\", Bag_mod1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_lgnO3bdVGi",
        "outputId": "755937ac-e6b5-47de-aeee-71a4418165d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'sklearn.ensemble._forest.RandomForestRegressor'>\n",
            "96.39005554279487 \n",
            " 8.326765641025641\n",
            "<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>\n",
            "87.12687040730293 \n",
            " 8.104288434857288\n",
            "<class 'sklearn.svm._classes.SVR'>\n",
            "101.81105951473484 \n",
            " 8.332686401345462\n",
            "AVERAGING\n",
            "Averaging Model MAE: 8.172551360898495\n",
            "Averaging Model MSE: 91.59906860696474\n",
            "BOOSTING\n",
            "Averaging Model MAE: 8.10582223775113\n",
            "Averaging Model MSE: 87.1552282668046\n",
            "BAGGING\n",
            "[06:14:45] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[06:14:46] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[06:14:47] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[06:14:47] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[06:14:48] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[06:14:49] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[06:14:50] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[06:14:51] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[06:14:52] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[06:14:53] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "Bagging Model MAE: 8.113711887359619\n",
            "Bagging Model MSE: 87.47452405066277\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Fire size 50-200"
      ],
      "metadata": {
        "id": "nITWJ035dXUT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "########## Spliting by size\n",
        "\n",
        "MidWest2 = SouthEast[(SouthEast[\"FIRE_SIZE\"] > 50) & (SouthEast[\"FIRE_SIZE\"] <= 200)]\n",
        "\n",
        "Fires12 = MidWest2[MidWest2['FIRE_YEAR'] <= 2017].copy()\n",
        "Fires13 = MidWest2[MidWest2['FIRE_YEAR'] > 2017].copy()\n",
        "X_train = Fires12.loc[:, Fires12.columns != 'FIRE_SIZE']\n",
        "y_train = Fires12['FIRE_SIZE'].copy()\n",
        "X_test = Fires13.loc[:, Fires13.columns != 'FIRE_SIZE']\n",
        "y_test = Fires13['FIRE_SIZE'].copy()\n",
        "\n",
        "############################################ Testing best methods\n",
        "\n",
        "model_list = [RandomForestRegressor, GradientBoostingRegressor, SVR] #GaussianNB, \n",
        "model_scores = dict()\n",
        "for model in model_list:\n",
        "  # fit model to data\n",
        "  print(model)\n",
        "  regressor = model().fit(X_train,y_train)\n",
        "  y_pred = regressor.predict(X_test)\n",
        "  model_mse = mean_squared_error(y_pred,y_test)\n",
        "  model_mae = mean_absolute_error(y_pred,y_test)\n",
        "  model_scores[model] = (regressor,y_pred,model_mse,model_mae)\n",
        "  print(model_mse,'\\n',model_mae)\n",
        "\n",
        "############################################# Averaging Method\n",
        "#############################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import xgboost as xgb\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        " \n",
        "# initializing all the model objects with default parameters\n",
        "model_1 = GradientBoostingRegressor()\n",
        "model_2 = SVR()\n",
        "model_3 = RandomForestRegressor()\n",
        " \n",
        "# training all the model on the training dataset\n",
        "model_1.fit(X_train, y_train)\n",
        "model_2.fit(X_train, y_train)\n",
        "model_3.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the validation dataset\n",
        "pred_1 = model_1.predict(X_test)\n",
        "pred_2 = model_2.predict(X_test)\n",
        "pred_3 = model_3.predict(X_test)\n",
        " \n",
        "# final prediction after averaging on the prediction of all 3 models\n",
        "pred_final = (pred_1+pred_2+pred_3)/3.0\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "mod1 = (mean_absolute_error(pred_1, y_test))\n",
        "mod2 = (mean_absolute_error(pred_2, y_test))\n",
        "mod3 = (mean_absolute_error(pred_3, y_test))\n",
        "#print(mean_absolute_error(pred_final, y_test))  ##switch Y_pred and test\n",
        "#Averaging_model = (mod1+mod2+mod3)/3.0\n",
        "Avg_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Avg_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('AVERAGING')\n",
        "print(\"Averaging Model MAE:\", Avg_mod)\n",
        "print(\"Averaging Model MSE:\", Avg_mod1)\n",
        "\n",
        "############################################# Boosting Method\n",
        "#############################################\n",
        "\n",
        "# importing utility modules\n",
        "import pandas as pd\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        " \n",
        "# initializing the boosting module with default parameters\n",
        "model = GradientBoostingRegressor() #163\n",
        " \n",
        "# training the model on the train dataset\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Boost_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Boost_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('BOOSTING')\n",
        "print(\"Averaging Model MAE:\", Boost_mod)\n",
        "print(\"Averaging Model MSE:\", Boost_mod1)\n",
        "print('BAGGING')\n",
        "\n",
        "################################################# Bagging Method\n",
        "#################################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "import xgboost as xgb\n",
        " \n",
        "# importing bagging module\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "\n",
        "# initializing the bagging model using XGboost as base model with default parameters\n",
        "model = BaggingRegressor(base_estimator=xgb.XGBRegressor())\n",
        " \n",
        "# training model\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Bag_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Bag_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print(\"Bagging Model MAE:\", Bag_mod)\n",
        "print(\"Bagging Model MSE:\", Bag_mod1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-tnFrGhJdY1i",
        "outputId": "f59ed13f-f13b-492e-a425-57a7f0b6e9c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'sklearn.ensemble._forest.RandomForestRegressor'>\n",
            "1868.4908284330327 \n",
            " 34.19644810521019\n",
            "<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>\n",
            "1730.218699534702 \n",
            " 32.69211977514611\n",
            "<class 'sklearn.svm._classes.SVR'>\n",
            "1873.958761570906 \n",
            " 32.50155949884756\n",
            "AVERAGING\n",
            "Averaging Model MAE: 32.40740572414719\n",
            "Averaging Model MSE: 1769.2824689846466\n",
            "BOOSTING\n",
            "Averaging Model MAE: 32.69211977514611\n",
            "Averaging Model MSE: 1730.5070148116406\n",
            "BAGGING\n",
            "[06:15:34] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[06:15:34] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[06:15:35] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[06:15:35] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[06:15:36] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[06:15:37] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[06:15:37] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[06:15:38] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[06:15:38] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[06:15:39] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "Bagging Model MAE: 32.67918297505083\n",
            "Bagging Model MSE: 1739.7106048627054\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Fire size 200-1000"
      ],
      "metadata": {
        "id": "95I8GqFjdbcG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "########## Spliting by size\n",
        "\n",
        "MidWest2 = SouthEast[(SouthEast[\"FIRE_SIZE\"] > 200) & (SouthEast[\"FIRE_SIZE\"] <= 1000)]\n",
        "\n",
        "Fires12 = MidWest2[MidWest2['FIRE_YEAR'] <= 2017].copy()\n",
        "Fires13 = MidWest2[MidWest2['FIRE_YEAR'] > 2017].copy()\n",
        "X_train = Fires12.loc[:, Fires12.columns != 'FIRE_SIZE']\n",
        "y_train = Fires12['FIRE_SIZE'].copy()\n",
        "X_test = Fires13.loc[:, Fires13.columns != 'FIRE_SIZE']\n",
        "y_test = Fires13['FIRE_SIZE'].copy()\n",
        "\n",
        "############################################ Testing best methods\n",
        "\n",
        "model_list = [RandomForestRegressor, GradientBoostingRegressor, SVR] #GaussianNB, \n",
        "model_scores = dict()\n",
        "for model in model_list:\n",
        "  # fit model to data\n",
        "  print(model)\n",
        "  regressor = model().fit(X_train,y_train)\n",
        "  y_pred = regressor.predict(X_test)\n",
        "  model_mse = mean_squared_error(y_pred,y_test)\n",
        "  model_mae = mean_absolute_error(y_pred,y_test)\n",
        "  model_scores[model] = (regressor,y_pred,model_mse,model_mae)\n",
        "  print(model_mse,'\\n',model_mae)\n",
        "\n",
        "############################################# Averaging Method\n",
        "#############################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import xgboost as xgb\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        " \n",
        "# initializing all the model objects with default parameters\n",
        "model_1 = GradientBoostingRegressor()\n",
        "model_2 = SVR()\n",
        "model_3 = RandomForestRegressor()\n",
        " \n",
        "# training all the model on the training dataset\n",
        "model_1.fit(X_train, y_train)\n",
        "model_2.fit(X_train, y_train)\n",
        "model_3.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the validation dataset\n",
        "pred_1 = model_1.predict(X_test)\n",
        "pred_2 = model_2.predict(X_test)\n",
        "pred_3 = model_3.predict(X_test)\n",
        " \n",
        "# final prediction after averaging on the prediction of all 3 models\n",
        "pred_final = (pred_1+pred_2+pred_3)/3.0\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "mod1 = (mean_absolute_error(pred_1, y_test))\n",
        "mod2 = (mean_absolute_error(pred_2, y_test))\n",
        "mod3 = (mean_absolute_error(pred_3, y_test))\n",
        "#print(mean_absolute_error(pred_final, y_test))  ##switch Y_pred and test\n",
        "#Averaging_model = (mod1+mod2+mod3)/3.0\n",
        "Avg_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Avg_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('AVERAGING')\n",
        "print(\"Averaging Model MAE:\", Avg_mod)\n",
        "print(\"Averaging Model MSE:\", Avg_mod1)\n",
        "\n",
        "############################################# Boosting Method\n",
        "#############################################\n",
        "\n",
        "# importing utility modules\n",
        "import pandas as pd\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        " \n",
        "# initializing the boosting module with default parameters\n",
        "model = GradientBoostingRegressor() #163\n",
        " \n",
        "# training the model on the train dataset\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Boost_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Boost_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('BOOSTING')\n",
        "print(\"Averaging Model MAE:\", Boost_mod)\n",
        "print(\"Averaging Model MSE:\", Boost_mod1)\n",
        "print('BAGGING')\n",
        "\n",
        "################################################# Bagging Method\n",
        "#################################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "import xgboost as xgb\n",
        " \n",
        "# importing bagging module\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "\n",
        "# initializing the bagging model using XGboost as base model with default parameters\n",
        "model = BaggingRegressor(base_estimator=xgb.XGBRegressor())\n",
        " \n",
        "# training model\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Bag_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Bag_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print(\"Bagging Model MAE:\", Bag_mod)\n",
        "print(\"Bagging Model MSE:\", Bag_mod1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODBjdC5XddP8",
        "outputId": "894ddf92-08fa-4220-b652-b3e0b5918283"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'sklearn.ensemble._forest.RandomForestRegressor'>\n",
            "49297.57744407192 \n",
            " 180.99261455164321\n",
            "<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>\n",
            "42702.39669178168 \n",
            " 161.6217075488321\n",
            "<class 'sklearn.svm._classes.SVR'>\n",
            "48769.91748533581 \n",
            " 168.40944486130118\n",
            "AVERAGING\n",
            "Averaging Model MAE: 167.21506695071375\n",
            "Averaging Model MSE: 44784.879305848335\n",
            "BOOSTING\n",
            "Averaging Model MAE: 161.45898415798268\n",
            "Averaging Model MSE: 42625.71456824167\n",
            "BAGGING\n",
            "[05:37:23] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[05:37:24] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[05:37:25] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[05:37:25] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[05:37:26] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[05:37:26] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[05:37:27] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[05:37:28] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[05:37:28] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[05:37:29] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "Bagging Model MAE: 165.81484702813233\n",
            "Bagging Model MSE: 45155.28513612657\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Fire size 500-1000"
      ],
      "metadata": {
        "id": "iowofO_udf0y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "########## Spliting by size\n",
        "\n",
        "MidWest2 = SouthEast[(SouthEast[\"FIRE_SIZE\"] > 500) & (SouthEast[\"FIRE_SIZE\"] <= 1000)]\n",
        "\n",
        "Fires12 = MidWest2[MidWest2['FIRE_YEAR'] <= 2017].copy()\n",
        "Fires13 = MidWest2[MidWest2['FIRE_YEAR'] > 2017].copy()\n",
        "X_train = Fires12.loc[:, Fires12.columns != 'FIRE_SIZE']\n",
        "y_train = Fires12['FIRE_SIZE'].copy()\n",
        "X_test = Fires13.loc[:, Fires13.columns != 'FIRE_SIZE']\n",
        "y_test = Fires13['FIRE_SIZE'].copy()\n",
        "\n",
        "############################################ Testing best methods\n",
        "\n",
        "model_list = [RandomForestRegressor, GradientBoostingRegressor, SVR] #GaussianNB, \n",
        "model_scores = dict()\n",
        "for model in model_list:\n",
        "  # fit model to data\n",
        "  print(model)\n",
        "  regressor = model().fit(X_train,y_train)\n",
        "  y_pred = regressor.predict(X_test)\n",
        "  model_mse = mean_squared_error(y_pred,y_test)\n",
        "  model_mae = mean_absolute_error(y_pred,y_test)\n",
        "  model_scores[model] = (regressor,y_pred,model_mse,model_mae)\n",
        "  print(model_mse,'\\n',model_mae)\n",
        "\n",
        "############################################# Averaging Method\n",
        "#############################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import xgboost as xgb\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        " \n",
        "# initializing all the model objects with default parameters\n",
        "model_1 = GradientBoostingRegressor()\n",
        "model_2 = SVR()\n",
        "model_3 = RandomForestRegressor()\n",
        " \n",
        "# training all the model on the training dataset\n",
        "model_1.fit(X_train, y_train)\n",
        "model_2.fit(X_train, y_train)\n",
        "model_3.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the validation dataset\n",
        "pred_1 = model_1.predict(X_test)\n",
        "pred_2 = model_2.predict(X_test)\n",
        "pred_3 = model_3.predict(X_test)\n",
        " \n",
        "# final prediction after averaging on the prediction of all 3 models\n",
        "pred_final = (pred_1+pred_2+pred_3)/3.0\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "mod1 = (mean_absolute_error(pred_1, y_test))\n",
        "mod2 = (mean_absolute_error(pred_2, y_test))\n",
        "mod3 = (mean_absolute_error(pred_3, y_test))\n",
        "#print(mean_absolute_error(pred_final, y_test))  ##switch Y_pred and test\n",
        "#Averaging_model = (mod1+mod2+mod3)/3.0\n",
        "Avg_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Avg_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('AVERAGING')\n",
        "print(\"Averaging Model MAE:\", Avg_mod)\n",
        "print(\"Averaging Model MSE:\", Avg_mod1)\n",
        "\n",
        "############################################# Boosting Method\n",
        "#############################################\n",
        "\n",
        "# importing utility modules\n",
        "import pandas as pd\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        " \n",
        "# initializing the boosting module with default parameters\n",
        "model = GradientBoostingRegressor() #163\n",
        " \n",
        "# training the model on the train dataset\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Boost_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Boost_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('BOOSTING')\n",
        "print(\"Averaging Model MAE:\", Boost_mod)\n",
        "print(\"Averaging Model MSE:\", Boost_mod1)\n",
        "print('BAGGING')\n",
        "\n",
        "################################################# Bagging Method\n",
        "#################################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "import xgboost as xgb\n",
        " \n",
        "# importing bagging module\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "\n",
        "# initializing the bagging model using XGboost as base model with default parameters\n",
        "model = BaggingRegressor(base_estimator=xgb.XGBRegressor())\n",
        " \n",
        "# training model\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Bag_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Bag_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print(\"Bagging Model MAE:\", Bag_mod)\n",
        "print(\"Bagging Model MSE:\", Bag_mod1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5rXysuxdh5Z",
        "outputId": "4700522a-9b3f-4140-8ecb-b5cc14460eeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'sklearn.ensemble._forest.RandomForestRegressor'>\n",
            "28497.17575101017 \n",
            " 143.91867120757573\n",
            "<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>\n",
            "30180.608191823834 \n",
            " 147.13738503868447\n",
            "<class 'sklearn.svm._classes.SVR'>\n",
            "27220.850184477225 \n",
            " 139.0441812739085\n",
            "AVERAGING\n",
            "Averaging Model MAE: 142.5655414831778\n",
            "Averaging Model MSE: 27468.329586392596\n",
            "BOOSTING\n",
            "Averaging Model MAE: 147.23891217438774\n",
            "Averaging Model MSE: 30235.504285506962\n",
            "BAGGING\n",
            "[06:15:50] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[06:15:50] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[06:15:51] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[06:15:51] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[06:15:51] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[06:15:51] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[06:15:51] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[06:15:51] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[06:15:51] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[06:15:51] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "Bagging Model MAE: 144.0586054021662\n",
            "Bagging Model MSE: 28414.210633128812\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Fire size 1000+"
      ],
      "metadata": {
        "id": "KGyYieOKdkfk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "########## Spliting by size\n",
        "\n",
        "MidWest2 = SouthEast[(SouthEast[\"FIRE_SIZE\"] > 1000)]\n",
        "\n",
        "Fires12 = MidWest2[MidWest2['FIRE_YEAR'] <= 2017].copy()\n",
        "Fires13 = MidWest2[MidWest2['FIRE_YEAR'] > 2017].copy()\n",
        "X_train = Fires12.loc[:, Fires12.columns != 'FIRE_SIZE']\n",
        "y_train = Fires12['FIRE_SIZE'].copy()\n",
        "X_test = Fires13.loc[:, Fires13.columns != 'FIRE_SIZE']\n",
        "y_test = Fires13['FIRE_SIZE'].copy()\n",
        "\n",
        "############################################ Testing best methods\n",
        "\n",
        "model_list = [RandomForestRegressor, GradientBoostingRegressor, SVR] #GaussianNB, \n",
        "model_scores = dict()\n",
        "for model in model_list:\n",
        "  # fit model to data\n",
        "  print(model)\n",
        "  regressor = model().fit(X_train,y_train)\n",
        "  y_pred = regressor.predict(X_test)\n",
        "  model_mse = mean_squared_error(y_pred,y_test)\n",
        "  model_mae = mean_absolute_error(y_pred,y_test)\n",
        "  model_scores[model] = (regressor,y_pred,model_mse,model_mae)\n",
        "  print(model_mse,'\\n',model_mae)\n",
        "\n",
        "############################################# Averaging Method\n",
        "#############################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import xgboost as xgb\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        " \n",
        "# initializing all the model objects with default parameters\n",
        "model_1 = GradientBoostingRegressor()\n",
        "model_2 = SVR()\n",
        "model_3 = RandomForestRegressor()\n",
        " \n",
        "# training all the model on the training dataset\n",
        "model_1.fit(X_train, y_train)\n",
        "model_2.fit(X_train, y_train)\n",
        "model_3.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the validation dataset\n",
        "pred_1 = model_1.predict(X_test)\n",
        "pred_2 = model_2.predict(X_test)\n",
        "pred_3 = model_3.predict(X_test)\n",
        " \n",
        "# final prediction after averaging on the prediction of all 3 models\n",
        "pred_final = (pred_1+pred_2+pred_3)/3.0\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "mod1 = (mean_absolute_error(pred_1, y_test))\n",
        "mod2 = (mean_absolute_error(pred_2, y_test))\n",
        "mod3 = (mean_absolute_error(pred_3, y_test))\n",
        "#print(mean_absolute_error(pred_final, y_test))  ##switch Y_pred and test\n",
        "#Averaging_model = (mod1+mod2+mod3)/3.0\n",
        "Avg_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Avg_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('AVERAGING')\n",
        "print(\"Averaging Model MAE:\", Avg_mod)\n",
        "print(\"Averaging Model MSE:\", Avg_mod1)\n",
        "\n",
        "############################################# Boosting Method\n",
        "#############################################\n",
        "\n",
        "# importing utility modules\n",
        "import pandas as pd\n",
        "\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        " \n",
        "# initializing the boosting module with default parameters\n",
        "model = GradientBoostingRegressor() #163\n",
        " \n",
        "# training the model on the train dataset\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Boost_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Boost_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print('BOOSTING')\n",
        "print(\"Averaging Model MAE:\", Boost_mod)\n",
        "print(\"Averaging Model MSE:\", Boost_mod1)\n",
        "print('BAGGING')\n",
        "\n",
        "################################################# Bagging Method\n",
        "#################################################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        " \n",
        "# importing machine learning models for prediction\n",
        "import xgboost as xgb\n",
        " \n",
        "# importing bagging module\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "\n",
        "# initializing the bagging model using XGboost as base model with default parameters\n",
        "model = BaggingRegressor(base_estimator=xgb.XGBRegressor())\n",
        " \n",
        "# training model\n",
        "model.fit(X_train, y_train)\n",
        " \n",
        "# predicting the output on the test dataset\n",
        "pred_final = model.predict(X_test)\n",
        " \n",
        "# printing the root mean squared error between real value and predicted value\n",
        "Bag_mod = (mean_absolute_error(pred_final, y_test))\n",
        "Bag_mod1 = (mean_squared_error(pred_final, y_test))\n",
        "print(\"Bagging Model MAE:\", Bag_mod)\n",
        "print(\"Bagging Model MSE:\", Bag_mod1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhWjgCaKdmGz",
        "outputId": "72146b1c-e0f9-4a1f-fda2-0d76ffaeaa98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'sklearn.ensemble._forest.RandomForestRegressor'>\n",
            "584529102.1153295 \n",
            " 11928.167497820226\n",
            "<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>\n",
            "327929247.90868235 \n",
            " 9178.808399440955\n",
            "<class 'sklearn.svm._classes.SVR'>\n",
            "89942397.63305333 \n",
            " 3617.61087619413\n",
            "AVERAGING\n",
            "Averaging Model MAE: 7483.293486283077\n",
            "Averaging Model MSE: 187867178.33727163\n",
            "BOOSTING\n",
            "Averaging Model MAE: 9178.808399440955\n",
            "Averaging Model MSE: 327929247.90868235\n",
            "BAGGING\n",
            "[06:15:55] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[06:15:55] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[06:15:55] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[06:15:55] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[06:15:55] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[06:15:56] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[06:15:56] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[06:15:56] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[06:15:56] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[06:15:56] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "Bagging Model MAE: 10169.509022779142\n",
            "Bagging Model MSE: 285699740.1293536\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFUPnTZNZLkC"
      },
      "source": [
        "### Confidence Interval correct version\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RiyVCmwN5xxg"
      },
      "outputs": [],
      "source": [
        "## created models on the .05, .5. .95 intervals\n",
        "## only could use gradient boosting regressoor due to alpha variable\n",
        "model_list = [GradientBoostingRegressor]\n",
        "model_scores = dict()\n",
        "for model in model_list:\n",
        "  print(model)\n",
        "  alpha_models = []\n",
        "  alpha_preds = []\n",
        "  for alpha in [.05,.5,.95]:\n",
        "    reg = model(loss=\"quantile\", alpha=alpha).fit(X_train,y_train)\n",
        "    alpha_models.append(reg)\n",
        "    y_pred = reg.predict(X_test)\n",
        "    alpha_preds.append(y_pred)\n",
        "  model_scores[model] = (alpha_models,alpha_preds,model_mse,model_mae)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGblR7zf8vc6"
      },
      "outputs": [],
      "source": [
        "from joblib import dump"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijdZ0-9k8R3U"
      },
      "outputs": [],
      "source": [
        "counter = 1\n",
        "for m in alpha_models:\n",
        "  dump(m,'/content/drive/MyDrive/DS440/alpha_model%s.joblib' %str(counter))\n",
        "  counter += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZoqktxo83zK"
      },
      "outputs": [],
      "source": [
        "counter = 1\n",
        "for m in alpha_preds:\n",
        "  dump(m,'/content/drive/MyDrive/DS440/alpha_preds%s.joblib' %str(counter))\n",
        "  counter += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eG6u53rt7YEi"
      },
      "outputs": [],
      "source": [
        "#for m in model_scores.keys():\n",
        " # alpha_model[m][0]\n",
        "  #model_scores[m][1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LC44WkLY8CvL"
      },
      "outputs": [],
      "source": [
        "#model_scores[<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpQpO2v_gDlo"
      },
      "outputs": [],
      "source": [
        "from joblib import dump, load\n",
        "\n",
        "# Read in models\n",
        "model1 = load('/content/drive/MyDrive/DS440/alpha_model1.joblib')\n",
        "model2 = load('/content/drive/MyDrive/DS440/alpha_model2.joblib')\n",
        "model3 = load('/content/drive/MyDrive/DS440/alpha_model3.joblib')\n",
        "\n",
        "# Make predictions on fire data\n",
        "training_cols = ['FIRE_YEAR','LATITUDE','LONGITUDE','DAYS_TO_CONT','DISCOVERY_MONTH']\n",
        "Fires['pred1'] = model1.predict(Fires[training_cols])\n",
        "Fires['pred2'] = model2.predict(Fires[training_cols])\n",
        "Fires['pred3'] = model3.predict(Fires[training_cols])\n",
        "\n",
        "# calculate mean by year on the predictions\n",
        "means1 = Fires.groupby(['FIRE_YEAR'])['pred1'].mean()\n",
        "means2 = Fires.groupby(['FIRE_YEAR'])['pred2'].mean()\n",
        "means3 = Fires.groupby(['FIRE_YEAR'])['pred3'].mean()\n",
        "\n",
        "# plot results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QEad_Y6RiieK"
      },
      "outputs": [],
      "source": [
        "years = list(Fires['FIRE_YEAR'].unique())\n",
        "years.sort()\n",
        "years"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJFBgFm2ioV-"
      },
      "outputs": [],
      "source": [
        "#years = list(Fires['FIRE_YEAR'].unique())\n",
        "#years\n",
        "#years.sort(reverse=True)\n",
        "plt.plot(Fires['FIRE_YEAR'].unique(),means1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdMN4ThKJ3Q0"
      },
      "source": [
        ".05 confidence interval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjjTrRAYi13k"
      },
      "outputs": [],
      "source": [
        "plt.scatter(means1, Fires['FIRE_YEAR'].unique(), marker='o');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cqBnb8u9VjJf"
      },
      "outputs": [],
      "source": [
        "plt.hist(means1, density=False, bins=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZGz6UcbPiM5t"
      },
      "outputs": [],
      "source": [
        "plt.plot(Fires['FIRE_YEAR'].unique(),means1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t13pxhl1J5no"
      },
      "source": [
        ".50 confidence interval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UVqj6FYtZwHW"
      },
      "outputs": [],
      "source": [
        "plt.plot(Fires['FIRE_YEAR'].unique(),means2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tIRLFnyFZxOj"
      },
      "outputs": [],
      "source": [
        "plt.scatter(means2, Fires['FIRE_YEAR'].unique(), marker='o');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4XZpcJdKZ1ID"
      },
      "outputs": [],
      "source": [
        "plt.hist(means2, density=False, bins=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YvGWrySEZ2Fr"
      },
      "outputs": [],
      "source": [
        "plt.plot(Fires['FIRE_YEAR'].unique(),means2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8tVNLCXJ7dB"
      },
      "source": [
        ".95 confidence interval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9xhurHSZ8IU"
      },
      "outputs": [],
      "source": [
        "plt.plot(Fires['FIRE_YEAR'].unique(),means3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8OovbKJiZ9lA"
      },
      "outputs": [],
      "source": [
        "plt.scatter(means3, Fires['FIRE_YEAR'].unique(), marker='o');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yP_oFHIfZ-Uc"
      },
      "outputs": [],
      "source": [
        "plt.hist(means3, density=False, bins=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXDUK1RpZ_JY"
      },
      "outputs": [],
      "source": [
        "plt.plot(Fires['FIRE_YEAR'].unique(),means3)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyperparameter tuning GridSearchCV"
      ],
      "metadata": {
        "id": "OpF6t-BRTGHW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8sXEfmNucMS"
      },
      "outputs": [],
      "source": [
        "from sklearn import model_selection\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from imblearn.over_sampling import SMOTE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kc7eryFYu1QJ"
      },
      "outputs": [],
      "source": [
        "# x_train, x_test, y_train, y_test = model_selection.train_test_split(Fires, label_severity, test_size= 0.50, stratify = label_severity)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMZOzJBmttFA"
      },
      "outputs": [],
      "source": [
        "#### These parameters ran for 4+ hours\n",
        "\n",
        "param_grid = {\n",
        "    'bootstrap': [True, False],\n",
        "    'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
        "    'max_features': ['auto', 'sqrt'],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]\n",
        "}\n",
        "# Create a based model\n",
        "rf = RandomForestRegressor()\n",
        "# Instantiate the grid search model\n",
        "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
        "                          cv = 3, n_jobs = -1, verbose = 2)\n",
        "\n",
        "# Fit the grid search to the data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "#### trained on 3960 candidates totalling 11800 fits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g3Pl5Ehkvloe"
      },
      "outputs": [],
      "source": [
        "param_grid = {\n",
        "    'bootstrap': [True],\n",
        "    'max_depth': [80, 90, 100, 110],\n",
        "    'max_features': [2, 3],\n",
        "    'min_samples_leaf': [3, 4, 5],\n",
        "    'min_samples_split': [8, 10, 12],\n",
        "    'n_estimators': [100, 200, 300, 500]\n",
        "}\n",
        "# Create a based model\n",
        "rf = RandomForestRegressor()\n",
        "# Instantiate the grid search model\n",
        "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
        "                          cv = 3, n_jobs = -1, verbose = 2)\n",
        "\n",
        "# Fit the grid search to the data\n",
        "grid_search.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmLzjYbMpd2a",
        "outputId": "865f7334-a15b-48d7-f500-e2e286024774"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'bootstrap': True,\n",
              " 'max_depth': 110,\n",
              " 'max_features': 3,\n",
              " 'min_samples_leaf': 3,\n",
              " 'min_samples_split': 8,\n",
              " 'n_estimators': 300}"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "grid_search.best_params_\n",
        "\n",
        "### on .001 fraction\n",
        "#{'bootstrap': True,\n",
        " #'max_depth': 110,\n",
        " #'max_features': 3,\n",
        " #'min_samples_leaf': 3,\n",
        " #'min_samples_split': 8,\n",
        " #'n_estimators': 300}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfmDivJQpjzG"
      },
      "outputs": [],
      "source": [
        "#Create a Gaussian Regressor\n",
        "clf=RandomForestRegressor(n_estimators = 100, \n",
        "                           bootstrap = True, \n",
        "                           max_depth = 35, \n",
        "                           max_features = 3, \n",
        "                           min_samples_leaf = 5,\n",
        "                           min_samples_split = 12)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4DnvgHFq_jC",
        "outputId": "55c56304-47c0-4959-c1c8-20333efa8750"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "RandomForestRegressor(max_depth=35, max_features=3, min_samples_leaf=5,\n",
              "                      min_samples_split=12)"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "clf.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUeQCdtEqPdS",
        "outputId": "37a9e313-da7b-48f3-fbc6-fe93d70e6a61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean Absolute Error: 105.12 degrees.\n"
          ]
        }
      ],
      "source": [
        "### .001\n",
        "# Use the forest's predict method on the test data\n",
        "prediction_s = clf.predict(X_test)\n",
        "# Calculate the absolute errors\n",
        "errors_s = abs(prediction_s - y_test)\n",
        "# Print out the mean absolute error (mae)\n",
        "print('Mean Absolute Error:', round(np.mean(errors_s), 2), 'degrees.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FST3DNstYug"
      },
      "outputs": [],
      "source": [
        "### .05\n",
        "# Use the forest's predict method on the test data\n",
        "prediction_s = clf.predict(X_test)\n",
        "# Calculate the absolute errors\n",
        "errors_s = abs(prediction_s - y_test)\n",
        "# Print out the mean absolute error (mae)\n",
        "print('Mean Absolute Error:', round(np.mean(errors_s), 2), 'degrees.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FcQbmoa2z5q0"
      },
      "outputs": [],
      "source": [
        "from joblib import load\n",
        "import scipy.stats as st\n",
        "### loading in prediction for our best model for confidence interval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L5EL1oOK0_KQ"
      },
      "outputs": [],
      "source": [
        "Pred_SVR = load('/content/drive/MyDrive/DS440/Predictions/PredForSVR().joblib')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w15UMWv11rry",
        "outputId": "3037785e-aaec-499c-da4a-af3de1932ddd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1.04378747, 0.88985227, 0.95828591, ..., 0.63937614, 1.04136657,\n",
              "       0.88600587])"
            ]
          },
          "execution_count": 91,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Pred_SVR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXU19mcb2DVy"
      },
      "outputs": [],
      "source": [
        "st.t.interval(alpha=0.95, df=len(Pred_SVR)-1, loc=np.mean(Pred_SVR), scale=st.sem(Pred_SVR)) \n",
        "\n",
        "(16.758, 24.042)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVFgvK6HiVsP"
      },
      "outputs": [],
      "source": [
        "Fires"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Implementation Colby",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}